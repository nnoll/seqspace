<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Manifold learning · SeqSpace.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><script src="../../../copy.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">SeqSpace.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Science</span><ul><li><a class="tocitem" href="../normalize/">scRNAseq normalization</a></li><li><a class="tocitem" href="../inference/">scRNAseq spatial inference</a></li><li class="is-active"><a class="tocitem" href>Manifold learning</a><ul class="internal"><li><a class="tocitem" href="#Introduction"><span>Introduction</span></a></li><li><a class="tocitem" href="#Empirical-analysis"><span>Empirical analysis</span></a></li><li><a class="tocitem" href="#De-novo-manifold-learning"><span>De-novo manifold learning</span></a></li><li><a class="tocitem" href="#Drosophila-manifold"><span>Drosophila manifold</span></a></li></ul></li></ul></li><li><span class="tocitem">Library</span><ul><li><a class="tocitem" href="../../lib/distance/">Distances</a></li><li><a class="tocitem" href="../../lib/generate/">Point Cloud Generation</a></li><li><a class="tocitem" href="../../lib/infer/">Supervised Spatial Inference</a></li><li><a class="tocitem" href="../../lib/io/">Data Input/Output</a></li><li><a class="tocitem" href="../../lib/manifold/">Point Cloud Differential Geometry</a></li><li><a class="tocitem" href="../../lib/model/">Autoencoder Manifold Learning</a></li><li><a class="tocitem" href="../../lib/normalize/">scRNAseq Normalization</a></li><li><a class="tocitem" href="../../lib/pointcloud/">Point Cloud</a></li><li><a class="tocitem" href="../../lib/queue/">Priority Queue</a></li><li><a class="tocitem" href="../../lib/rank/">Differentiable Rank</a></li><li><a class="tocitem" href="../../lib/scrna/">scRNAseq Data</a></li><li><a class="tocitem" href="../../lib/util/">Utilities</a></li><li><a class="tocitem" href="../../lib/voronoi/">Voronoi</a></li></ul></li><li><span class="tocitem">Command Line</span><ul><li><a class="tocitem" href="../../cli/drosophila/">Drosophila pipeline</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Science</a></li><li class="is-active"><a href>Manifold learning</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Manifold learning</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com//blob/master/docs/src/sci/autoencode.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Manifold-learning"><a class="docs-heading-anchor" href="#Manifold-learning">Manifold learning</a><a id="Manifold-learning-1"></a><a class="docs-heading-anchor-permalink" href="#Manifold-learning" title="Permalink"></a></h1><h2 id="Introduction"><a class="docs-heading-anchor" href="#Introduction">Introduction</a><a id="Introduction-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction" title="Permalink"></a></h2><p>In section <a href="../inference/#scRNAseq-spatial-inference">scRNAseq spatial inference</a>, we demonstrated that scRNAseq expression data can be directly mapped to space using a reference database  of gene expression patterns, provided by the Berkeley Drosophila Transcriptional Network Project. These results motivate a more ambitious question: is positional information directly encoded within gene expression and, as such, can we utilize <em>just</em> scRNAseq counts to infer cellular spatial position <em>de-novo</em>?</p><p>We anchor our inference approach on the <em>continuum ansatz</em> of cellular states in a developing embryo. Specifically, we postulate that gene expression is a <em>smoothly</em> varying, continuous function of space, and potentially other continuous degrees of freedom such as time. Said another way, neighboring cells are assumed to be infinitesimally &quot;close&quot; in expression. This formulation is a departure from the conventional, <em>discrete</em>, view of cellular fates taken, for example, in the description of the striped gene patterning observed during the Anterior-Posterior segmentation of the <em>Drosophila melongaster</em> embryo. From our viewpoint, a &quot;discontinuity&quot; of gene expression in a handful of components can also be explained by the curvature of a <em>smooth</em> surface embedded in very high dimensions.</p><p>As formulated, spatial inference from scRNAseq data is equivalent to non-linear dimensional reduction: we want to find a small number of degrees of freedom that parameterize the variation across <span>$\sim 10^4$</span> genes. From <a href="../normalize/#Drosophila-results">Drosophila results</a>, we know that at least minimally, we can describe gene expression within early Drosophila embryogenesis by <span>$31$</span> relevant components. In this section, we analyze the data within this subspace to show that the data is ultimately generated by an even smaller set of degrees of freedom. Furthermore, we outline a protocol to &quot;learn&quot; this parameterization and show that it can recapitulate space.</p><h2 id="Empirical-analysis"><a class="docs-heading-anchor" href="#Empirical-analysis">Empirical analysis</a><a id="Empirical-analysis-1"></a><a class="docs-heading-anchor-permalink" href="#Empirical-analysis" title="Permalink"></a></h2><h3 id="Hausdorff-dimension-estimation"><a class="docs-heading-anchor" href="#Hausdorff-dimension-estimation">Hausdorff dimension estimation</a><a id="Hausdorff-dimension-estimation-1"></a><a class="docs-heading-anchor-permalink" href="#Hausdorff-dimension-estimation" title="Permalink"></a></h3><p>A critical parameter to determine <em>empirically</em> is the underlying dimensionality of gene expression of early _Drosophila embryogenesis, as given by the scRNAseq data. Informally, we expect that if the data is sampled from an underlying <span>$d$</span> dimensional manifold, than the number of points <span>$n$</span> enclosed by a sphere of radius <span>$R$</span> should grow as</p><p class="math-container">\[    n(R) \sim R^d\]</p><p>However, we note that performing this comparison utilizing the Euclidean metric between normalized cellular expression would be manifestly incorrect; we postulated that gene expression is a manifold which implies a Euclidean metric <em>locally</em> between neighbors. There is no <em>a priori</em> reason to expect Euclidean distances to be a good measure for far cells. Similar considerations hold for other postulated global metrics. Instead, we estimate geodesic distances empirically, in analog to the Isomap algorithm <sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup>.</p><h4 id="Neighborhood-graph-construction"><a class="docs-heading-anchor" href="#Neighborhood-graph-construction">Neighborhood graph construction</a><a id="Neighborhood-graph-construction-1"></a><a class="docs-heading-anchor-permalink" href="#Neighborhood-graph-construction" title="Permalink"></a></h4><p>The first task is to formulate an algorithm to approximate the metric space the point cloud is sampled from and subsequently utilize our estimate to compute all pairwise distances. We proceed guided by intuition gleaned from <em>Differential Geometry</em>: pairwise distances within local neighborhoods are expected to be well-described by a Euclidean metric in the tangent space. Conversely, macroscopic distances can only be computed via integration against the underlying metric along the corresponding geodesic. We denote <span>$D_{\alpha\beta}$</span> as the resultant pairwise distances between cell <span>$\alpha,\beta$</span>.</p><p>The basic construction of the neighborhood graph is demonstrated in the below cartoon.</p><p align="center">
<img src="/seqspace/assets/drosophila/radius_scaling_neighborhood.svg" width="32%" class="center"/>
<img src="/seqspace/assets/drosophila/radius_scaling_shortest_path.svg" width="32%" class="center"/>
<img src="/seqspace/assets/drosophila/radius_scaling_ball.svg" width="32%" class="center"/>
</p><p>Our algorithm to construct the neighborhood graph proceeds in three main steps. First, we visit the local neighborhood of each point (cell) within our dataset. In principle, the neighborhood can be defined by either a fixed number of neighbors <span>$k$</span> or a given radius <span>$R$</span>. In practice, for <em>Drosophila melongaster</em> we chose to define neighborhood by <span>$k=8$</span>. The neighborhood is assumed to be a good approximation to the manifold tangent space and thus pairwise distances between the point and its neighbors are computed within the euclidean metric. Each pairwise neighborhood relationship is captured by an edge, weighted by the pairwise distance. The collection of all cells, and their neighborhood edges, constitute the <em>neighborhood graph</em>.</p><p>Given a neighborhood graph as defined above, computing the &quot;geodesic&quot; distance between cells reduces to finding the shortest path between all pairs of points. This can be solved using either the Floyd-Warshall algorithm in <span>$\mathcal{O}(N^3)$</span> time <sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup> or iteratively using Dijikstra&#39;s algorithm in <span>$\mathcal{O}(N(N\log N + E))$</span> time <sup class="footnote-reference"><a id="citeref-3" href="#footnote-3">[3]</a></sup>. We chose the later due to the sparsity of the neighborhood graph. The estimation of geodesic distances, if performed as described, asymptotically approaches the true value as the density of points increases <sup class="footnote-reference"><a id="citeref-4" href="#footnote-4">[4]</a></sup>.</p><p>Given our estimate for all pairwise geodesic distances, we can now empirically estimate the <em>Hausdorff</em> dimension. Simply stated, the estimate is done by analyzing the scaling relationship between number of points enclosed within a <em>geodesic</em> ball of radius <span>$R$</span> and the radius <span>$R$</span> itself. Below we show the results plotted for each cell within our dataset, along with a trend line <span>$n(R) \sim R^3$</span> shown in red.</p><p align="center">
<img src="/seqspace/assets/drosophila/pointcloud_scaling.png" width="68%" class="center"/>
</p><p>The manifold clearly looks three-dimensional.</p><h3 id="Physical-proximity-implies-expression-proximity"><a class="docs-heading-anchor" href="#Physical-proximity-implies-expression-proximity">Physical proximity implies expression proximity</a><a id="Physical-proximity-implies-expression-proximity-1"></a><a class="docs-heading-anchor-permalink" href="#Physical-proximity-implies-expression-proximity" title="Permalink"></a></h3><p>In <a href="#Neighborhood-graph-construction">Neighborhood graph construction</a>, we demonstrated that the normalized scRNAseq appears to be three-dimensional. This supports <em>half</em> of our continuum ansatz: gene expression of early Drosophila embryogenesis is consistent with being distributed along a low-dimensional manifold. However, <em>a priori</em> is is unclear that such dimensions meaningfully correspond to space. Additionally, at the developmental stage sampled, the <em>Drosophila</em> embryo is a two-dimensional epithelial monolayer and as such it is unclear what a potential third dimension would parameterize. To this end, we utilize the positional labels calculated in <a href="../inference/#scRNAseq-spatial-inference">scRNAseq spatial inference</a> to assay if physically close cells are close along the putative manifold.</p><p align="center">
<img src="/seqspace/assets/drosophila/pointcloud_locality.png" width="68%" class="center"/>
</p><p>Specifically, we demonstrate that conditioning on varying <em>average</em> spatial geodesic distances results in quantitative effects in the distribution of pairwise expression geodesic distances. Physically close cells are not only close in expression, but the median of the distribution increases quantitatively (albeit non-linearly) with increasing conditioned shells of distance. Taken together, the scRNAseq data is empirically <em>consistent</em> with our continuum expression ansatz.</p><h3 id="Isomap-dimensional-reduction"><a class="docs-heading-anchor" href="#Isomap-dimensional-reduction">Isomap dimensional reduction</a><a id="Isomap-dimensional-reduction-1"></a><a class="docs-heading-anchor-permalink" href="#Isomap-dimensional-reduction" title="Permalink"></a></h3><p>We provide additional evidence in support of our underlying hypothesis by utilizing the Isomap algorithm <sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup>. As such, we simply perform a Principal Coordinates Analysis on the geodesic distances estimated in <a href="#Neighborhood-graph-construction">Neighborhood graph construction</a>. This is tantamount to finding a set of low-dimensional coordinates <span>$\xi_\alpha$</span> that minimize the strain between their Euclidean embedding and the given distance matrix <span>$D_{\alpha\beta}$</span>.</p><p class="math-container">\[    E(\{\xi_\alpha\}) = \left(\frac{\left(\sum_{\alpha,\beta} D_{\alpha\beta} - |\xi_\alpha - \xi_\beta|^2\right)^2}{\sum_{\alpha,\beta} D_{\alpha\beta}^2 }\right)^{1/2}\]</p><p align="center">
<img src="/seqspace/assets/drosophila/isomap_AP.png" width="42%" class="center"/>
<img src="/seqspace/assets/drosophila/isomap_DV.png" width="42%" class="center"/>
</p><p>Above, we show the resultant embedding, colored by the estimated AP (anterior-posterior) and DV (dorsal-vental) position, obtained by averaging over the distribution obtained in <a href="../inference/#scRNAseq-spatial-inference">scRNAseq spatial inference</a>. It is clear that two of the major axes of the embedding quantitatively segregate both spatial axes of the embryo. We emphasize that the estimated spatial positions <em>were not</em> used to generate the shown embedding, but rather <em>only</em> the underlying neighborhood distances of cells within our scRNAseq dataset. This is taken as strong evidence in support of our underlying hypothesis.</p><p>By varying the dimensionality of the Isomap embedding, we see that three dimensions is the &quot;knee&quot; of dimensioning returns, consistent with the 3-dimensional scaling observed above.</p><p align="center">
<img src="/seqspace/assets/drosophila/isomap_dimension.png" width="68%" class="center"/>
</p><h2 id="De-novo-manifold-learning"><a class="docs-heading-anchor" href="#De-novo-manifold-learning">De-novo manifold learning</a><a id="De-novo-manifold-learning-1"></a><a class="docs-heading-anchor-permalink" href="#De-novo-manifold-learning" title="Permalink"></a></h2><p>All considerations taken together, we wish to formulate an unsupervised method for nonlinear, dimensional reduction. The natural choice for network architecture is thus an autoencoder <sup class="footnote-reference"><a id="citeref-5" href="#footnote-5">[5]</a></sup>, shown graphically below.</p><p align="center">
<img src="/seqspace/assets/autoencode/auto_encoder.svg" width="68%" class="center"/>
</p><p>However, in order to enforce the <em>continuum expression ansatz</em>, the neural network should also <em>explicitly</em> conserve the topology of our neighborhood graph; neighborhood rankings should be preserved under the identified mapping. Lastly, we constrain the learned latent representation to be uniformally sampled. This can be viewed as an additional assumption on top of our <em>continuum expression ansatz</em>. Specifically, we assume cells were sampled from the embryonic positions <em>uniformally</em>, i.e. without bias, and, as such, restrict our view to uniformally sampled parameterizations of the underlying expression manifold.</p><h3 id="Network-architecture"><a class="docs-heading-anchor" href="#Network-architecture">Network architecture</a><a id="Network-architecture-1"></a><a class="docs-heading-anchor-permalink" href="#Network-architecture" title="Permalink"></a></h3><p>The input to the neural network was the <code>35</code> <strong>statistically significant</strong> components determined in <a href="../normalize/#Drosophila-results">Drosophila results</a>. The encoder layer was designed to be 100 neurons wide and 6 levels deep with dense connections between each layer. In order to prevent overfitting and accelerate training of deep layers, batch normalization was utilized between each latent layer <sup class="footnote-reference"><a id="citeref-6" href="#footnote-6">[6]</a></sup>. Linear and exponential linear unit activation functions were chosen for the input/output and latent layers respectively. The decoder layer was taken to be mirror symmetric with respect to the encoding layer for simplicity. Training occurred on <span>$80\%$</span> of the data, batched into groups of <span>$128$</span> cells. Validation of the resultant network was performed on the remaining <span>$20\%$</span> to ensure parameters were not overfit. We note that the final results presented here were determined to not be highly sensitive to the specific architectural details outlined here.</p><h3 id="Topological-conservation"><a class="docs-heading-anchor" href="#Topological-conservation">Topological conservation</a><a id="Topological-conservation-1"></a><a class="docs-heading-anchor-permalink" href="#Topological-conservation" title="Permalink"></a></h3><p>In order to learn an interpretable latent space representation of the intrinsic gene expression manifold, we wish to constrain the estimated pullback to conserve the topology of the input scRNAseq data. This immediately poses the question: what topological features do we wish to preserve from the data to latent space and how do we empirically measure them? The answers immediately determine the additional terms one must add to the objective function used for training.</p><p>We opt to utilize an explicitly <em>geometric</em> formalism that will implicitly constrain <em>topology</em>. The intuition for this choice is guided by <em>Differential Geometry</em>: a metric tensor uniquely defines a distance function between any two points on a manifold; the topology induced by this distance function will always coincide with the original topology of the manifold. Thus, by imposing preservation of pairwise distances in the latent space relative to the data, we implicitly conserve topology. It is important to note that this assumes our original scRNAseq data is sampled from a metric space that we have access to. We note that there have been recent promising attempts at designing loss functions parameterized by explicit topological invariants formulated by <em>Topological Data Analysis</em>, e.g. persistent homology. Lastly, one could envision having each network layer operate on a simplicial complex, rather than a flat vector of real numbers, however it is unclear how to parameterize the feed-forward function.</p><h4 id="Isometric-formulation"><a class="docs-heading-anchor" href="#Isometric-formulation">Isometric formulation</a><a id="Isometric-formulation-1"></a><a class="docs-heading-anchor-permalink" href="#Isometric-formulation" title="Permalink"></a></h4><p>The most straightforward manner to preserve distances between the input data and the latent representation is to impose isometry, i.e. distances in both spaces quantitatively agree. This would be achieved by supplementing the objective function with an <em>Isomap</em> analog</p><p class="math-container">\[E_{iso} \sim \displaystyle\sum\limits_{\alpha,\beta} \left(D_{\alpha\beta} - \left|\left| \xi_\alpha - \xi_\beta \right|\right| \right)^2\]</p><p>Utilizing this term is problematic for several reasons:</p><ol><li>Large distances dominate the energetics and as such large-scale features of the intrinsic manifold will be preferentially fit.</li><li>Generically, <span>$d$</span> dimensional manifolds can not be isometrically embedded into <span>$\mathbb{R}^d$</span>, e.g. the sphere into the plane. This is seen empirically by the empirical three dimensional scaling but long tail of dimensions seen in the isomap correlation.</li><li>It trusts the computed distances quantitatively. We simply want close cells to be close in the resultant latent space.</li></ol><h4 id="Monometric-formulation"><a class="docs-heading-anchor" href="#Monometric-formulation">Monometric formulation</a><a id="Monometric-formulation-1"></a><a class="docs-heading-anchor-permalink" href="#Monometric-formulation" title="Permalink"></a></h4><p>Consider a vector <span>$\psi_\alpha$</span> of scores of length <span>$n$</span> we wish to rank. Furthermore, define <span>$\sigma \in \Sigma_n$</span> to be an arbitrary permutation of <span>$n$</span> such scores. We define the <strong>argsort</strong> to be the permutation that sorts <span>$\psi$</span> in descending order</p><p class="math-container">\[    \bar{\sigma}\left(\bm{\psi}\right) \equiv \left(\sigma_1\left(\bm{\psi}\right),...,\sigma_n\left(\bm{\psi}\right)\right) \qquad \text{such that} \qquad
    \psi_{\bar{\sigma}_1} \ge \psi_{\bar{\sigma}_1} \ge ... \ge \psi_{\bar{\sigma}_n}\]</p><p>The definition of the <strong>sorted</strong> vector of scores <span>$\bar{\bm{\psi}}_\alpha \equiv \psi_{\bar{\sigma}_\alpha}$</span> thus follows naturally. Lastly, the <strong>rank</strong> of vector <span>$\bm{\psi}$</span> is defined as the inverse permutation of <strong>argsort</strong>.</p><p class="math-container">\[    R\left(\bm{\psi}\right) \equiv \bar{\sigma}^{-1}\left(\bm{\psi}\right)\]</p><p>We wish to devise an objective function that contains functions of the rank of some latent space variables. However, <span>$R(\bm{\psi})$</span> is a non-differentiable function; it maps a vector in <span>$\mathbb{R}^n$</span> to a permutation of <span>$n$</span> items. Hence, we can not directly utilize the rank in a loss function as there is no way to backpropagate gradient information to the network parameters. In order to rectify this limitation, we first reformulate the ranking problem as a linear programming problem that permits efficient regularization. Note, the presentation here follows closely the original paper <sup class="footnote-reference"><a id="citeref-7" href="#footnote-7">[7]</a></sup></p><h5 id="Linear-program-formulation"><a class="docs-heading-anchor" href="#Linear-program-formulation">Linear program formulation</a><a id="Linear-program-formulation-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-program-formulation" title="Permalink"></a></h5><p>The <strong>sorting</strong> and <strong>ranking</strong> problem can be formulated as discrete optimization over the set of n-permutations <span>$\Sigma_n$</span></p><p class="math-container">\[    \bar{\sigma}\left(\bm{\psi}\right) \equiv \underset{\bm{\sigma}\in\Sigma_n}{\mathrm{argmax}} \ \displaystyle\sum\limits_{\alpha} \psi_{\sigma_\alpha} \rho_{\alpha}\]</p><p class="math-container">\[    R\left(\bm{\psi}\right)
    \equiv \bar{\sigma}\left(\bm{\psi}\right)^{-1} 
    \equiv \left[\underset{\bm{\sigma}\in\Sigma_n}{\mathrm{argmax}} \ \displaystyle\sum\limits_{\alpha} \psi_{\sigma_\alpha} \rho_{\alpha} \right]^{-1}
    \equiv \left[\underset{\bm{\sigma^{-1}}\in\Sigma_n}{\mathrm{argmax}} \ \displaystyle\sum\limits_{\alpha} \psi_{\alpha} \rho_{\sigma^{-1}_\alpha} \right]^{-1}
    \equiv \underset{\bm{\pi}\in\Sigma_n}{\mathrm{argmax}} \ \displaystyle\sum\limits_{\alpha} \psi_\alpha \rho_{\pi(\alpha)}\]</p><p>where <span>$\rho_\alpha \equiv \left(n, n-1, ..., 1\right)$</span> In order to regularize the problem, and thus allow for continuous optimization, we imagine the convex hull of all permutations induced by an arbitrary vector <span>$\bm{\omega} \in \mathbb{R}^n$</span>.</p><p class="math-container">\[    \Omega\left(\bm{\omega}\right) \equiv \text{convhull}\left[\left\{\bm{\omega}_{\sigma_\alpha}: \sigma \in \Sigma_n \right\}\right] \subset \mathbb{R}^n\]</p><p>This is often referred to as the <em>permutahedron</em> of <span>$\bm{\omega}$</span>; it is a convex polytope in n-dimensions whose vertices are the permutations of <span>$\bm{\omega}$</span> It follows directly from the fundamental theorem of linear programming, that the solution will almost surely be achieved at the vertex. Thus the above discrete formulation can be rewritten as an optimization over continuous vectors contained on the <em>permutahedron</em></p><p class="math-container">\[    \bm{\psi}_{\bar{\sigma}\left(\bm{\psi}\right)} \equiv \underset{\bm{\omega}\in\Omega\left(\bm{\psi}\right)}{\mathrm{argmax}} \ \bm{\omega}\cdot\bm{\rho}
    \qquad
    \bm{\rho}_{R\left(\bm{\psi}\right)} \equiv \underset{\bm{\omega}\in\Omega\left(\bm{\rho}\right)}{\mathrm{argmax}} \ \bm{\psi}\cdot\bm{\omega}\]</p><p>Utilizing the fact that <span>$\rho_{R\left(\bm{\psi}\right)} = R\left(-\bm{\psi}\right)$</span></p><p class="math-container">\[    R\left(\bm{\psi}\right) \equiv -\underset{\bm{\omega}\in\Omega\left(\bm{\rho}\right)}{\mathrm{argmax}} \ \bm{\psi}\cdot\bm{\omega}\]</p><p>Unfortunately, since <span>$\bm{\psi}$</span> appears in the <strong>rank</strong> objective function, any small perturbation in <span>$\bm{\psi}$</span> can force the solution of the linear program to discontinuously transition to another vertex. As such, in its current form, it is still not differentiable. Note, this is not true for the sorted vector, it appears in the constraint polyhedron; it has a unique Jacobian and can be directly used in neural networks. The only way to proceed is to introduce convex regularization.</p><h5 id="Regularization"><a class="docs-heading-anchor" href="#Regularization">Regularization</a><a id="Regularization-1"></a><a class="docs-heading-anchor-permalink" href="#Regularization" title="Permalink"></a></h5><p>We revise our objective function by Euclidean projection and thus introduce quadratic regularization on the norm of the solution. Specifically, we define the <strong>soft rank</strong> operators as the extrema of the objective function</p><p class="math-container">\[    \tilde{R}\left(\bm{\psi}\right) \equiv \underset{\bm{\omega}\in\Omega\left(\bm{\rho}\right)}{\mathrm{argmax}} \left[ -\bm{\psi}\cdot\bm{\omega}
    - \frac{\epsilon}{2}\left|\left|\omega\right|\right|^2 \right]\]</p><p>Note that the limit <span>$\epsilon \rightarrow 0$</span> reproduces the linear programming formulation of the rank operator introduced above. Conversely, in the limit <span>$\epsilon \rightarrow \infty$</span>, the solution will go to a constant vector that has the smallest modulus on the <em>permutahedron</em>.</p><h5 id="Solution"><a class="docs-heading-anchor" href="#Solution">Solution</a><a id="Solution-1"></a><a class="docs-heading-anchor-permalink" href="#Solution" title="Permalink"></a></h5><p>It has been demonstrated before that the above problem reduces to simple isotonic regression<sup class="footnote-reference"><a id="citeref-7" href="#footnote-7">[7]</a></sup><sup class="footnote-reference"><a id="citeref-8" href="#footnote-8">[8]</a></sup>. Specifically,</p><p class="math-container">\[R\left(\bm{\psi}\right) = -\frac{\bm{\psi}}{\epsilon} -
    \left[\underset{\omega_1 \ge \omega_2 \ge ... \ge \omega_n}{\mathrm{argmin}}
    \frac{1}{2} \left|\left|\bm{\omega} + \bm{\rho} + \frac{\bm{\psi}}{\epsilon} \right|\right|^2\right]_{\sigma^{-1}(\bm{\psi})}
\equiv -\frac{\bm{\psi}}{\epsilon} - \tilde{\bm{\omega}}\left(\bm{\psi},\bm{\rho}\right)\]</p><p>Importantly, isotonic regression is well-studied and can be solved in linear time. Furthermore, the solution admits a simple, calculatable Jacobian</p><p class="math-container">\[\partial_{\psi_\alpha} R_\beta\left(\bm{\psi}\right)
= \frac{-\delta_{\alpha\beta}}{\epsilon} - \partial_{\psi_\alpha}\tilde{\omega_\beta}\left(\bm{\psi},\bm{\rho}\right)
= \frac{-\delta_{\alpha\beta}}{\epsilon} - 
    \begin{pmatrix}
    \bm{B}_1 &amp; \bm{0} &amp; \bm{0} \\
    \bm{0}   &amp; \ddots &amp; \bm{0} \\
    \bm{0}   &amp; \bm{0} &amp; \bm{B}_m \\
    \end{pmatrix}_{\alpha\beta}\]</p><p>where <span>$\bm{B}_i$</span> denotes the matrix corresponding to the <span>$i^{th}$</span> block obtained during isotonic regression. It is a constant matrix whose number of rows and columns equals the size of the block, and whose values all sum to 1. The Julia implementation can be found in the <a href="sci/@ref">Differentiable rank</a> section.</p><h4 id="Loss-function"><a class="docs-heading-anchor" href="#Loss-function">Loss function</a><a id="Loss-function-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-function" title="Permalink"></a></h4><p>The term of the loss function enforcing monometricity is thus taken to be</p><p class="math-container">\[    E_{x} \equiv \frac{\sum\limits_{\alpha\beta} \left(R\left(D_{\alpha\beta}^2\right) - R\left(|\vec{\psi}_{\alpha}-\vec{\psi}_{\beta}|^2\right)\right)^2}{\sqrt{\sum\limits_{\alpha\beta}R^2\left(D_{\alpha\beta}^2\right)\sum\limits_{\alpha\beta} R^2\left(|\vec{\psi}_{\alpha}-\vec{\psi}_{\beta}|^2\right)}}\]</p><h3 id="Uniform-sampling-of-latent-space"><a class="docs-heading-anchor" href="#Uniform-sampling-of-latent-space">Uniform sampling of latent space</a><a id="Uniform-sampling-of-latent-space-1"></a><a class="docs-heading-anchor-permalink" href="#Uniform-sampling-of-latent-space" title="Permalink"></a></h3><p>In order to learn an interpretable latent space representation of the intrinsic gene expression manifold, we also wish to constrain the estimated pullback to uniformally sample the latent space. When posed in the language of Optimal Transport, this is equivalent to a semi-discrete formulation of the Wasserstein distance, in which the point cloud in <span>$d$</span> dimensions is mapped optimally to <span>$[0,1]^d$</span>. It can be shown <sup class="footnote-reference"><a id="citeref-9" href="#footnote-9">[9]</a></sup> that this is equivalent to integrating the moment of inertia over the equal area power diagram in <span>$d$</span> dimensions, a computationally intensive procedure as the latent dimension grows.</p><p>We seek a balance between analytical correctness and computational tractability. To this end, we forgo imposing that the full d-dimensional joint distribution is uniform; instead we impose that the one-dimensional marginals over each independent latent dimension is uniform. For one-dimensional distributions, the Wasserstein metric has a unique analytical expression in terms of the empirical cumulative density function <span>$F(x)$</span> and thus can be computed in linear time <sup class="footnote-reference"><a id="citeref-10" href="#footnote-10">[10]</a></sup>. As such, the term in the loss function enforcing latent uniformity is taken to be</p><p class="math-container">\[    E_u \equiv \frac{1}{d}\displaystyle\sum\limits_d \frac{1}{N} \displaystyle\sum\limits_{\alpha} \left| F\left(\Phi^{-1}\left(\vec{z}_{\alpha}\right)_d\right) - R\left(\Phi^{-1}\left(\vec{z}_{\alpha}\right)_d\right)\right|\]</p><h3 id="Network-objective-function"><a class="docs-heading-anchor" href="#Network-objective-function">Network objective function</a><a id="Network-objective-function-1"></a><a class="docs-heading-anchor-permalink" href="#Network-objective-function" title="Permalink"></a></h3><p>Putting all terms together, we arrive at our final formulation of the loss function (where <span>$\Phi$</span> and <span>$\Phi^{-1}$</span> denote the decoder and encoder stages respectively).</p><p class="math-container">\[E \equiv \frac{\displaystyle\sum\limits_{\alpha} \left|\vec{z}_{\alpha} - \Phi\left(\Phi^{-1}\left(\vec{z}_\alpha\right)\right) \right|^2}{\displaystyle\sum\limits_{\alpha} |\vec{z}_{\alpha}|^2} + \lambda_x E_{x} + \lambda_u E_{u}\]</p><p>Stated plainly, all three terms, in left-to-right order, seek to find a low-dimensional map that is (i) invertible, (ii) preserves topological neighborhood relationships, (iii) is uniformally sampled by our dataset. For all results shown below, we chose <span>$\lambda_x=\lambda_u=1$</span> for simplicity; exploration of hyperparameter optimization would be an interesting future direction.</p><h3 id="Swiss-roll-validation"><a class="docs-heading-anchor" href="#Swiss-roll-validation">Swiss roll validation</a><a id="Swiss-roll-validation-1"></a><a class="docs-heading-anchor-permalink" href="#Swiss-roll-validation" title="Permalink"></a></h3><p>Before continuing with analysis of the <em>Drosophila melongaster</em> analysis, we first verify our proposed novel manifold learning technique on a canonical surface, the two dimensional swiss roll, shown below.</p><p align="center">
<img src="/seqspace/assets/swissroll/inputdata.png" width="48%" class="center"/>
<img src="/seqspace/assets/swissroll/loss.png" width="48%" class="center"/>
</p><p>Specifically, we generated a 2D swiss roll, embedded in <span>$30$</span> dimensions, with <span>$1\%$</span> random Gaussian noise added to each component. The neural network, as specified in <a href="#Network-objective-function">Network objective function</a>, was minimized until convergence (shown above). The resultant 2D latent space, the output of the encoding stage, is shown below after a simple affine transformation to align the <span>$\phi$</span> axis along the horizontal axis. Importantly, the uniformization constraint results in an approximately <em>linear</em> relationship between the known latent variables used to generate the 2D surface, and the latent parameterization variables.</p><p align="center">
<img src="/seqspace/assets/swissroll/latent.png" width="48%" class="center"/>
<img src="/seqspace/assets/swissroll/latent_correlation.png" width="48%" class="center"/>
</p><p>Critically, the output of the decoder stage correlates with the input data such that <span>$R^2 \sim .98$</span>. This held true for 10 independent trained networks with no observed large-scale non-linearity left in the latent space. We conclude that our construction accurately recapitulates the 2D intrinsic parameterization of the toy data.</p><h2 id="Drosophila-manifold"><a class="docs-heading-anchor" href="#Drosophila-manifold">Drosophila manifold</a><a id="Drosophila-manifold-1"></a><a class="docs-heading-anchor-permalink" href="#Drosophila-manifold" title="Permalink"></a></h2><p>We now show the results of the minimization of the energy specified in <a href="#Network-objective-function">Network objective function</a> using the architecture outlined in <a href="#Network-architecture">Network architecture</a>, with three latent dimensions. Below, we show a scatter plot of all scRNAseq cells transformed to the learned latent representation. The left and right plots are colored according to the estimated AP and DV positions respectively, as obtained by <a href="../inference/#scRNAseq-spatial-inference">scRNAseq spatial inference</a>. We emphasize here that we <em>did not</em> train on the spatial positions, but rather the continuity of space directly follows from imposing the continuity of gene expression in the latent space. Additionally, we did not constrain AP and DV axes, shown as red and green arrows in the triad, to be orthogonal. Rather, we simply looked for the directions in the latent space that <em>maximally correlate</em> with our estimated position. The resultant two directions have negligible overlap. The input data and the output of the decoder stage correlate such that <span>$R^2 \sim .68$</span>, implying we can capture roughly <span>$\sim 70 \%$</span> of the variance observed in normalized gene expression with just <em>three</em> latent dimensions.</p><p align="center">
<img src="/seqspace/assets/autoencode/AP_colormap.png" width="48%" class="center"/>
<img src="/seqspace/assets/autoencode/DV_colormap.png" width="48%" class="center"/>
</p><p>We observe good <em>linear</em> agreement with our estimated AP/DV directions in the latent space and the estimated spatial position for each cell, as shown above. Our ability to resolve AP position is limited to a precision of roughly <span>$.08$</span>mm, much larger than the cellular resolution obtained utilizing FISH data of the 4 gap genes <sup class="footnote-reference"><a id="citeref-11" href="#footnote-11">[11]</a></sup>. The causal driver behind this is likely multifaceted: (i) scRNAseq is fundamentally noisy (ii) we have a relatively small sized dataset, (iii) our estimates of embryonic position themselves are noisy values. It would be interesting to revisit this analysis leveraging a higher sequencing depth per cell allowable with modern scRNAseq techniques or utilizing novel spatial-omics to provide more exact positional values. Interestingly, our estimated DV axis appears to be qualitatively nonlinear with strongest predictive power at the poles and weak relationship in the bulk.</p><p>The benefit of using an autoencoder network is that its is both generalizable and generative: we ultimately have a function that maps scRNAseq to a latent space and back out to the original expression domain. For example, we can &quot;generate&quot; novel scRNAseq data from our interpolated model by sampling points in the latent space and transforming the points through the decoder stage. More importantly, this allows us to directly leverage tools from vector calculus to estimate volumes of phase space. As shown in the below right figure, we observe both the anterior and posterior poles contain more &quot;volume&quot; of the gene expression manifold for a unit volume of physical space when compared to cells sampled from the lateral region. Assuming a constant density of states over the intrinsic gene expression manifold, this implies a higher density of cell fates at the head and tail of the embryo than the lateral ectoderm. This is in contrast to previous analyses done on the AP pattern of the 4 gap genes over the mid-sadigittal plane of the embryo which predicted a constant positional information <sup class="footnote-reference"><a id="citeref-11" href="#footnote-11">[11]</a></sup>. We note that this is an interesting direction for future study.</p><p align="center">
<img src="/seqspace/assets/autoencode/coordinate_scatter.png" width="48%" class="center"/>
<img src="/seqspace/assets/autoencode/positional_info.png" width="48%" class="center"/>
</p><p>An interesting question to consider for future studies: what is the third latent dimension we discover within the scRNAseq data? We analyzed the genes that maximally correlate (negatively or positively) with this direction in the latent space, as shown by the blue arrow above. The below table captures some examples of the top hits for both signs. Theses genes are putatively ubiquitously expressed, consistent with the notion that this is an orthogonal axis to the directions of spatial variation. We postulate this direction is involved in the cell cycle as <em>RNApolymerase</em>, <em>myosin light chain</em>, and other mitotic markers form on &quot;pole&quot; of the axis, while <em>ed</em> and other negative growth regulators are enriched on the conjugate pole.</p><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a><a href="https://www.science.org/doi/10.1126/science.290.5500.2319">A Global Geometric Framework for Nonlinear Dimensionality Reduction</a></li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a><a href="https://dl.acm.org/doi/pdf/10.1145/367766.368168">Algorithm 97: shortest path</a></li><li class="footnote" id="footnote-3"><a class="tag is-link" href="#citeref-3">3</a><a href="https://link.springer.com/article/10.1007/BF01386390">A note on two problems in connexion with graphs</a></li><li class="footnote" id="footnote-4"><a class="tag is-link" href="#citeref-4">4</a><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.359.7343&amp;rep=rep1&amp;type=pdf">Graph approximations to geodesics on embedded manifolds</a></li><li class="footnote" id="footnote-5"><a class="tag is-link" href="#citeref-5">5</a><a href="https://www.aaai.org/Library/AAAI/1987/aaai87-050.php">Modular learning in neural networks</a></li><li class="footnote" id="footnote-6"><a class="tag is-link" href="#citeref-6">6</a><a href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>.</li><li class="footnote" id="footnote-7"><a class="tag is-link" href="#citeref-7">7</a><a href="https://arxiv.org/abs/2002.08871">Fast Differentiable Sorting and Ranking</a></li><li class="footnote" id="footnote-8"><a class="tag is-link" href="#citeref-8">8</a><a href="https://arxiv.org/abs/1802.04223">SparseMAP: Differentiable Sparse Structured Inference</a></li><li class="footnote" id="footnote-9"><a class="tag is-link" href="#citeref-9">9</a><a href="https://link.springer.com/book/10.1007/978-3-319-20828-2">Optimal Transport for Applied Mathematicians</a></li><li class="footnote" id="footnote-10"><a class="tag is-link" href="#citeref-10">10</a><a href="https://epubs.siam.org/doi/10.1137/1118101">Calculation of the Wasserstein Distance Between Probability Distributions on the Line</a></li><li class="footnote" id="footnote-11"><a class="tag is-link" href="#citeref-11">11</a><a href="https://www.pnas.org/doi/10.1073/pnas.1315642110">Positional information, in bits</a></li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../inference/">« scRNAseq spatial inference</a><a class="docs-footer-nextpage" href="../../lib/distance/">Distances »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.12 on <span class="colophon-date" title="Tuesday 1 March 2022 15:54">Tuesday 1 March 2022</span>. Using Julia version 1.7.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
