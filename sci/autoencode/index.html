<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Manifold learning · SeqSpace.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><script src="../../../copy.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">SeqSpace.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Science</span><ul><li><a class="tocitem" href="../normalize/">scRNAseq Normalization</a></li><li><a class="tocitem" href="../inference/">scRNAseq Spatial Inference</a></li><li class="is-active"><a class="tocitem" href>Manifold learning</a><ul class="internal"><li><a class="tocitem" href="#Introduction"><span>Introduction</span></a></li><li><a class="tocitem" href="#Architecture"><span>Architecture</span></a></li><li><a class="tocitem" href="#Results"><span>Results</span></a></li></ul></li></ul></li><li><span class="tocitem">Library</span><ul><li><a class="tocitem" href="../../lib/distance/">Distances</a></li><li><a class="tocitem" href="../../lib/generate/">Point Cloud Generation</a></li><li><a class="tocitem" href="../../lib/infer/">Supervised Spatial Inference</a></li><li><a class="tocitem" href="../../lib/io/">Data Input/Output</a></li><li><a class="tocitem" href="../../lib/manifold/">Point Cloud Differential Geometry</a></li><li><a class="tocitem" href="../../lib/model/">Autoencoder Manifold Learning</a></li><li><a class="tocitem" href="../../lib/normalize/">scRNAseq Normalization</a></li><li><a class="tocitem" href="../../lib/pointcloud/">Point Cloud</a></li><li><a class="tocitem" href="../../lib/queue/">Priority Queue</a></li><li><a class="tocitem" href="../../lib/rank/">Differentiable Rank</a></li><li><a class="tocitem" href="../../lib/scrna/">scRNAseq Data</a></li><li><a class="tocitem" href="../../lib/util/">Utilities</a></li><li><a class="tocitem" href="../../lib/voronoi/">Voronoi</a></li></ul></li><li><span class="tocitem">Command Line</span><ul><li><a class="tocitem" href="../../cli/drosophila/">Drosophila pipeline</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Science</a></li><li class="is-active"><a href>Manifold learning</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Manifold learning</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com//blob/master/docs/src/sci/autoencode.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Manifold-learning"><a class="docs-heading-anchor" href="#Manifold-learning">Manifold learning</a><a id="Manifold-learning-1"></a><a class="docs-heading-anchor-permalink" href="#Manifold-learning" title="Permalink"></a></h1><h2 id="Introduction"><a class="docs-heading-anchor" href="#Introduction">Introduction</a><a id="Introduction-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction" title="Permalink"></a></h2><p>Want:</p><ul><li>Dimensional reduction</li><li>Nonlinear</li><li>Differentiable</li><li>Generalizable</li><li>Unsupervised</li></ul><p>Natural choice for an autoencoder. Utilize known positional labels as a validation step. Not used for training purposes.</p><h2 id="Architecture"><a class="docs-heading-anchor" href="#Architecture">Architecture</a><a id="Architecture-1"></a><a class="docs-heading-anchor-permalink" href="#Architecture" title="Permalink"></a></h2><p>How to pick depth? Width? Worry about overfitting: enter dropout and batch normalization. Vanilla autoencoder: latent space is not readily interpretable.</p><h3 id="Topological-conservation"><a class="docs-heading-anchor" href="#Topological-conservation">Topological conservation</a><a id="Topological-conservation-1"></a><a class="docs-heading-anchor-permalink" href="#Topological-conservation" title="Permalink"></a></h3><p>In order to learn an interpretable latent space representation of the intrinsic gene expression manifold, we wish to constrain the estimated pullback to conserve the topology of the input scRNAseq data. This immediately poses the question: what topological features do we wish to preserve from the data to latent space and how do we empirically measure them? The answers immediately determine the additional terms one must add to the objective function used for training.</p><p>We opt to utilize an explicitly <em>geometric</em> formalism that will implicitly constrain <em>topology</em>. The intuition for this choice is guided by <em>Differential Geometry</em>: a metric tensor uniquely defines a distance function between any two points on a manifold; the topology induced by this distance function will always coincide with the original topology of the manifold. Thus, by imposing preservation of pairwise distances in the latent space relative to the data, we implicitly conserve topology. It is important to note that this assumes our original scRNAseq data is sampled from a metric space that we have access to. We note that there have been recent promising attempts at designing loss functions parameterized by explicit topological invariants formulated by <em>Topological Data Analysis</em>, e.g. persistent homology. Lastly, one could envision having each network layer operate on a simplicial complex, rather than a flat vector of real numbers, however it is unclear how to parameterize the feed-forward function.</p><p>Thus the first task is to formulate an algorithm to approximate the metric space the point cloud is sampled from and subsequently utilize our estimate to compute all pairwise distances. Again we proceed guided by intuition gleaned from <em>Differential Geometry</em>: pairwise distances within local neighborhoods are expected to be well-described by a Euclidean metric in the tangent space. Conversely, macroscopic distances can only be computed via integration against the underlying metric along the corresponding geodesic. As such, we first estimate the local tangent space of our input data by computing pairwise distances within local neighborhoods around each point, either defined by a fixed radius or fixed number of neighbors. This defines a sparse, undirected graph in which edges only exist within our estimated tangent spaces and are weighted by the euclidean distance within the embedded space. The resultant neighborhood graph serves as the basis for many dimensional reduction algorithms, such as <strong>Isomap</strong>, <strong>UMAP</strong> and <strong>tSNE</strong>. Pairwise distances between <em>any</em> two points in the original dataset can then be found by simple graph traversal to find the shortest possible path between two graph vertices, the discrete analog of a continuum geodesic. It has been shown that the distance estimated by this algorithm asymptotically approaches the true distance as the number of data points sampled increases. We denote <span>$D_{\alpha\beta}$</span> as the resultant pairwise distances between cell <span>$\alpha,\beta$</span>.</p><h4 id="Isometric-formulation"><a class="docs-heading-anchor" href="#Isometric-formulation">Isometric formulation</a><a id="Isometric-formulation-1"></a><a class="docs-heading-anchor-permalink" href="#Isometric-formulation" title="Permalink"></a></h4><p>The most straightforward manner to preserve distances between the input data and the latent representation is to impose isometry, i.e. distances in both spaces quantitatively agree. This would be achieved by supplementing the objective function with the term</p><p class="math-container">\[E_{iso} = \displaystyle\sum\limits_{\alpha,\beta} \left(D_{\alpha\beta} - \left|\left| \xi_\alpha - \xi_\beta \right|\right| \right)^2\]</p><p>Utilizing this term is problematic for several reasons:</p><ol><li>Large distances dominate the energetics and as such large-scale features of the intrinsic manifold will be preferentially fit.</li><li>Generically, <span>$d$</span> dimensional manifolds can not be isometrically embedded into <span>$\mathbb{R}^d$</span>, e.g. the sphere into the plane.</li><li>It trusts the computed distances quantitatively. We simply want close cells to be close in the resultant latent space.</li></ol><h4 id="Differentiable-ranking"><a class="docs-heading-anchor" href="#Differentiable-ranking">Differentiable ranking</a><a id="Differentiable-ranking-1"></a><a class="docs-heading-anchor-permalink" href="#Differentiable-ranking" title="Permalink"></a></h4><p>Consider a vector <span>$\psi_\alpha$</span> of scores of length <span>$n$</span> we wish to rank. Furthermore, define <span>$\sigma \in \Sigma_n$</span> to be an arbitrary permutation of <span>$n$</span> such scores. We define the <strong>argsort</strong> to be the permutation that sorts <span>$\psi$</span> in descending order</p><p class="math-container">\[    \bar{\sigma}\left(\bm{\psi}\right) \equiv \left(\sigma_1\left(\bm{\psi}\right),...,\sigma_n\left(\bm{\psi}\right)\right) \qquad \text{such that} \qquad
    \psi_{\bar{\sigma}_1} \ge \psi_{\bar{\sigma}_1} \ge ... \ge \psi_{\bar{\sigma}_n}\]</p><p>The definition of the <strong>sorted</strong> vector of scores <span>$\bar{\bm{\psi}}_\alpha \equiv \psi_{\bar{\sigma}_\alpha}$</span> thus follows naturally. Lastly, the <strong>rank</strong> of vector <span>$\bm{\psi}$</span> is defined as the inverse permutation of <strong>argsort</strong>.</p><p class="math-container">\[    R\left(\bm{\psi}\right) \equiv \bar{\sigma}^{-1}\left(\bm{\psi}\right)\]</p><p>We wish to devise an objective function that contains functions of the rank of some latent space variables. However, <span>$R(\bm{\psi})$</span> is a non-differentiable function; it maps a vector in <span>$\mathbb{R}^n$</span> to a permutation of <span>$n$</span> items. Hence, we can not directly utilize the rank in a loss function as there is no way to backpropagate gradient information to the network parameters. In order to rectify this limitation, we first reformulate the ranking problem as a linear programming problem that permits efficient regularization. Note, the presentation here follows closely the original paper <sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup></p><h5 id="Linear-program-formulation"><a class="docs-heading-anchor" href="#Linear-program-formulation">Linear program formulation</a><a id="Linear-program-formulation-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-program-formulation" title="Permalink"></a></h5><p>The <strong>sorting</strong> and <strong>ranking</strong> problem can be formulated as discrete optimization over the set of n-permutations <span>$\Sigma_n$</span></p><p class="math-container">\[    \bar{\sigma}\left(\bm{\psi}\right) \equiv \underset{\bm{\sigma}\in\Sigma_n}{\mathrm{argmax}} \ \displaystyle\sum\limits_{\alpha} \psi_{\sigma_\alpha} \rho_{\alpha}\]</p><p class="math-container">\[    R\left(\bm{\psi}\right)
    \equiv \bar{\sigma}\left(\bm{\psi}\right)^{-1} 
    \equiv \left[\underset{\bm{\sigma}\in\Sigma_n}{\mathrm{argmax}} \ \displaystyle\sum\limits_{\alpha} \psi_{\sigma_\alpha} \rho_{\alpha} \right]^{-1}
    \equiv \left[\underset{\bm{\sigma^{-1}}\in\Sigma_n}{\mathrm{argmax}} \ \displaystyle\sum\limits_{\alpha} \psi_{\alpha} \rho_{\sigma^{-1}_\alpha} \right]^{-1}
    \equiv \underset{\bm{\pi}\in\Sigma_n}{\mathrm{argmax}} \ \displaystyle\sum\limits_{\alpha} \psi_\alpha \rho_{\pi(\alpha)}\]</p><p>where <span>$\rho_\alpha \equiv \left(n, n-1, ..., 1\right)$</span> In order to regularize the problem, and thus allow for continuous optimization, we imagine the convex hull of all permutations induced by an arbitrary vector <span>$\bm{\omega} \in \mathbb{R}^n$</span>.</p><p class="math-container">\[    \Omega\left(\bm{\omega}\right) \equiv \text{convhull}\left[\left\{\bm{\omega}_{\sigma_\alpha}: \sigma \in \Sigma_n \right\}\right] \subset \mathbb{R}^n\]</p><p>This is often referred to as the <em>permutahedron</em> of <span>$\bm{\omega}$</span>; it is a convex polytope in n-dimensions whose vertices are the permutations of <span>$\bm{\omega}$</span> It follows directly from the fundamental theorem of linear programming, that the solution will almost surely be achieved at the vertex. Thus the above discrete formulation can be rewritten as an optimization over continuous vectors contained on the <em>permutahedron</em></p><p class="math-container">\[    \bm{\psi}_{\bar{\sigma}\left(\bm{\psi}\right)} \equiv \underset{\bm{\omega}\in\Omega\left(\bm{\psi}\right)}{\mathrm{argmax}} \ \bm{\omega}\cdot\bm{\rho}
    \qquad
    \bm{\rho}_{R\left(\bm{\psi}\right)} \equiv \underset{\bm{\omega}\in\Omega\left(\bm{\rho}\right)}{\mathrm{argmax}} \ \bm{\psi}\cdot\bm{\omega}\]</p><p>Utilizing the fact that <span>$\rho_{R\left(\bm{\psi}\right)} = R\left(-\bm{\psi}\right)$</span></p><p class="math-container">\[    R\left(\bm{\psi}\right) \equiv -\underset{\bm{\omega}\in\Omega\left(\bm{\rho}\right)}{\mathrm{argmax}} \ \bm{\psi}\cdot\bm{\omega}\]</p><p>Unfortunately, since <span>$\bm{\psi}$</span> appears in the <strong>rank</strong> objective function, any small perturbation in <span>$\bm{\psi}$</span> can force the solution of the linear program to discontinuously transition to another vertex. As such, in its current form, it is still not differentiable. Note, this is not true for the sorted vector, it appears in the constraint polyhedron; it has a unique Jacobian and can be directly used in neural networks. The only way to proceed is to introduce convex regularization.</p><h5 id="Regularization"><a class="docs-heading-anchor" href="#Regularization">Regularization</a><a id="Regularization-1"></a><a class="docs-heading-anchor-permalink" href="#Regularization" title="Permalink"></a></h5><p>We revise our objective function by Euclidean projection and thus introduce quadratic regularization on the norm of the solution. Specifically, we define the <strong>soft rank</strong> operators as the extrema of the objective function</p><p class="math-container">\[    \tilde{R}\left(\bm{\psi}\right) \equiv \underset{\bm{\omega}\in\Omega\left(\bm{\rho}\right)}{\mathrm{argmax}} \left[ -\bm{\psi}\cdot\bm{\omega}
    - \frac{\epsilon}{2}\left|\left|\omega\right|\right|^2 \right]\]</p><p>Note that the limit <span>$\epsilon \rightarrow 0$</span> reproduces the linear programming formulation of the rank operator introduced above. Conversely, in the limit <span>$\epsilon \rightarrow \infty$</span>, the solution will go to a constant vector that has the smallest modulus on the <em>permutahedron</em>.</p><h5 id="Solution"><a class="docs-heading-anchor" href="#Solution">Solution</a><a id="Solution-1"></a><a class="docs-heading-anchor-permalink" href="#Solution" title="Permalink"></a></h5><p>It has been demonstrated before that the above problem reduces to simple isotonic regression<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup><sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup>. Specifically,</p><p class="math-container">\[R\left(\bm{\psi}\right) = -\frac{\bm{\psi}}{\epsilon} -
    \left[\underset{\omega_1 \ge \omega_2 \ge ... \ge \omega_n}{\mathrm{argmin}}
    \frac{1}{2} \left|\left|\bm{\omega} + \bm{\rho} + \frac{\bm{\psi}}{\epsilon} \right|\right|^2\right]_{\sigma^{-1}(\bm{\psi})}
\equiv -\frac{\bm{\psi}}{\epsilon} - \tilde{\bm{\omega}}\left(\bm{\psi},\bm{\rho}\right)\]</p><p>Importantly, isotonic regression is well-studied and can be solved in linear time. Furthermore, the solution admits a simple, calculatable Jacobian</p><p class="math-container">\[\partial_{\psi_\alpha} R_\beta\left(\bm{\psi}\right)
= \frac{-\delta_{\alpha\beta}}{\epsilon} - \partial_{\psi_\alpha}\tilde{\omega_\beta}\left(\bm{\psi},\bm{\rho}\right)
= \frac{-\delta_{\alpha\beta}}{\epsilon} - 
    \begin{pmatrix}
    \bm{B}_1 &amp; \bm{0} &amp; \bm{0} \\
    \bm{0}   &amp; \ddots &amp; \bm{0} \\
    \bm{0}   &amp; \bm{0} &amp; \bm{B}_m \\
    \end{pmatrix}_{\alpha\beta}\]</p><p>where <span>$\bm{B}_i$</span> denotes the matrix corresponding to the <code>i^{th}</code> block obtained during isotonic regression. It is a constant matrix whose number of rows and columns equals the size of the block, and whose values all sum to 1.</p><h4 id="Loss-function"><a class="docs-heading-anchor" href="#Loss-function">Loss function</a><a id="Loss-function-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-function" title="Permalink"></a></h4><h3 id="Uniform-sampling-of-latent-space"><a class="docs-heading-anchor" href="#Uniform-sampling-of-latent-space">Uniform sampling of latent space</a><a id="Uniform-sampling-of-latent-space-1"></a><a class="docs-heading-anchor-permalink" href="#Uniform-sampling-of-latent-space" title="Permalink"></a></h3><h2 id="Results"><a class="docs-heading-anchor" href="#Results">Results</a><a id="Results-1"></a><a class="docs-heading-anchor-permalink" href="#Results" title="Permalink"></a></h2><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a><a href="https://arxiv.org/abs/2002.08871">Fast Differentiable Sorting and Ranking</a></li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a><a href="https://arxiv.org/abs/1802.04223">SparseMAP: Differentiable Sparse Structured Inference</a></li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../inference/">« scRNAseq Spatial Inference</a><a class="docs-footer-nextpage" href="../../lib/distance/">Distances »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.12 on <span class="colophon-date" title="Wednesday 23 February 2022 13:32">Wednesday 23 February 2022</span>. Using Julia version 1.7.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
