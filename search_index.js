var documenterSearchIndex = {"docs":
[{"location":"lib/io/#Data-Input/Output","page":"Data Input/Output","title":"Data Input/Output","text":"","category":"section"},{"location":"lib/io/#Types","page":"Data Input/Output","title":"Types","text":"","category":"section"},{"location":"lib/io/","page":"Data Input/Output","title":"Data Input/Output","text":"Modules = [SeqSpace.DataIO]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/io/#SeqSpace.DataIO.PLYProp","page":"Data Input/Output","title":"SeqSpace.DataIO.PLYProp","text":"struct PLYProp\n    name :: Symbol\n    type :: Type\n    len  :: Union{Type,Nothing}\nend\n\nData structure that encapsulates a property of a PLY file. If len is not nothing, it is assumed to be a collection of properties, i.e. a list.\n\n\n\n\n\n","category":"type"},{"location":"lib/io/#SeqSpace.DataIO.plytype","page":"Data Input/Output","title":"SeqSpace.DataIO.plytype","text":"const plytype = Dict{String, Type}\n\nLookup table for keywords of PLY file format matched to type information.\n\n\n\n\n\n","category":"constant"},{"location":"lib/io/#Functions","page":"Data Input/Output","title":"Functions","text":"","category":"section"},{"location":"lib/io/","page":"Data Input/Output","title":"Data Input/Output","text":"Modules = [SeqSpace.DataIO]\nOrder = [:function]","category":"page"},{"location":"lib/io/#SeqSpace.DataIO.expand_matrix-Tuple{IO, String}","page":"Data Input/Output","title":"SeqSpace.DataIO.expand_matrix","text":"expand_matrix(io::IO, dir::String)\n\nRead a matrix in MTX format from io stream and dump as 3 text files into directory dir.\n\n\n\n\n\n","category":"method"},{"location":"lib/io/#SeqSpace.DataIO.fill!-Tuple{Array{SeqSpace.DataIO.PLYProp}, IO}","page":"Data Input/Output","title":"SeqSpace.DataIO.fill!","text":"fill!(prop::Array{PLYProp}, io::IO)\n\nRead and parse a single line from io stream. Fill the interpreted property into prop.\n\n\n\n\n\n","category":"method"},{"location":"lib/io/#SeqSpace.DataIO.read_matrix-Tuple{IO}","page":"Data Input/Output","title":"SeqSpace.DataIO.read_matrix","text":"read_matrix(io::IO; type=Float64, named_cols=false, named_rows=false, start_cols=1, start_rows=1)\n\nRead a white-space delimited matrix from io stream of element type type. If named_cols is true, the first line is assumed to be column labels. If named_rows is true, the first column of each row is assumed to be denote the label.\n\n\n\n\n\n","category":"method"},{"location":"lib/io/#SeqSpace.DataIO.read_mtx-Tuple{IO}","page":"Data Input/Output","title":"SeqSpace.DataIO.read_mtx","text":"read_mtx(io::IO)\n\nRead a matrix from io stream assuming bytes are formatted in matrix exchance file format.\n\n\n\n\n\n","category":"method"},{"location":"lib/io/#SeqSpace.DataIO.read_obj-Tuple{IO}","page":"Data Input/Output","title":"SeqSpace.DataIO.read_obj","text":"read_obj(io::IO)\n\nRead an oriented mesh from io stream assuming bytes are formatted in OBJ file format.\n\n\n\n\n\n","category":"method"},{"location":"lib/io/#SeqSpace.DataIO.read_ply-Tuple{IO}","page":"Data Input/Output","title":"SeqSpace.DataIO.read_ply","text":"read_ply(io::IO)\n\nRead an oriented mesh from io stream assuming bytes are formatted in PLY file format.\n\n\n\n\n\n","category":"method"},{"location":"lib/io/#SeqSpace.DataIO.readprops-Tuple{IO, Vector{SeqSpace.DataIO.PLYProp}}","page":"Data Input/Output","title":"SeqSpace.DataIO.readprops","text":"readprops(io::IO, props::Array{PLYProp,1})\n\nRead a collection of properties from io stream into props array. Return a named tuple of parsed properties.\n\n\n\n\n\n","category":"method"},{"location":"lib/pointcloud/#Point-Cloud","page":"Point Cloud","title":"Point Cloud","text":"","category":"section"},{"location":"lib/pointcloud/#Types","page":"Point Cloud","title":"Types","text":"","category":"section"},{"location":"lib/pointcloud/","page":"Point Cloud","title":"Point Cloud","text":"Modules = [SeqSpace.PointCloud]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.Edge","page":"Point Cloud","title":"SeqSpace.PointCloud.Edge","text":"struct Edge{T <: Real}\n    verts    :: Tuple{Int, Int}\n    distance :: T\nend\n\nConnects two neighboring verts by an edge of length distance.\n\n\n\n\n\n","category":"type"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.Graph","page":"Point Cloud","title":"SeqSpace.PointCloud.Graph","text":"struct Graph{T <: Real}\n    verts :: Array{Vertex{T},1}\n    edges :: Array{Edge{T},1}\nend\n\nA generic graph data structure containing vertices (points in space) stored within verts connected by edges.\n\n\n\n\n\n","category":"type"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.Vertex","page":"Point Cloud","title":"SeqSpace.PointCloud.Vertex","text":"struct Vertex{T <: Real}\n    position :: Array{T}\nend\n\nRepresents a single cell within a larger point cloud. The embedding space is normalized gene expression.\n\n\n\n\n\n","category":"type"},{"location":"lib/pointcloud/#Functions","page":"Point Cloud","title":"Functions","text":"","category":"section"},{"location":"lib/pointcloud/","page":"Point Cloud","title":"Point Cloud","text":"Modules = [SeqSpace.PointCloud]\nOrder = [:function]","category":"page"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.adjacency_list-Tuple{SeqSpace.PointCloud.Graph}","page":"Point Cloud","title":"SeqSpace.PointCloud.adjacency_list","text":"adjacency_list(G :: Graph)\n\nReturn the flattened adjacency list for graph G.\n\n\n\n\n\n","category":"method"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.dijkstra!-Tuple{Any, Any, Any}","page":"Point Cloud","title":"SeqSpace.PointCloud.dijkstra!","text":"dijkstra!(dist, adj, src)\n\nCompute the shortest path from src to all other points given adjacency list adj and distances dist using Dijkstra's algorithm.\n\n\n\n\n\n","category":"method"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.floyd_warshall-Tuple{SeqSpace.PointCloud.Graph}","page":"Point Cloud","title":"SeqSpace.PointCloud.floyd_warshall","text":"floyd_warshall(G :: Graph)\n\nCompute the shortest path from all vertices to all other vertices within graph G.\n\n\n\n\n\n","category":"method"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.geodesics-Tuple{Any, Any}","page":"Point Cloud","title":"SeqSpace.PointCloud.geodesics","text":"geodesics(x, k; D=missing, accept=(d)->true, sparse=true)\n\nCompute the matrix of pairwise distances, given a pointcloud x and neighborhood cutoff k, from the resultant neighborhood graph. If sparse is true, it will utilize Dijkstra's algorithm, individually for each point. If sparse is false, it will utilize the Floyd Warshall algorithm.\n\n\n\n\n\n","category":"method"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.geodesics-Tuple{SeqSpace.PointCloud.Graph}","page":"Point Cloud","title":"SeqSpace.PointCloud.geodesics","text":"geodesics(G :: Graph; sparse=true)\n\nCompute the matrix of pairwise distances, given a neighborhood graph G, weighted by local Euclidean distance. If sparse is true, it will utilize Dijkstra's algorithm, individually for each point. If sparse is false, it will utilize the Floyd Warshall algorithm.\n\n\n\n\n\n","category":"method"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.isomap-Tuple{Any, Any}","page":"Point Cloud","title":"SeqSpace.PointCloud.isomap","text":"isomap(x, dₒ; k=12, sparse=true)\n\nCompute the isometric embedding of point cloud x into dₒ dimensions. Geodesic distances between all points are estimated by utilizing the shortest path defined by the neighborhood graph. The embedding is computed from a multidimensional scaling analysis on the resultant geodesics.\n\n\n\n\n\n","category":"method"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.mds-Tuple{Any, Any}","page":"Point Cloud","title":"SeqSpace.PointCloud.mds","text":"mds(D², dₒ)\n\nComputes the lowest dₒ components from a Multidimensional Scaling analysis given pairwise squared distances D².\n\n\n\n\n\n","category":"method"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.neighborhood-Union{Tuple{T}, Tuple{Any, T}} where T<:AbstractFloat","page":"Point Cloud","title":"SeqSpace.PointCloud.neighborhood","text":"neighborhood(x, k :: T; D=missing, accept=(d)->true) where T <: AbstractFloat\n\nConstructs a neighborhood graph of all neighbors within euclidean distance k for each point of cloud x. If D is given, it is assumed to be a dense matrix of pairwise distances.\n\n\n\n\n\n","category":"method"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.neighborhood-Union{Tuple{T}, Tuple{Any, T}} where T<:Integer","page":"Point Cloud","title":"SeqSpace.PointCloud.neighborhood","text":"neighborhood(x, k :: T; D=missing, accept=(d)->true) where T <: Integer\n\nConstructs a neighborhood graph of the k nearest neighbor for each point of cloud x. If D is given, it is assumed to be a dense matrix of pairwise distances.\n\n\n\n\n\n","category":"method"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.scaling-Tuple{Any, Any}","page":"Point Cloud","title":"SeqSpace.PointCloud.scaling","text":"scaling(D, N)\n\nEstimate the Hausdorff dimension by computing how the number of points contained within balls scales with varying radius.\n\n\n\n\n\n","category":"method"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.upper_tri-Tuple{Any}","page":"Point Cloud","title":"SeqSpace.PointCloud.upper_tri","text":"upper_tri(x)\n\nReturns the upper triangular portion of matrix x.\n\n\n\n\n\n","category":"method"},{"location":"lib/infer/#Supervised-Spatial-Inference","page":"Supervised Spatial Inference","title":"Supervised Spatial Inference","text":"","category":"section"},{"location":"lib/infer/#Types","page":"Supervised Spatial Inference","title":"Types","text":"","category":"section"},{"location":"lib/infer/","page":"Supervised Spatial Inference","title":"Supervised Spatial Inference","text":"Modules = [SeqSpace.Inference]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/infer/#Functions","page":"Supervised Spatial Inference","title":"Functions","text":"","category":"section"},{"location":"lib/infer/","page":"Supervised Spatial Inference","title":"Supervised Spatial Inference","text":"Modules = [SeqSpace.Inference]\nOrder = [:function]","category":"page"},{"location":"lib/infer/#SeqSpace.Inference.cost-Tuple{Any, Any}","page":"Supervised Spatial Inference","title":"SeqSpace.Inference.cost","text":"cost(ref, qry; α=1, β=1, γ=0, ω=nothing)\n\nReturn the cost matrix J_ilpha associated to matching cells in qry to cells in ref. The cost matrix is computed by a heuristic distance between quantiles. Deprecated.\n\n\n\n\n\n","category":"method"},{"location":"lib/infer/#SeqSpace.Inference.cost_simple-Tuple{Any, Any}","page":"Supervised Spatial Inference","title":"SeqSpace.Inference.cost_simple","text":"cost_simple(ref, qry)\n\nReturn the cost matrix J_ialpha associated to matching cells in qry to cells in ref. The cost matrix is computed by hamming distance between cells via transforming quantiles to continuous spin variables. Deprecated.\n\n\n\n\n\n","category":"method"},{"location":"lib/infer/#SeqSpace.Inference.cost_transform-Tuple{Any, Any}","page":"Supervised Spatial Inference","title":"SeqSpace.Inference.cost_transform","text":"cost_transform(ref, qry; ω=nothing, ν=nothing)\n\nReturn the cost matrix J_ialpha associated to matching cells in qry to cells in ref. The cost matrix is computed by:\n\nTransforming the qry distribution to the ref distribution.\nLooking at the SSE across transformed genes.\n\nUse this unless you know what you are doing.\n\n\n\n\n\n","category":"method"},{"location":"lib/infer/#SeqSpace.Inference.inversion-Tuple{Any, Any}","page":"Supervised Spatial Inference","title":"SeqSpace.Inference.inversion","text":"inversion(counts, genes; ν=nothing, ω=nothing, refdb=nothing)\n\nInfer the original position of scRNAseq data counts where genes, given by genes are arranged along rows. The sampling probability over space is computed by regularized optimal transport by comparing to the Berkeley Drosophila Transcription Network Project database. The cost matrix is determined by summing over the 1D Wasserstein metric over all genes within the BDTNP databse. Returns the inversion as a function of inverse temperature.\n\n\n\n\n\n","category":"method"},{"location":"lib/infer/#SeqSpace.Inference.sinkhorn-Tuple{Matrix{Float64}}","page":"Supervised Spatial Inference","title":"SeqSpace.Inference.sinkhorn","text":"sinkhorn(M::Array{Float64,2};\n              a::Maybe{Array{Float64}} = missing,\n              b::Maybe{Array{Float64}} = missing,\n              maxᵢ::Integer            = 1000,\n              τ::Real                  = 1e-5,\n              verbose::Bool            = false\n)\n\nRescale matrix M to have row & column marginals a and b respectively. Will terminate either when constraints are held to within tolerance τ or the number of iterations exceed maxᵢ.\n\n\n\n\n\n","category":"method"},{"location":"lib/infer/#SeqSpace.Inference.transform-Tuple{Any, Any, Any}","page":"Supervised Spatial Inference","title":"SeqSpace.Inference.transform","text":"transform(src, dst, ν)\n\nTransform distribution src to distribution dst by minimizing the Wasserstein metric. This is equivalent to x to F^-1_dstleft(F_srcleft(xright)right) where F denotes the cumulative density function.\n\n\n\n\n\n","category":"method"},{"location":"lib/infer/#SeqSpace.Inference.virtualembryo-Tuple{}","page":"Supervised Spatial Inference","title":"SeqSpace.Inference.virtualembryo","text":"virtualembryo(;directory=\"/home/nolln/mnt/data/drosophila/dvex\")\n\nLoad the Berkeley Drosophila Transcriptional Network Project database. directory should be path to folder containing two folders:\n\nbdtnp.txt.gz    : gene expression over point cloud of virtual cells\ngeometry.txt.gz : spatial position (x,y,z) of point cloud of virtual cells.\n\n\n\n\n\n","category":"method"},{"location":"lib/generate/#Point-Cloud-Generation","page":"Point Cloud Generation","title":"Point Cloud Generation","text":"","category":"section"},{"location":"lib/generate/#Types","page":"Point Cloud Generation","title":"Types","text":"","category":"section"},{"location":"lib/generate/","page":"Point Cloud Generation","title":"Point Cloud Generation","text":"Modules = [SeqSpace.Generate]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/generate/#Functions","page":"Point Cloud Generation","title":"Functions","text":"","category":"section"},{"location":"lib/generate/","page":"Point Cloud Generation","title":"Point Cloud Generation","text":"Modules = [SeqSpace.Generate]\nOrder = [:function]","category":"page"},{"location":"lib/generate/#SeqSpace.Generate.sphere-Tuple{Any}","page":"Point Cloud Generation","title":"SeqSpace.Generate.sphere","text":"sphere(N; R=1)\n\nGenerate a spherical point cloud of N points with extent of radius R.\n\n\n\n\n\n","category":"method"},{"location":"lib/generate/#SeqSpace.Generate.swissroll-Tuple{Any}","page":"Point Cloud Generation","title":"SeqSpace.Generate.swissroll","text":"swissroll(N; z₀=10, R=1/20)\n\nGenerate a point cloud of N distributed on a swiss roll manifold with unit radius and length racz₀R.\n\n\n\n\n\n","category":"method"},{"location":"lib/generate/#SeqSpace.Generate.torus-Tuple{Any}","page":"Point Cloud Generation","title":"SeqSpace.Generate.torus","text":"torus(N; R=2, r=1)\n\nGenerate a point cloud of N distributed on a torus, sized inner r and outer radius R respectively.\n\n\n\n\n\n","category":"method"},{"location":"lib/util/#Utilities","page":"Utilities","title":"Utilities","text":"","category":"section"},{"location":"lib/util/#Types","page":"Utilities","title":"Types","text":"","category":"section"},{"location":"lib/util/","page":"Utilities","title":"Utilities","text":"Modules = [SeqSpace.scRNA.Utility]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/util/#Functions","page":"Utilities","title":"Functions","text":"","category":"section"},{"location":"lib/util/","page":"Utilities","title":"Utilities","text":"Modules = [SeqSpace.scRNA.Utility]\nOrder = [:function]","category":"page"},{"location":"lib/util/#SeqSpace.scRNA.Utility.sinkhorn-Tuple{Any}","page":"Utilities","title":"SeqSpace.scRNA.Utility.sinkhorn","text":"sinkhorn(A; r=[], c=[], maxit=1000, δ=1e-6, verbose=false)\n\nCompute the row and column multiplicative factors that constrain marginals of A to unity. Returns a boolean flag if algorithm converges within tolerance δ within maxit iterations. If r is given and non-empty, row marginals will be constrained to user-supplied inputs. If c is given and non-empty, column marginals will be constrained to user-supplied inputs.\n\n\n\n\n\n","category":"method"},{"location":"sci/autoencode/#Manifold-learning","page":"Manifold learning","title":"Manifold learning","text":"","category":"section"},{"location":"sci/autoencode/#Introduction","page":"Manifold learning","title":"Introduction","text":"","category":"section"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Want:","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Dimensional reduction\nNonlinear\nDifferentiable\nGeneralizable\nUnsupervised","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Natural choice for an autoencoder. Utilize known positional labels as a validation step. Not used for training purposes.","category":"page"},{"location":"sci/autoencode/#Architecture","page":"Manifold learning","title":"Architecture","text":"","category":"section"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"How to pick depth? Width? Worry about overfitting: enter dropout and batch normalization. Vanilla autoencoder: latent space is not readily interpretable.","category":"page"},{"location":"sci/autoencode/#Topological-conservation","page":"Manifold learning","title":"Topological conservation","text":"","category":"section"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"In order to learn an interpretable latent space representation of the intrinsic gene expression manifold, we wish to constrain the estimated pullback to conserve the topology of the input scRNAseq data. This immediately poses the question: what topological features do we wish to preserve from the data to latent space and how do we empirically measure them? The answers immediately determine the additional terms one must add to the objective function used for training.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"We opt to utilize an explicitly geometric formalism that will implicitly constrain topology. The intuition for this choice is guided by Differential Geometry: a metric tensor uniquely defines a distance function between any two points on a manifold; the topology induced by this distance function will always coincide with the original topology of the manifold. Thus, by imposing preservation of pairwise distances in the latent space relative to the data, we implicitly conserve topology. It is important to note that this assumes our original scRNAseq data is sampled from a metric space that we have access to. We note that there have been recent promising attempts at designing loss functions parameterized by explicit topological invariants formulated by Topological Data Analysis, e.g. persistent homology. Lastly, one could envision having each network layer operate on a simplicial complex, rather than a flat vector of real numbers, however it is unclear how to parameterize the feed-forward function.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Thus the first task is to formulate an algorithm to approximate the metric space the point cloud is sampled from and subsequently utilize our estimate to compute all pairwise distances. Again we proceed guided by intuition gleaned from Differential Geometry: pairwise distances within local neighborhoods are expected to be well-described by a Euclidean metric in the tangent space. Conversely, macroscopic distances can only be computed via integration against the underlying metric along the corresponding geodesic. As such, we first estimate the local tangent space of our input data by computing pairwise distances within local neighborhoods around each point, either defined by a fixed radius or fixed number of neighbors. This defines a sparse, undirected graph in which edges only exist within our estimated tangent spaces and are weighted by the euclidean distance within the embedded space. The resultant neighborhood graph serves as the basis for many dimensional reduction algorithms, such as Isomap, UMAP and tSNE. Pairwise distances between any two points in the original dataset can then be found by simple graph traversal to find the shortest possible path between two graph vertices, the discrete analog of a continuum geodesic. It has been shown that the distance estimated by this algorithm asymptotically approaches the true distance as the number of data points sampled increases. We denote D_alphabeta as the resultant pairwise distances between cell alphabeta.","category":"page"},{"location":"sci/autoencode/#Isometric-formulation","page":"Manifold learning","title":"Isometric formulation","text":"","category":"section"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"The most straightforward manner to preserve distances between the input data and the latent representation is to impose isometry, i.e. distances in both spaces quantitatively agree. This would be achieved by supplementing the objective function with the term","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"E_iso = displaystylesumlimits_alphabeta left(D_alphabeta - leftleft xi_alpha - xi_beta rightright right)^2","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Utilizing this term is problematic for several reasons:","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Large distances dominate the energetics and as such large-scale features of the intrinsic manifold will be preferentially fit.\nGenerically, d dimensional manifolds can not be isometrically embedded into mathbbR^d, e.g. the sphere into the plane.\nIt trusts the computed distances quantitatively. We simply want close cells to be close in the resultant latent space.","category":"page"},{"location":"sci/autoencode/#Differentiable-ranking","page":"Manifold learning","title":"Differentiable ranking","text":"","category":"section"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Consider a vector psi_alpha of scores of length n we wish to rank. Furthermore, define sigma in Sigma_n to be an arbitrary permutation of n such scores. We define the argsort to be the permutation that sorts psi in descending order","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"    barsigmaleft(bmpsiright) equiv left(sigma_1left(bmpsiright)sigma_nleft(bmpsiright)right) qquad textsuch that qquad\n    psi_barsigma_1 ge psi_barsigma_1 ge  ge psi_barsigma_n","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"The definition of the sorted vector of scores barbmpsi_alpha equiv psi_barsigma_alpha thus follows naturally. Lastly, the rank of vector bmpsi is defined as the inverse permutation of argsort.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"    Rleft(bmpsiright) equiv barsigma^-1left(bmpsiright)","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"We wish to devise an objective function that contains functions of the rank of some latent space variables. However, R(bmpsi) is a non-differentiable function; it maps a vector in mathbbR^n to a permutation of n items. Hence, we can not directly utilize the rank in a loss function as there is no way to backpropagate gradient information to the network parameters. In order to rectify this limitation, we first reformulate the ranking problem as a linear programming problem that permits efficient regularization. Note, the presentation here follows closely the original paper [1]","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"[1]: Fast Differentiable Sorting and Ranking","category":"page"},{"location":"sci/autoencode/#Linear-program-formulation","page":"Manifold learning","title":"Linear program formulation","text":"","category":"section"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"The sorting and ranking problem can be formulated as discrete optimization over the set of n-permutations Sigma_n","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"    barsigmaleft(bmpsiright) equiv undersetbmsigmainSigma_nmathrmargmax  displaystylesumlimits_alpha psi_sigma_alpha rho_alpha","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"    Rleft(bmpsiright)\n    equiv barsigmaleft(bmpsiright)^-1 \n    equiv leftundersetbmsigmainSigma_nmathrmargmax  displaystylesumlimits_alpha psi_sigma_alpha rho_alpha right^-1\n    equiv leftundersetbmsigma^-1inSigma_nmathrmargmax  displaystylesumlimits_alpha psi_alpha rho_sigma^-1_alpha right^-1\n    equiv undersetbmpiinSigma_nmathrmargmax  displaystylesumlimits_alpha psi_alpha rho_pi(alpha)","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"where rho_alpha equiv left(n n-1  1right) In order to regularize the problem, and thus allow for continuous optimization, we imagine the convex hull of all permutations induced by an arbitrary vector bmomega in mathbbR^n.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"    Omegaleft(bmomegaright) equiv textconvhullleftleftbmomega_sigma_alpha sigma in Sigma_n rightright subset mathbbR^n","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"This is often referred to as the permutahedron of bmomega; it is a convex polytope in n-dimensions whose vertices are the permutations of bmomega It follows directly from the fundamental theorem of linear programming, that the solution will almost surely be achieved at the vertex. Thus the above discrete formulation can be rewritten as an optimization over continuous vectors contained on the permutahedron","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"    bmpsi_barsigmaleft(bmpsiright) equiv undersetbmomegainOmegaleft(bmpsiright)mathrmargmax  bmomegacdotbmrho\n    qquad\n    bmrho_Rleft(bmpsiright) equiv undersetbmomegainOmegaleft(bmrhoright)mathrmargmax  bmpsicdotbmomega","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Utilizing the fact that rho_Rleft(bmpsiright) = Rleft(-bmpsiright)","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"    Rleft(bmpsiright) equiv -undersetbmomegainOmegaleft(bmrhoright)mathrmargmax  bmpsicdotbmomega","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Unfortunately, since bmpsi appears in the rank objective function, any small perturbation in bmpsi can force the solution of the linear program to discontinuously transition to another vertex. As such, in its current form, it is still not differentiable. Note, this is not true for the sorted vector, it appears in the constraint polyhedron; it has a unique Jacobian and can be directly used in neural networks. The only way to proceed is to introduce convex regularization.","category":"page"},{"location":"sci/autoencode/#Regularization","page":"Manifold learning","title":"Regularization","text":"","category":"section"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"We revise our objective function by Euclidean projection and thus introduce quadratic regularization on the norm of the solution. Specifically, we define the soft rank operators as the extrema of the objective function","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"    tildeRleft(bmpsiright) equiv undersetbmomegainOmegaleft(bmrhoright)mathrmargmax left -bmpsicdotbmomega\n    - fracepsilon2leftleftomegarightright^2 right","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Note that the limit epsilon rightarrow 0 reproduces the linear programming formulation of the rank operator introduced above. Conversely, in the limit epsilon rightarrow infty, the solution will go to a constant vector that has the smallest modulus on the permutahedron.","category":"page"},{"location":"sci/autoencode/#Solution","page":"Manifold learning","title":"Solution","text":"","category":"section"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"It has been demonstrated before that the above problem reduces to simple isotonic regression[1][2]. Specifically,","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Rleft(bmpsiright) = -fracbmpsiepsilon -\n    leftundersetomega_1 ge omega_2 ge  ge omega_nmathrmargmin\n    frac12 leftleftbmomega + bmrho + fracbmpsiepsilon rightright^2right_sigma^-1(bmpsi)\nequiv -fracbmpsiepsilon - tildebmomegaleft(bmpsibmrhoright)","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Importantly, isotonic regression is well-studied and can be solved in linear time. Furthermore, the solution admits a simple, calculatable Jacobian","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"partial_psi_alpha R_betaleft(bmpsiright)\n= frac-delta_alphabetaepsilon - partial_psi_alphatildeomega_betaleft(bmpsibmrhoright)\n= frac-delta_alphabetaepsilon - \n    beginpmatrix\n    bmB_1  bm0  bm0 \n    bm0    ddots  bm0 \n    bm0    bm0  bmB_m \n    endpmatrix_alphabeta","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"where bmB_i denotes the matrix corresponding to the i^{th} block obtained during isotonic regression. It is a constant matrix whose number of rows and columns equals the size of the block, and whose values all sum to 1.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"[2]: SparseMAP: Differentiable Sparse Structured Inference","category":"page"},{"location":"sci/autoencode/#Loss-function","page":"Manifold learning","title":"Loss function","text":"","category":"section"},{"location":"sci/autoencode/#Uniform-sampling-of-latent-space","page":"Manifold learning","title":"Uniform sampling of latent space","text":"","category":"section"},{"location":"sci/autoencode/#Results","page":"Manifold learning","title":"Results","text":"","category":"section"},{"location":"lib/scrna/#scRNAseq-Data","page":"scRNAseq Data","title":"scRNAseq Data","text":"","category":"section"},{"location":"lib/scrna/#Types","page":"scRNAseq Data","title":"Types","text":"","category":"section"},{"location":"lib/scrna/","page":"scRNAseq Data","title":"scRNAseq Data","text":"Modules = [SeqSpace.scRNA]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/scrna/#SeqSpace.scRNA.Count","page":"scRNAseq Data","title":"SeqSpace.scRNA.Count","text":"struct Count{T <: Real} <: AbstractArray{T,2}\n    data :: Array{T,2}\n    gene :: Array{AbstractString,1}\n    cell :: Array{AbstractString,1}\nend\n\nData structure used to represent count data obtained during a scRNAseq sequencing experiment. Individual cells are stored as column vectors while expression of singular genes are obtained as row vectors. data contains the raw/normalized count matrix. gene and cell contain the row/column labels respectively. Genes and cells can be indexed either by integers or names, i.e. strings.\n\n\n\n\n\n","category":"type"},{"location":"lib/scrna/#Functions","page":"scRNAseq Data","title":"Functions","text":"","category":"section"},{"location":"lib/scrna/","page":"scRNAseq Data","title":"scRNAseq Data","text":"Modules = [SeqSpace.scRNA]\nOrder = [:function]","category":"page"},{"location":"lib/scrna/#Base.:∪-Union{Tuple{S}, Tuple{T}, Tuple{SeqSpace.scRNA.Count{T}, SeqSpace.scRNA.Count{S}}} where {T<:Real, S<:Real}","page":"scRNAseq Data","title":"Base.:∪","text":"∪(seq₁::Count{T}, seq₂::Count{S}) where {T <: Real, S <: Real}\n\nCollate count matrix seq₁ and seq₂ by taking the union across genes. Reorders rows of seq₂ to match gene names of seq₁. Additional genes in seq₂ not contained in seq₁ are added as augmented rows.\n\n\n\n\n\n","category":"method"},{"location":"lib/scrna/#SeqSpace.scRNA.:∩-Union{Tuple{S}, Tuple{T}, Tuple{SeqSpace.scRNA.Count{T}, SeqSpace.scRNA.Count{S}}} where {T<:Real, S<:Real}","page":"scRNAseq Data","title":"SeqSpace.scRNA.:∩","text":"∩(seq₁::Count{T}, seq₂::Count{S}) where {T <: Real, S <: Real}\n\nCollate count matrix seq₁ and seq₂ by taking the union across genes. Reorders rows of seq₂ to match gene names of seq₁. Only keeps genes present in both seq₁ and seq₂.\n\n\n\n\n\n","category":"method"},{"location":"lib/scrna/#SeqSpace.scRNA.filtercell-Tuple{Any, SeqSpace.scRNA.Count}","page":"scRNAseq Data","title":"SeqSpace.scRNA.filtercell","text":"filtercells(f, seq::Count)\n\nFilters cells of count matrix seq based upon column function f.\n\n\n\n\n\n","category":"method"},{"location":"lib/scrna/#SeqSpace.scRNA.filtergene-Tuple{Any, SeqSpace.scRNA.Count}","page":"scRNAseq Data","title":"SeqSpace.scRNA.filtergene","text":"filtergene(f, seq::Count)\n\nFilters genes of count matrix seq based upon row function f.\n\n\n\n\n\n","category":"method"},{"location":"lib/scrna/#SeqSpace.scRNA.generate-Tuple{Any, Any}","page":"scRNAseq Data","title":"SeqSpace.scRNA.generate","text":"generate(ngene, ncell; ρ=(α=Gamma(0.25,2), β=Normal(1,.01), γ=Gamma(3,3)))\n\nGenerate scRNAseq data assuming a monoclonal population of cells sampled against a heteroskedastic negative binomial model.\n\n\n\n\n\n","category":"method"},{"location":"lib/scrna/#SeqSpace.scRNA.load-Tuple{AbstractString}","page":"scRNAseq Data","title":"SeqSpace.scRNA.load","text":"load(dir::AbstractString; batch=missing)\n\nRead in scRNAseq experimental data from directory dir. The directory is expected to contain the following files:\n\nbarcodes.tsv : one cell name per line\nfeatures.tsv : one gene name per line\nmatrix.mtx : count matrix in mtx format\n\nIf batch is not missing, then it will be appended to each cell label.\n\n\n\n\n\n","category":"method"},{"location":"lib/distance/#Distances","page":"Distances","title":"Distances","text":"","category":"section"},{"location":"lib/distance/#Types","page":"Distances","title":"Types","text":"","category":"section"},{"location":"lib/distance/","page":"Distances","title":"Distances","text":"Modules = [SeqSpace.PointCloud.Distances]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/distance/#Functions","page":"Distances","title":"Functions","text":"","category":"section"},{"location":"lib/distance/","page":"Distances","title":"Distances","text":"Modules = [SeqSpace.PointCloud.Distances]\nOrder = [:function]","category":"page"},{"location":"lib/distance/#SeqSpace.PointCloud.Distances.euclidean-Tuple{Any}","page":"Distances","title":"SeqSpace.PointCloud.Distances.euclidean","text":"euclidean(X)\n\nCompute the pairwise distances between points X according to the Euclidean metric. Assumes X is sized d times N where d and N denote dimensionality and cardinality respectively.\n\n\n\n\n\n","category":"method"},{"location":"lib/distance/#SeqSpace.PointCloud.Distances.euclidean²-Tuple{Any}","page":"Distances","title":"SeqSpace.PointCloud.Distances.euclidean²","text":"euclidean²(X)\n\nCompute the pairwise squared distances between points X according to the Euclidean metric. Assumes X is sized d times N where d and N denote dimensionality and cardinality respectively.\n\n\n\n\n\n","category":"method"},{"location":"lib/distance/#SeqSpace.PointCloud.Distances.jensen_shannon-Tuple{Any}","page":"Distances","title":"SeqSpace.PointCloud.Distances.jensen_shannon","text":"jensen_shannon(P)\n\nCompute the pairwise distances between probability distributions P according to the Jensen-Shannon divergence. Assumes P is sized d times N where d and N denote dimensionality and cardinality respectively.\n\n\n\n\n\n","category":"method"},{"location":"lib/distance/#SeqSpace.PointCloud.Distances.kullback_liebler-Tuple{Any, Any}","page":"Distances","title":"SeqSpace.PointCloud.Distances.kullback_liebler","text":"kullback_liebler(p, q)\n\nCompute the pairwise distances between probability distributions p and q according to the Kullback-Liebler divergence. Assumes p and q are normalized.\n\n\n\n\n\n","category":"method"},{"location":"lib/queue/#Priority-Queue","page":"Priority Queue","title":"Priority Queue","text":"","category":"section"},{"location":"lib/queue/#Types","page":"Priority Queue","title":"Types","text":"","category":"section"},{"location":"lib/queue/","page":"Priority Queue","title":"Priority Queue","text":"Modules = [SeqSpace.PointCloud.PriorityQueue]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/queue/#SeqSpace.PointCloud.PriorityQueue.RankedQueue","page":"Priority Queue","title":"SeqSpace.PointCloud.PriorityQueue.RankedQueue","text":"struct RankedQueue{T <: Real, S <: Any}\n    rank :: Array{T, 1}\n    data :: Array{S, 1}\nend\n\nMaintains a priority queue of data. Each datum has a rank that determines it's priority in the queue. rank and data are sorted in ascending order.\n\n\n\n\n\n","category":"type"},{"location":"lib/queue/#Functions","page":"Priority Queue","title":"Functions","text":"","category":"section"},{"location":"lib/queue/","page":"Priority Queue","title":"Priority Queue","text":"Modules = [SeqSpace.PointCloud.PriorityQueue]\nOrder = [:function]","category":"page"},{"location":"lib/queue/#Base.insert!-Union{Tuple{S}, Tuple{T}, Tuple{SeqSpace.PointCloud.PriorityQueue.RankedQueue{T}, S, T}} where {T<:Real, S}","page":"Priority Queue","title":"Base.insert!","text":"insert!(q::RankedQueue{T}, data::S, rank::T) where {T <: Real, S <: Any}\n\nPush a new element data with priority rank onto the ranked queue q. Rotates the queue until priority is sorted in ascending order.\n\n\n\n\n\n","category":"method"},{"location":"lib/queue/#Base.take!-Tuple{SeqSpace.PointCloud.PriorityQueue.RankedQueue}","page":"Priority Queue","title":"Base.take!","text":"take!(q::RankedQueue)\n\nPop off the element with element with lowest rank/highest priority.\n\n\n\n\n\n","category":"method"},{"location":"lib/queue/#SeqSpace.PointCloud.PriorityQueue.left-Tuple{Any}","page":"Priority Queue","title":"SeqSpace.PointCloud.PriorityQueue.left","text":"left(i)\n\nReturn the index of the left child of node i.\n\n\n\n\n\n","category":"method"},{"location":"lib/queue/#SeqSpace.PointCloud.PriorityQueue.parent-Tuple{Any}","page":"Priority Queue","title":"SeqSpace.PointCloud.PriorityQueue.parent","text":"parent(i)\n\nReturn the index of the parent of node i.\n\n\n\n\n\n","category":"method"},{"location":"lib/queue/#SeqSpace.PointCloud.PriorityQueue.right-Tuple{Any}","page":"Priority Queue","title":"SeqSpace.PointCloud.PriorityQueue.right","text":"right(i)\n\nReturn the index of the right child of node i.\n\n\n\n\n\n","category":"method"},{"location":"lib/queue/#SeqSpace.PointCloud.PriorityQueue.rotatedown!-Tuple{SeqSpace.PointCloud.PriorityQueue.RankedQueue, Any}","page":"Priority Queue","title":"SeqSpace.PointCloud.PriorityQueue.rotatedown!","text":"rotatedown!(q::RankedQueue, i)\n\nModify the ranked queue by pushing down the node at index i until the priority is sorted.\n\n\n\n\n\n","category":"method"},{"location":"lib/queue/#SeqSpace.PointCloud.PriorityQueue.rotatedown!-Tuple{SeqSpace.PointCloud.PriorityQueue.RankedQueue}","page":"Priority Queue","title":"SeqSpace.PointCloud.PriorityQueue.rotatedown!","text":"rotatedown!(q::RankedQueue)\n\nModify the ranked queue by pushing down the root until the priority is sorted.\n\n\n\n\n\n","category":"method"},{"location":"lib/queue/#SeqSpace.PointCloud.PriorityQueue.rotateup!-Tuple{SeqSpace.PointCloud.PriorityQueue.RankedQueue, Any}","page":"Priority Queue","title":"SeqSpace.PointCloud.PriorityQueue.rotateup!","text":"rotateup!(q::RankedQueue, i)\n\nModify the ranked queue by pushing up the node at index i until the priority is sorted again.\n\n\n\n\n\n","category":"method"},{"location":"lib/queue/#SeqSpace.PointCloud.PriorityQueue.rotateup!-Tuple{SeqSpace.PointCloud.PriorityQueue.RankedQueue}","page":"Priority Queue","title":"SeqSpace.PointCloud.PriorityQueue.rotateup!","text":"rotateup!(q::RankedQueue)\n\nModify the ranked queue by pushing up the last element until the priority is sorted.\n\n\n\n\n\n","category":"method"},{"location":"lib/queue/#SeqSpace.PointCloud.PriorityQueue.update!-Union{Tuple{S}, Tuple{T}, Tuple{SeqSpace.PointCloud.PriorityQueue.RankedQueue{T, S}, S, T}} where {T<:Real, S}","page":"Priority Queue","title":"SeqSpace.PointCloud.PriorityQueue.update!","text":"update!(q::RankedQueue{T, S}, data::S, new::T) where {T <: Real, S <: Any}\n\nChange the priority of element data to rank new. Will panic if data is not contained in queue q.\n\n\n\n\n\n","category":"method"},{"location":"lib/rank/#Differentiable-Rank","page":"Differentiable Rank","title":"Differentiable Rank","text":"","category":"section"},{"location":"lib/rank/#Types","page":"Differentiable Rank","title":"Types","text":"","category":"section"},{"location":"lib/rank/","page":"Differentiable Rank","title":"Differentiable Rank","text":"Modules = [SeqSpace.SoftRank]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/rank/#Functions","page":"Differentiable Rank","title":"Functions","text":"","category":"section"},{"location":"lib/rank/","page":"Differentiable Rank","title":"Differentiable Rank","text":"Modules = [SeqSpace.SoftRank]\nOrder = [:function]","category":"page"},{"location":"lib/rank/#SeqSpace.SoftRank.isotonic-Tuple{Any}","page":"Differentiable Rank","title":"SeqSpace.SoftRank.isotonic","text":"isotonic(x)\n\nIsotonically regress on data vector x. Returns the monotonically increasing fit.\n\n\n\n\n\n","category":"method"},{"location":"lib/rank/#SeqSpace.SoftRank.partition-Tuple{Any}","page":"Differentiable Rank","title":"SeqSpace.SoftRank.partition","text":"partition(x; ϵ=1e-9)\n\nCompute the sizes of each partition of constant value obtained by isotonic regression of data x. ϵ denotes the tolerance for what is considered equal in floating point terms.\n\n\n\n\n\n","category":"method"},{"location":"lib/rank/#SeqSpace.SoftRank.projection-Tuple{Any}","page":"Differentiable Rank","title":"SeqSpace.SoftRank.projection","text":"projection(x)\n\nRegularize the ranking algorithm by euclidean projection onto the permutehedron.\n\n\n\n\n\n","category":"method"},{"location":"lib/rank/#SeqSpace.SoftRank.rank-Tuple{Any}","page":"Differentiable Rank","title":"SeqSpace.SoftRank.rank","text":"rank(x)\n\nRank the items of vector x in ascending order.\n\n\n\n\n\n","category":"method"},{"location":"lib/rank/#SeqSpace.SoftRank.softrank-Tuple{Any}","page":"Differentiable Rank","title":"SeqSpace.SoftRank.softrank","text":"softrank(x)\n\nRegularization of the ranking algorithm. Scaled euclidean projection onto the permutehedron.\n\n\n\n\n\n","category":"method"},{"location":"lib/rank/#SeqSpace.SoftRank.∇isotonic-Tuple{Any, Any}","page":"Differentiable Rank","title":"SeqSpace.SoftRank.∇isotonic","text":"∇isotonic(x)\n\nCompute the gradient of isotonic regression with respect to the inputs about the solution point.\n\n\n\n\n\n","category":"method"},{"location":"sci/normalize/#scRNAseq-Normalization","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"","category":"section"},{"location":"sci/normalize/#Introduction","page":"scRNAseq Normalization","title":"Introduction","text":"","category":"section"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"scRNA sequencing is a relatively novel technique added to the Biologist's toolkit that allows one to quantitatively probe the \"Statistical Mechanics\" of Biology. In contrast to traditional bulk RNAseq techniques, which provide population averages, scRNAseq technology measures the complete ensemble of cellular gene expression. Consequently, considerable activity has been focused on mapping the taxonomy of cell states; projects such as the Human Cell Atlas promise to provide a complete enumeration of microscopic cell types within the human body. However, despite recent progress, scRNAseq data have many numerous technical limitations and sources of statistical bias that must be considered before any downstream analysis. The sources of noise within the data include, but are not limited to:","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"Sequence depth bias: PCR and reverse transcription efficiency will vary across reactions, resulting in artificial variation of sequencing depth across different cells.\nAmplification bias: PCR primers are not perfectly random. As a result, some genes will amplify preferentially over others. This will distort the underlying expression distribution.\nBatch effect: scRNAseq runs have non-trivial distortions causing cells within a given sequencing run to have greater correlation within than across batches.\nDropout: Due to molecular competition in the underlying amplification reactions, some genes will fail to amplify during PCR due to early losses. This will lead to more zeros in the resulting count matrix than you would expect by chance.","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"These biases are approximately rectified by a preprocessing step known as normalization. At a coarse level, all such methods can be thought of as transforming the obtained count matrix from absolute numbers into an estimate of differential expression, i.e. a comparison of a count relative to the measured distribution. Such a procedure is analogous to that of the z-score of a univariate normally-distributed random variable; however, in the case of scRNAseq data, we don't have access to the underlying sampling distributions a priori, it must be estimated empirically. The choice of sampling prior, and details of how the distribution is estimated, delineate normalization practices.","category":"page"},{"location":"sci/normalize/#Overview-of-current-methods","page":"scRNAseq Normalization","title":"Overview of current methods","text":"","category":"section"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"TODO: FILL OUT","category":"page"},{"location":"sci/normalize/#Our-approach","page":"scRNAseq Normalization","title":"Our approach","text":"","category":"section"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"We model the count matrix n_alphai, where alpha and i indexes cells and genes respectively, obtained from scRNA sequencing as an unknown, low-rank mean mu_alphai with additive full-rank noise delta_alphai that captures all unknown technical noise and bias.","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"    tag1 n_alphai = mu_alphai + delta_alphai ","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"Our goal during the normalization procedure is two-fold:","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"Estimate the low-rank mean mu_alphai.\nEstimate the sampling variance langledelta_alphai^2rangle of the experimental noise. Brackets denote an average over (theoretical) realizations of sequencing. This will eventually require an explicit model.","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"Given both quantities, normalization over gene and cell-specific biases can be achieved by imposing the variances of all marginal distributions to be one. Specifically, we rescale all counts by cell-specific c_alpha and gene-specific g_i factors tilden_alphai equiv c_alpha n_alphai g_i such that","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"    tag2 displaystylesumlimits_alpha langle tildedelta_alphai^2 rangle = displaystylesumlimits_alphac_alpha^2 langle delta_alphai^2 rangle g_i^2 = N_g quad textand quad displaystylesumlimits_i langle tildedelta_alphai^2 rangle = displaystylesumlimits_ic_alpha^2 langle delta_alphai^2 rangle g_i^2 = N_c","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"This system of equations can be solved by the Sinkhorn-Knopp algorithm, provided we have a model for langledelta_alphai^2rangle parameterized by measurables. Within this formalism, this choice of model fully determines the normalization scheme. Owing to dropout and other sources of overdispersion, we model scRNAseq counts as sampled from an empirically estimated Heteroskedastic Negative Binomial distribution.","category":"page"},{"location":"sci/normalize/#Case-studies","page":"scRNAseq Normalization","title":"Case studies","text":"","category":"section"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"Before detailing our explicit algorithm, it is helpful to consider a few simpler examples. In the following sections, homoskedastic is used to denote count matrices whose elements are  independent and identically distributed (iid), while heteroskedastic denotes more complicated scenarios where each element has a unique distribution.","category":"page"},{"location":"sci/normalize/#Homoskedastic-Gaussian","page":"scRNAseq Normalization","title":"Homoskedastic Gaussian","text":"","category":"section"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"This is slight perturbation of canonical random matrix theory and will serve as a useful pedagogical starting point. Assume mu_alphai is a quenched, low-rank matrix and each element of delta_alphai is sampled from a Gaussian with zero mean and variance sigma^2. In this limit, Eq.(1) reduces to the well-studied spiked population covariance model.","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"If mu_alphai = 0, the spectral decomposition of the count matrix n_alphai would be given by the Marchenko-Pastur distribution asymptotically. As such, singular values would be bounded by barlambda equiv 1+sigmasqrtN_gN_c. Now consider the case of a rank 1 mean, i.e. 1 cell type in the data.","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"    n_alphai = gamma x_alpha barx_i + delta_alphai","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"It has been shown[1][2] that this model exhibits the following asymptotic phase transition:","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"If gamma le barlambda, the top singular value of n_alphai converges to barlambda. Additionally, the overlap of the left and right eigenvector with x and barx respectively converge to 0.\nIf gamma  barlambda, the top singular value of n_alphai converges to gamma + barlambdagamma. Additionally, the overlap of the left and right eigenvector with x and barx respectively converge to 1-(barlambdagamma)^2","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"This procedure can generalized to higher rank spike-ins; sub-leading principal components can be found by simply subtracting the previously inferred component from the count matrix n_alphai. As such, we can only expect to meaningful measure the principal components of mu_alphai that fall above the sampling noise floor, given by the Marchenko-Pastur distribution. Consequently, this forces us to define the statistically significant rank of the count matrix.","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"[1]: The singular values and vectors of low rank perturbations of large rectangular random matrices","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"[2]: The largest eigenvalue of rank one deformation of large Wigner matrices","category":"page"},{"location":"sci/normalize/#Heteroskedastic-Poisson","page":"scRNAseq Normalization","title":"Heteroskedastic Poisson","text":"","category":"section"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"The above results have been shown to hold for non-Gaussian Wigner noise matrices[3], provided each element is still iid. This is manifestly not the case we care about; the normalization procedure must account for heterogeneous sampling variances across both cells and genes. However, it has been shown[4][5] that the distribution of eigenvalues converges almost surely to the Marchenko-Pastur distribution, provided the constraint of Eq.(2) is satisfied! In other words, the eigenvalues of a heteroskedastic matrix is expected converge to that of a random Wigner matrix, provided the row and column variances are uniform (set to unity for convenience).","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"For the remainder of this section, assume the count matrix is sampled from a Poisson distribution, such that langledelta_alphai^2rangle = mu_alphai. As n_alphai is an unbiased estimator for the mean mu_alphai, Eq (2) reduces to","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"displaystylesumlimits_alphac_alpha^2 n_alpha i g_i^2 = N_g quad textand quad displaystylesumlimits_ic_alpha^2 n_alphai g_i^2 = N_c","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"which provides an explicit system of equations to estimate the scaling factors c_alpha and g_i. Once obtained, tildemu_alphai can be estimated via singular value decomposition of c_alpha n_alphai g_i; all components with singular value greater than barlambda = sqrtN_c+sqrtN_g can be confidently attributed to the \"true mean\" mu_alphai while all other components fall amongst the noise. An example is shown below:","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"TODO: ADD FIGURE","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"[3]: Asymptotics of Sample Eigenstructure for a Large Dimensional Spiked Covariance Model","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"[4]: Biwhitening Reveals the Rank of a Count Matrix","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"[5]: A Review of Matrix Scaling and Sinkhorn's Normal Form for Matrices and Positive Maps","category":"page"},{"location":"sci/normalize/#Heteroskedastic-Negative-Binomial","page":"scRNAseq Normalization","title":"Heteroskedastic Negative Binomial","text":"","category":"section"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"A negative binomial distribution is often used to model overdispersed count data, i.e. a process in which the variance grows superlinearly with the mean. Canonically the distribution arises from the distribution of the number of successes (with probabiliy p) obtained after phi failures of a Bernoulli process. However, the generative stochastic process can equivalently be modelled as an underlying Poisson process in which the emission rate is itself a stochastic variable drawn from a Gamma distribution. This allows us to analytically continue phi from an integer to the reals. Provided we have estimated the mean mu and the overdisperson factor phi, the unbiased estimator for the variance is given by","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"    langle delta^2 rangle= = mufrac1 + muphi1 + phi","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"Direct substitution into Eq. (2) would provide the necessary cell-specific c_alpha and gene-specific g_i scaling factors.","category":"page"},{"location":"sci/normalize/#Empirical-Sampling-Distribution-Estimation","page":"scRNAseq Normalization","title":"Empirical Sampling Distribution Estimation","text":"","category":"section"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"As such, in order to normalize the estimated sampling variance, we must formulate a method to fit a negative binomial to the measured count data per gene. We parameterize the distribution as follows:","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"    pleft(nmuphiright) = fracGammaleft(n+phiright)Gammaleft(n+1right)Gammaleft(phiright)left(fracmumu+phiright)^nleft(fracphimu+phiright)^phi","category":"page"},{"location":"sci/normalize/#Generalized-Linear-Model","page":"scRNAseq Normalization","title":"Generalized Linear Model","text":"","category":"section"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"A central complication in modeling counts of a given gene across cells with the above function is that there are confounding variables that require explicit consideration. For the present discussion, we explictly model the hetereogeneous sequencing depth across cells: naively we expect that if we sequenced a given cell with 2x depth, each gene should scale accordingly. As such, we formulate a generalized linear model","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"    log mu_ialpha = A_i + B_i log n_alpha","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"where n_alpha denotes the total sequencing depth of cell alpha. We note this formulation can be easily extended to account for other confounds such as batch effect or cell type. The likelihood function for cell alpha, gene i is","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"    pleft(n_ialphaA_i B_i n_alpha phi_iright) = fracGammaleft(n_ialpha+phi_iright)Gammaleft(n_ialpha+1right)Gammaleft(phi_iright)left(fracmu_ialphamu_ialpha+phi_iright)^n_ialphaleft(fracphi_imu_ialpha+phi_iright)^phi_i","category":"page"},{"location":"sci/normalize/#Maximum-Likelihood-Estimation","page":"scRNAseq Normalization","title":"Maximum Likelihood Estimation","text":"","category":"section"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"A_i B_i phi_i represent 3N_g parameters we must infer from the data. A priori this seems underdetermined given our N_c times N_g sized matrix and thus we attempt to estimate all parameters within a maximum likelihood framework. This is equivalent to minimizing (for each gene independently)","category":"page"},{"location":"sci/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"    mathcalL = displaystylesumlimits_alpha\n        left(n_ialpha + phi_iright)logleft(e^A_in_alpha^B_i + phi_iright) \n      - phi_ilogleft(phi_iright)\n      - n_ialphaleft(A_i + B_ilog n_alpharight)\n      - logleft(fracGammaleft(n_ialpha+phi_iright)Gammaleft(n_ialpha+1right)Gammaleft(phi_iright)right)","category":"page"},{"location":"sci/normalize/#Overfitting","page":"scRNAseq Normalization","title":"Overfitting","text":"","category":"section"},{"location":"sci/normalize/#Maximum-A-Posterior","page":"scRNAseq Normalization","title":"Maximum A Posterior","text":"","category":"section"},{"location":"sci/normalize/#Empirical-Estimation-of-Priors","page":"scRNAseq Normalization","title":"Empirical Estimation of Priors","text":"","category":"section"},{"location":"sci/normalize/#Results","page":"scRNAseq Normalization","title":"Results","text":"","category":"section"},{"location":"cli/drosophila/#Drosophila-pipeline","page":"Drosophila pipeline","title":"Drosophila pipeline","text":"","category":"section"},{"location":"cli/drosophila/","page":"Drosophila pipeline","title":"Drosophila pipeline","text":"Found at path bin/drosophila.jl. Can either be pointed to filtered data from original Science paper or the raw data obtained from GEO ????. Running this command will reproduce all plots shown in the manuscript.","category":"page"},{"location":"lib/manifold/#Point-Cloud-Differential-Geometry","page":"Point Cloud Differential Geometry","title":"Point Cloud Differential Geometry","text":"","category":"section"},{"location":"lib/manifold/#Types","page":"Point Cloud Differential Geometry","title":"Types","text":"","category":"section"},{"location":"lib/manifold/","page":"Point Cloud Differential Geometry","title":"Point Cloud Differential Geometry","text":"Modules = [SeqSpace.DifferentialGeometry]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.Manifold","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.Manifold","text":"struct Manifold{T <: Real}\n    mesh :: Mesh{T}\n    surf :: Surface{T}\nend\n\nStore the representation of a differential geometry object. mesh is the empirical point cloud. surf is the estimated differentiable surface.\n\n\n\n\n\n","category":"type"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.Mesh","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.Mesh","text":"struct Mesh{T <: Real}\n    r   :: Array{T, 2}\n    vₙ  :: Array{T, 2}\n    tri :: Array{Int, 2}\nend\n\nStore a 2 dimensional triangular mesh, embedded into arbitrary dimensions. r and vₙ denote vertex positions and normals respectively. tri denotes (non-oriented) triangular faces.\n\n\n\n\n\n","category":"type"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.Surface","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.Surface","text":"struct Surface{T <: Real}\n    Θ  :: Array{T, 2}\n    x  :: Array{T}\n    L  :: T\n    Λ  :: Array{Polynomial{T}}\n    ∂Λ :: Array{Polynomial{T}}\nend\n\nStore a representation of a 2D surface embedded into higher dimensional Euclidean space. Fits the surface by:\n\nPartition the x axis so that each resultant interval has, on average, 20 points.\nFit points within each partition to a 2D ellipse.\nFit a polynomial function to each elliptical parameter over all partitions.\nUse polynomials to estimate tangent vectors.\n\nΘ denotes the elliptical parameters fit per partition. x denotes the input data. L denotes the length along the x axis. Λ denotes the polynomials for each elliptical parameter. ∂Λ denotes the derivative of polynomials for each elliptical parameter.\n\n\n\n\n\n","category":"type"},{"location":"lib/manifold/#Functions","page":"Point Cloud Differential Geometry","title":"Functions","text":"","category":"section"},{"location":"lib/manifold/","page":"Point Cloud Differential Geometry","title":"Point Cloud Differential Geometry","text":"Modules = [SeqSpace.DifferentialGeometry]\nOrder = [:function]","category":"page"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.basis-Tuple{SeqSpace.DifferentialGeometry.Surface, Any}","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.basis","text":"function basis(s::Surface, r)\n\nCompute the tangent vectors hatbme_phi hatbme_x associated to each point within r. Assumes all points within r are distributed over the surface. Assumes r is sized N times 3.\n\n\n\n\n\n","category":"method"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.ellipse-Tuple{Any}","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.ellipse","text":"ellipse(r)\n\nFit an ellipse to 2D point cloud r. r is assumed to be sized N times 2.\n\n\n\n\n\n","category":"method"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.interpolate-Tuple{Any, Any, Any}","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.interpolate","text":"function interpolate(M, ϕ, r)\n\nInterpolate the tensor field ϕ, defined at points r onto the vertices of manifold M. Interpolation is computed by finding the containing triangle within the mesh of M for each point r. Linear interpolation is performed per face.\n\n\n\n\n\n","category":"method"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.makefield-Tuple{SeqSpace.DifferentialGeometry.Manifold, AbstractArray}","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.makefield","text":"makefield(ℳ::Manifold, ϕ::AbstractArray; field::Symbol = :ℝ³)\n\nReturn an arbitrary tensor field defined over either the embedding space or the cylindrical pullback of manifold M. Higher level function than either scalar or vector.\n\n\n\n\n\n","category":"method"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.mesh","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.mesh","text":"mesh(io::IO, type::Symbol=:obj)\n\nLoad a Mesh object from stream io formatted with type. As of now, only .obj files are supported.\n\n\n\n\n\n","category":"function"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.order-Tuple{Any, Any}","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.order","text":"function order(tri, r)\n\nReorder the triangulation so that all labels are counterclockwise.\n\n\n\n\n\n","category":"method"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.pullback-Tuple{SeqSpace.DifferentialGeometry.Manifold}","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.pullback","text":"function pullback(ℳ::Manifold)\n\nReturns the 2D cylindrical projection of the mesh, as estimated by the internal surface.\n\n\n\n\n\n","category":"method"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.pullback-Tuple{SeqSpace.DifferentialGeometry.Surface, Any}","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.pullback","text":"function pullback(s::Surface, r)\n\nTransform Euclidean point cloud r into a 2D cylindrical projection defined by s. Cylindrical coordinates are [x,φ]. Assumes all points within r are distributed over the surface. Assumes r is sized N times 3.\n\n\n\n\n\n","category":"method"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.rescale-Tuple{Any}","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.rescale","text":"rescale(x)\n\nRescale array x to run between 01.\n\n\n\n\n\n","category":"method"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.scalar-Tuple{SeqSpace.DifferentialGeometry.Manifold, Any, Symbol}","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.scalar","text":"scalar(M::Manifold, ϕ, field::Symbol)\n\nReturn a scalar field ϕ interpolated onto either the embedded space of manifold M or the pullback. If field is :ℝ² the pullback is computed. If field is :ℝ³ the embedding space is returned.\n\n\n\n\n\n","category":"method"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.triangulation-Tuple{Any, Any}","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.triangulation","text":"function triangulation(r₀, rᵢ)\n\nReturn the triangle containing each point given in rᵢ. The triangulation used is delaunay triangulation defined by point cloud r₀.\n\n\n\n\n\n","category":"method"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.vector-Tuple{SeqSpace.DifferentialGeometry.Manifold, Any, Symbol}","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.vector","text":"scalar(M::Manifold, ϕ, field::Symbol)\n\nReturn a vector field field ϕ interpolated onto either the embedded space of manifold M or the pullback. If field is :ℝ² the pullback is computed. Only the tangent space component is kept. If field is :ℝ³ the embedding space is returned.\n\n\n\n\n\n","category":"method"},{"location":"lib/voronoi/#Voronoi","page":"Voronoi","title":"Voronoi","text":"","category":"section"},{"location":"lib/voronoi/#Types","page":"Voronoi","title":"Types","text":"","category":"section"},{"location":"lib/voronoi/","page":"Voronoi","title":"Voronoi","text":"Modules = [SeqSpace.Voronoi]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/voronoi/#Functions","page":"Voronoi","title":"Functions","text":"","category":"section"},{"location":"lib/voronoi/","page":"Voronoi","title":"Voronoi","text":"Modules = [SeqSpace.Voronoi]\nOrder = [:function]","category":"page"},{"location":"lib/voronoi/#SeqSpace.Voronoi.areas-Tuple{Any}","page":"Voronoi","title":"SeqSpace.Voronoi.areas","text":"areas(x)\n\nCompute the areas of all 2-dimensional simplices generated by the delaunay construction for pointcloud x. Assumes x is sized N times 2\n\n\n\n\n\n","category":"method"},{"location":"lib/voronoi/#SeqSpace.Voronoi.boundary-Tuple{Any}","page":"Voronoi","title":"SeqSpace.Voronoi.boundary","text":"boundary(d)\n\nReturns the boundary vertices of a unit cube in d dimensions.\n\n\n\n\n\n","category":"method"},{"location":"lib/voronoi/#SeqSpace.Voronoi.tessellation-Tuple{Any}","page":"Voronoi","title":"SeqSpace.Voronoi.tessellation","text":"tessellation(q)\n\nConstruct a voronoi tessellation from generating points q. Return just vertices of construction.\n\n\n\n\n\n","category":"method"},{"location":"lib/voronoi/#SeqSpace.Voronoi.volumes-Tuple{Any}","page":"Voronoi","title":"SeqSpace.Voronoi.volumes","text":"volumes(x)\n\nCompute the volume of all d-dimensional simplices generated by the delaunay construction for pointcloud x. Assumes x is sized N times d\n\n\n\n\n\n","category":"method"},{"location":"lib/seqspace/#SeqSpace-API","page":"SeqSpace API","title":"SeqSpace API","text":"","category":"section"},{"location":"lib/seqspace/#Types","page":"SeqSpace API","title":"Types","text":"","category":"section"},{"location":"lib/seqspace/","page":"SeqSpace API","title":"SeqSpace API","text":"Modules = [SeqSpace]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/seqspace/#SeqSpace.HyperParams","page":"SeqSpace API","title":"SeqSpace.HyperParams","text":"mutable struct HyperParams\n    dₒ :: Int\n    Ws :: Array{Int,1}\n    BN :: Array{Int,1}\n    DO :: Array{Int,1}\n    N  :: Int\n    δ  :: Int\n    η  :: Float64\n    B  :: Int\n    V  :: Int\n    k  :: Int\n    γₓ :: Float32\n    γᵤ :: Float32\n    g  :: Function\nend\n\nHyperParams is a collection of parameters that specify the network architecture and training hyperparameters of the autoencoder. dₒ is the desired output dimensionality of the encoding layer Ws is a collection of the network layer widths. The number of entries controls the depth. The decoder is mirror-symmetric. BN is the collection of layers that will be followed by batch normalization. DO is the collection of layers that will be followed by dropout. N  is the number of epochs to train against δ  is the number of epochs that will be sampled for logging η  is the learning rate B  is the batch size V  is the number of points to partition for validation purposes, i.e. won't be training against k  is the number of neighbors to use to estimate geodesics γₓ is the prefactor of distance soft rank loss γᵤ is the prefactor of uniform density loss g is the metric given to latent space\n\n\n\n\n\n","category":"type"},{"location":"lib/seqspace/#SeqSpace.Result","page":"SeqSpace API","title":"SeqSpace.Result","text":"struct Result\n    param :: HyperParams\n    loss  :: NamedTuple{(:train, :valid), Tuple{Array{Float64,1},Array{Float64,1}} }\n    model\nend\n\nStore the output of a trained autoencoder. param stores the input hyperparameters used to design and train the neural network. loss traces the dynamics of the optimization found during training. model represents the learned pullback and pushforward functions.\n\n\n\n\n\n","category":"type"},{"location":"lib/seqspace/#Functions","page":"SeqSpace API","title":"Functions","text":"","category":"section"},{"location":"lib/seqspace/","page":"SeqSpace API","title":"SeqSpace API","text":"Modules = [SeqSpace]\nOrder = [:function]","category":"page"},{"location":"lib/seqspace/#SeqSpace.buildloss-Tuple{Any, Any, Any}","page":"SeqSpace API","title":"SeqSpace.buildloss","text":"buildloss(model, D², param)\n\nReturn a loss function used to train a neural network model according to input hyperparameters param. model is a object with three fields, pullback, pushforward, and identity. pullback and pushforward refers to the encoder and decoder layers respectively, while the identity is the composition. D² is a matrix of pairwise distances that will be used as a quenched hyperparameter in the distance soft rank loss.\n\n\n\n\n\n","category":"method"},{"location":"lib/seqspace/#SeqSpace.cylinder²-Tuple{Any}","page":"SeqSpace API","title":"SeqSpace.cylinder²","text":"cylinders²(x)\n\nGenerate the matrix of pairwise distances between vectors x, assuming the points are distributed on a cylinder. x is assumed to be d \times N where d denotes the dimensionality of the vector and N denotes the number. The first coordinate of x is assumed to be the polar coordinate.\n\n\n\n\n\n","category":"method"},{"location":"lib/seqspace/#SeqSpace.euclidean²","page":"SeqSpace API","title":"SeqSpace.euclidean²","text":"euclidean²(x)\n\nGenerate the matrix of pairwise distances between vectors x, assuming the Euclidean metric. x is assumed to be d \times N where d denotes the dimensionality of the vector and N denotes the number.\n\n\n\n\n\n","category":"function"},{"location":"lib/seqspace/#SeqSpace.extendfit-Tuple{Result, Any, Any}","page":"SeqSpace API","title":"SeqSpace.extendfit","text":"extendfit(result::Result, input, epochs)\n\nRetrain model within result on input data for epochs more iterations. Returns a new Result.\n\n\n\n\n\n","category":"method"},{"location":"lib/seqspace/#SeqSpace.fitmodel-Tuple{Any, Any}","page":"SeqSpace API","title":"SeqSpace.fitmodel","text":"fitmodel(data, param; D²=nothing, chatty=true)\n\nTrain an autoencoder model, specified with param hyperparams, to fit data. data is assumed to be sized d \times N where d and N are dimensionality and cardinality respectively. If not nothing, D² is assumed to be a precomputed distance matrix of point cloud data. If chatty is true, function will print to stdout. Returns a Result type.\n\n\n\n\n\n","category":"method"},{"location":"lib/seqspace/#SeqSpace.linearprojection-Tuple{Any, Any}","page":"SeqSpace API","title":"SeqSpace.linearprojection","text":"linearprojection(x, d; Δ=1, Λ=nothing)\n\nProject an empirical distance matrix x onto d top principal components. Centers the result to have zero mean. Returns the projection, as well as a function to transform a projected vector back to the embedding space. Ignores top Δ principal components. If Λ is not nothing, assumes it is a precomputed SVD decomposition.\n\n\n\n\n\n","category":"method"},{"location":"lib/seqspace/#SeqSpace.marshal-Tuple{Result}","page":"SeqSpace API","title":"SeqSpace.marshal","text":"marshal(r::Result)\n\nSerialize a trained autoencoder to binary format suitable for disk storage. Store parameters of model as contiguous array\n\n\n\n\n\n","category":"method"},{"location":"lib/seqspace/#SeqSpace.unmarshal-Tuple{Result}","page":"SeqSpace API","title":"SeqSpace.unmarshal","text":"unmarshal(r::Result)\n\nDeserialize a trained autoencoder from binary format to semantic format. Represents model as a collection of functors.\n\n\n\n\n\n","category":"method"},{"location":"lib/normalize/#scRNAseq-Normalization","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"","category":"section"},{"location":"lib/normalize/#Types","page":"scRNAseq Normalization","title":"Types","text":"","category":"section"},{"location":"lib/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"Modules = [SeqSpace.Normalize]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/normalize/#SeqSpace.Normalize.FitType","page":"scRNAseq Normalization","title":"SeqSpace.Normalize.FitType","text":"FitType = NamedTuple{\n(\n :likelihood,\n :parameters,\n :uncertainty,\n :residual\n),\nTuple{Float64, Vector{Float64}, Vector{Float64}, Vector{Float64}}\n\nStores the result of MLE fit of one gene.\n\n\n\n\n\n","category":"type"},{"location":"lib/normalize/#Functions","page":"scRNAseq Normalization","title":"Functions","text":"","category":"section"},{"location":"lib/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"Modules = [SeqSpace.Normalize]\nOrder = [:function]","category":"page"},{"location":"lib/normalize/#SeqSpace.Normalize.bootstrap-Tuple{Any, Any}","page":"scRNAseq Normalization","title":"SeqSpace.Normalize.bootstrap","text":"bootstrap(count, depth; stochastic=negativebinomial, samples=50)\n\nEmpirically verify the MLE fit of count, using a GLM model generated by stochastic with confounding depth variables by bootstrap. One third of cells are removed and the parameters are re-estimated with the remaining cells. This process is repeated samples times. The resultant distribution of estimation is returned.\n\n\n\n\n\n","category":"method"},{"location":"lib/normalize/#SeqSpace.Normalize.fit-Tuple{Any, Any, Any}","page":"scRNAseq Normalization","title":"SeqSpace.Normalize.fit","text":"fit(stochastic, count, depth)\n\nFit a generative model stochastic to gene expression count data, assuming confounding sequencing depth. stochastic can be either negativebinomial or gamma.\n\n\n\n\n\n","category":"method"},{"location":"lib/normalize/#SeqSpace.Normalize.gamma-Tuple{Any, Any}","page":"scRNAseq Normalization","title":"SeqSpace.Normalize.gamma","text":"gamma(count, depth)\n\nCompute the log likelihood of a gamma distributed generalized linear model (GLM) with log link function for the estimated mean count of a single gene. The sequencing depth for each sequenced cell is assumed to be the only confounding variables.\n\n\n\n\n\n","category":"method"},{"location":"lib/normalize/#SeqSpace.Normalize.glm-Tuple{Any}","page":"scRNAseq Normalization","title":"SeqSpace.Normalize.glm","text":"glm(data; stochastic=negativebinomial, ϵ=1)\n\nFit a generalized linear model (GLM) to the matrix data. Genes are assumed to be on rows, cells over columns. The underlying generative model is passed by stochastic.\n\n\n\n\n\n","category":"method"},{"location":"lib/normalize/#SeqSpace.Normalize.logmean","page":"scRNAseq Normalization","title":"SeqSpace.Normalize.logmean","text":"logmean(x;ϵ=1)\n\nCompute the geometric mean: xpleft(langle logleft(x + epsilon right) rangleright) - epsilon\n\n\n\n\n\n","category":"function"},{"location":"lib/normalize/#SeqSpace.Normalize.logvar","page":"scRNAseq Normalization","title":"SeqSpace.Normalize.logvar","text":"logmean(x;ϵ=1)\n\nCompute the geometric variance: expleft(langle logleft(x + epsilon right)^2 rangle_cright) - epsilon\n\n\n\n\n\n","category":"function"},{"location":"lib/normalize/#SeqSpace.Normalize.negativebinomial-Tuple{Any, Any}","page":"scRNAseq Normalization","title":"SeqSpace.Normalize.negativebinomial","text":"negativebinomial(count, depth)\n\nCompute the log likelihood of a negative binomial generalized linear model (GLM) with log link function for count of a single gene. The sequencing depth for each sequenced cell is assumed to be the only confounding variables.\n\n\n\n\n\n","category":"method"},{"location":"lib/normalize/#SeqSpace.Normalize.prior-Tuple{Any}","page":"scRNAseq Normalization","title":"SeqSpace.Normalize.prior","text":"prior(params)\n\nEstimate a generalized normal distribution to params by maximum likelihood estimation. In practice, used to compute the empirical prior for overdispersion factor Θ₃ in the negative binomial.\n\n\n\n\n\n","category":"method"},{"location":"lib/model/#Autoencoder-Manifold-Learning","page":"Autoencoder Manifold Learning","title":"Autoencoder Manifold Learning","text":"","category":"section"},{"location":"lib/model/#Types","page":"Autoencoder Manifold Learning","title":"Types","text":"","category":"section"},{"location":"lib/model/","page":"Autoencoder Manifold Learning","title":"Autoencoder Manifold Learning","text":"Modules = [SeqSpace.ML]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/model/#SeqSpace.ML.LayerIterator","page":"Autoencoder Manifold Learning","title":"SeqSpace.ML.LayerIterator","text":"struct LayerIterator\n    width     :: Array{Int}\n    dropout   :: Set{Int}\n    normalize :: Set{Int}\n    σᵢ        :: Function\n    σₒ        :: Function\n    σ         :: Function\nend\n\nAn iterator used to generate dense latent layers within a neural network. width denotes the widths of each layer; the length of this array immediately determines the depth. dropout denotes the layers, as given by width that are followed by a dropout layer. normalize denotes the layers, as given by width that are followed by a batch normalization layer. σᵢ, σₒ, σ is the activation energy on the first, last, and intermediate layers respectively.\n\n\n\n\n\n","category":"type"},{"location":"lib/model/#Functions","page":"Autoencoder Manifold Learning","title":"Functions","text":"","category":"section"},{"location":"lib/model/","page":"Autoencoder Manifold Learning","title":"Autoencoder Manifold Learning","text":"Modules = [SeqSpace.ML]\nOrder = [:function]","category":"page"},{"location":"lib/model/#SeqSpace.ML.batch-Tuple{Any, Any}","page":"Autoencoder Manifold Learning","title":"SeqSpace.ML.batch","text":"batch(data, n)\n\nRandomly partition data into groups of size n.\n\n\n\n\n\n","category":"method"},{"location":"lib/model/#SeqSpace.ML.model-Tuple{Any, Any}","page":"Autoencoder Manifold Learning","title":"SeqSpace.ML.model","text":"model(dᵢ, dₒ; Ws=Int[], normalizes=Int[], dropouts=Int[], σ=elu)\n\nInitialize an autoencoding neural network with input dimension dᵢ and latent layers dₒ. Ws specifies both the width and depth of the encoder layer - the width of each layer is given as an entry in the array while the length specifies the depth. normalizes and dropouts denote which layers are followed by batch normalization and dropout specifically. The decoder layer is given the mirror symmetric architecture.\n\n\n\n\n\n","category":"method"},{"location":"lib/model/#SeqSpace.ML.train!-NTuple{4, Any}","page":"Autoencoder Manifold Learning","title":"SeqSpace.ML.train!","text":"train!(model, data, index, loss; B=64, η=1e-3, N=100, log=noop)\n\nTrains autoencoder model on data by minimizing loss. index stores the underlying indices of data used for training. Will mutate the underlying parameters of model. Optional parameters include:\n\nB denotes the batch size to be used.\nN denotes the number of epochs.\nη denotes the learning rate.\n\n\n\n\n\n","category":"method"},{"location":"lib/model/#SeqSpace.ML.update_dimension-Tuple{Any, Any}","page":"Autoencoder Manifold Learning","title":"SeqSpace.ML.update_dimension","text":"update_dimension(model, dₒ; ϵ = 1e-6)\n\nAdd a colection of new neurons in the encoding layer to encode in the encoding layer to increase dimensions to dₒ. Model weights for the initial dimensions are kept the same.\n\n\n\n\n\n","category":"method"},{"location":"lib/model/#SeqSpace.ML.validate-Tuple{Any, Any}","page":"Autoencoder Manifold Learning","title":"SeqSpace.ML.validate","text":"validate(data, len)\n\nReserve len samples from data during training process to allow for model validation.\n\n\n\n\n\n","category":"method"},{"location":"#SeqSpace","page":"Home","title":"SeqSpace","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A fast, self-contained Julia library to normalize scRNAseq data and learn its underlying geometric structure in both a supervised and unsupervised fashion.","category":"page"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"SeqSpace is an experimental Julia library and command line tool suite that will both normalize and subsequently learn/parameterize scRNAseq data to a low-dimensional manifold. Our approach is orthogonal to traditional scRNAseq pipelines such as UMAP or tSNE that focus primarily on clustering low-dimensional embeddings; we do not assume our data is drawn from categorical cell types. Instead, our methodology is intended to be used on datasets that are well described by a small number of continuous degrees of freedom, for example to measure:","category":"page"},{"location":"","page":"Home","title":"Home","text":"positional information of cells undergoing morphogenesis\ntime within cell cycle\ncellular aging","category":"page"},{"location":"","page":"Home","title":"Home","text":"However, at present, it has only been empirically validated on scRNAseq data obtained during early Drosophila embryogenesis.","category":"page"},{"location":"","page":"Home","title":"Home","text":"There are many loosely connected library modules that are designed to help parameterize low-dimensional scRNAseq data in some capacity . This documentation is written to both help navigate across these different modules, as well as to describe and motivate the algorithmic design in detail. The main functionality contained within the codebase is:","category":"page"},{"location":"","page":"Home","title":"Home","text":"scRNAseq normalization\nscRNAseq pointcloud operations\nsupervised scRNAseq spatial mapping\nunsupervised scRNAseq manifold learning","category":"page"},{"location":"","page":"Home","title":"Home","text":"We refer the interested reader to both our in-depth algorithmic expositions as well as the library API documentation for more details.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"There are multiple ways to install the SeqSpace library","category":"page"},{"location":"#From-Julia-REPL","page":"Home","title":"From Julia REPL","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"    (@v1.x) pkg> add https://github.com/nolln/seqspace.git","category":"page"},{"location":"#From-Command-Line","page":"Home","title":"From Command Line","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"    julia -e 'using Pkg; Pkg.add(\"https://github.com/nolln/seqspace.git\"); Pkg.build()'","category":"page"},{"location":"#Local-Environment","page":"Home","title":"Local Environment","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Clone the repository.","category":"page"},{"location":"","page":"Home","title":"Home","text":"    git clone https://github.com/nolln/seqspace.git && cd seqspace","category":"page"},{"location":"","page":"Home","title":"Home","text":"Build the package. This will create a seperate Julia environment for SeqSpace","category":"page"},{"location":"","page":"Home","title":"Home","text":"    julia --project=. -e 'using Pkg; Pkg.build()'","category":"page"},{"location":"","page":"Home","title":"Home","text":"Enter the REPL","category":"page"},{"location":"","page":"Home","title":"Home","text":"    julia --project=.","category":"page"},{"location":"#Citing","page":"Home","title":"Citing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"TBA","category":"page"},{"location":"sci/inference/#scRNAseq-Spatial-Inference","page":"scRNAseq Spatial Inference","title":"scRNAseq Spatial Inference","text":"","category":"section"},{"location":"sci/inference/#Introduction","page":"scRNAseq Spatial Inference","title":"Introduction","text":"","category":"section"},{"location":"sci/inference/","page":"scRNAseq Spatial Inference","title":"scRNAseq Spatial Inference","text":"In order to understand the regulation of morphogenesis, it is paramount to understand both the dynamics of gene expression at the single-cell level, as well as, interactions between the transcriptomic state of neighboring cells. With the advent of single-cell sequencing technology, it has now become possible to directly measure the transcriptome of cellular aggregates at cellular resolution, however, parameterizing such data by space remains challenging. Specifically, in the process of isolating individual cells for downstream sequencing, embryonic cells must be dissociated and suspended in liquid. This prevents straightforward application of scRNAseq technology to the problems of Developmental Biology.","category":"page"},{"location":"sci/inference/","page":"scRNAseq Spatial Inference","title":"scRNAseq Spatial Inference","text":"The straightforward resolution to this conundrum is to leverage pre-existing databases of known in-situ markers. Such curated datasets should provide the spatial expression profiles for a small subset of genes that could be matched against scRNAseq counts measured from the same organism at the same stage of development. Once such a subset of genes are \"matched\" between the in-situ and scRNAseq data, the spatial pattern for each gene for the remainder of the transcriptome would come for \"free.\" The collection of estimated gene expression patterns would function as a high-resolution atlas over physical space of the embryo's shape; a potentially important resource for researchers.","category":"page"},{"location":"sci/inference/","page":"scRNAseq Spatial Inference","title":"scRNAseq Spatial Inference","text":"Additionally, such an inference would provide us with an estimate of the position on the embryo each cell was sampled from. Assuming one has the ability to detect the intrinsic manifold of gene expression, as detailed elsewhere, such positional labels would provide an interesting overlay to attempt to learn the genotype to space map encoded by the genome. Below we provide an (incomplete) overview of past attempts at this problem, as well as detail our attempt to infer the position of scRNAseq cells.","category":"page"},{"location":"sci/inference/#Overview-of-current-methods","page":"scRNAseq Spatial Inference","title":"Overview of current methods","text":"","category":"section"},{"location":"sci/inference/","page":"scRNAseq Spatial Inference","title":"scRNAseq Spatial Inference","text":"FILL ME OUT There are published techniques that attempt to solve this problem, however our attempts to utilize them were unsuccessful.","category":"page"},{"location":"sci/inference/#Our-approach","page":"scRNAseq Spatial Inference","title":"Our approach","text":"","category":"section"},{"location":"sci/inference/#Objective-function","page":"scRNAseq Spatial Inference","title":"Objective function","text":"","category":"section"},{"location":"sci/inference/","page":"scRNAseq Spatial Inference","title":"scRNAseq Spatial Inference","text":"Let alpha and a denote indices over scRNAseq cells and embryonic spatial positions. Our goal is to solve for the sampling probability distribution rho_aalpha, i.e. the probability that cell alpha was sampled from position a. We pose the language of regularized optimal transport, and thus the solution found at the extrema of the following objective function","category":"page"},{"location":"sci/inference/","page":"scRNAseq Spatial Inference","title":"scRNAseq Spatial Inference","text":"    tag1 F(vecg_alpha vecG_a) equiv displaystylesumlimits_alphaa C_aalphaleft(vecg_alphavecG_aright)rho_aalpha + Trho_aalphalogleft(rho_aalpharight)","category":"page"},{"location":"sci/inference/","page":"scRNAseq Spatial Inference","title":"scRNAseq Spatial Inference","text":"where vecg_alpha and vecG_a denote the transcriptomic state of cell alpha and position a respectively. In the parlance of optimal transport, C_aalpha is a cost matrix - it denotes the energy required to map cell \\alpha onto position a. For now, we leave the functional form general but understood to implicitly depend upon the gene expression of both sequenced cell and in-situ position. T is a hyperparameter akin to thermodynamic \"temperature\"; it controls the precision that we demand of the inferred position for each sequenced cell. As T rightarrow 0, each cell must bijectively map onto a spatial position; the problem reduces to the assignment problem Conversely, as T rightarrow infty, the entropic term dominates; the solution is the uniform distribution.","category":"page"},{"location":"sci/inference/","page":"scRNAseq Spatial Inference","title":"scRNAseq Spatial Inference","text":"If extremized as written, Eq. (1) would not result in a well-formed probability distribution. Specifically, we must constrain the row and column sum of rho_aalpha to have correctly interpreted marginals. In order for rho_aalpha to be interpreted as the probability that cell alpha was sampled from position a, forall alpha  sum_a rho_aalpha = 1 must hold. Additionally, we assume there were no biases in cellular isolation and thus each position was sequencing uniformally, and thus impose uniform coverage forall a  sum_alpha rho_aalpha = fracN_cN_x. N_x and N_c denote the number of in-situ positions and sequenced cells respectively. Taken together, our full Free Energy is of the form","category":"page"},{"location":"sci/inference/","page":"scRNAseq Spatial Inference","title":"scRNAseq Spatial Inference","text":"    tildeF(vecg_alphavecG_a)equivdisplaystylesumlimits_alphaa C_aalphaleft(vecg_alphavecG_aright)rho_aalpha + Trho_aalphalogleft(rho_aalpharight) + displaystylesum_aLambda_aleftfracN_cN_x-sum_alpha rho_aalpha right + displaystylesum_alpha lambda_alphaleft1-sum_arho_aalpha right","category":"page"},{"location":"sci/inference/","page":"scRNAseq Spatial Inference","title":"scRNAseq Spatial Inference","text":"The solution is found to be","category":"page"},{"location":"sci/inference/","page":"scRNAseq Spatial Inference","title":"scRNAseq Spatial Inference","text":"    tag2 rho_aalpha^* = e^lambda_a e^-T^-1left(C_aalphaleft(vecg_alphavecG_aright)-1right) e^Lambda_alpha","category":"page"},{"location":"sci/inference/","page":"scRNAseq Spatial Inference","title":"scRNAseq Spatial Inference","text":"where lambda_a and Lambda_alpha can be found by utilizing the Sinkhorn-Knopp algorithm in conjunction with the marginal constraints prescribed above. Eq. (2) provides a fast, scalable algorithm to estimate the sampling posterior given a cost matrix C_aalpha(vecg_avecG_alpha). All that remains is to formulate an explicit model.","category":"page"},{"location":"sci/inference/#Cost-matrix","page":"scRNAseq Spatial Inference","title":"Cost matrix","text":"","category":"section"},{"location":"sci/inference/","page":"scRNAseq Spatial Inference","title":"scRNAseq Spatial Inference","text":"As seen by Eq. (2), the cost matrix can be viewed as the energy of a Boltzmann distribution: C_aalpha-1 equiv Eleft(vecg_alpha vecG_aright) Hence, an obvious interpretation of E(vecg_alpha vecG_a) is as the negative log-likelihood that vecg_alpha and vecG_a were sampled from the sample entity. Our first simplifying assumption is that genes within the database are statistically independent of each other and thus the energy is additive","category":"page"},{"location":"sci/inference/","page":"scRNAseq Spatial Inference","title":"scRNAseq Spatial Inference","text":"    E(vecg_alpha vecG_a) = frac1N_gdisplaystylesumlimits_i=1^N_g varepsilonleft(g_alpha i G_airight)","category":"page"},{"location":"sci/inference/","page":"scRNAseq Spatial Inference","title":"scRNAseq Spatial Inference","text":"where i indexes genes and varepsilon denotes the single-body energetics. Thus the problem has been reduced to parameterizing the log-likelihood that g_alpha and G_a where sampled from the sample underlying cell. However, the complication is that our scRNAseq data and the in-situ expression database are not directly relatable; both data are in manifestly different unit systems! Furthermore, there is no a priori obvious functional relationship one can use for regression to ultimately transform counts in one dataset to another.","category":"page"},{"location":"sci/inference/","page":"scRNAseq Spatial Inference","title":"scRNAseq Spatial Inference","text":"Overview:","category":"page"},{"location":"sci/inference/","page":"scRNAseq Spatial Inference","title":"scRNAseq Spatial Inference","text":"Look at each CDF of individual gene.\nOptimal transport of each 1D distribution: can be solved analytically.\nDenote cdf of i^th gene of scRNA and in-situ by phi_i and Phi_i respectively\nProvides us a unique map from one space to another that minimizes distortions in the observed distributions.\nAssume Gaussian sampling probability.\nMean is given by the in-situ database.\nVariance of Gaussian simply renormalizes the temperature of the original problem. Can unambiguously set to 1.\nResult is","category":"page"},{"location":"sci/inference/","page":"scRNAseq Spatial Inference","title":"scRNAseq Spatial Inference","text":"    varepsilonleft(g_alpha i G_airight) equiv left(Phi^-1_ileft(phi_ileft(g_alpha iright)right) - G_airight)^2","category":"page"},{"location":"sci/inference/#Results","page":"scRNAseq Spatial Inference","title":"Results","text":"","category":"section"},{"location":"sci/inference/#Discussion","page":"scRNAseq Spatial Inference","title":"Discussion","text":"","category":"section"},{"location":"sci/inference/","page":"scRNAseq Spatial Inference","title":"scRNAseq Spatial Inference","text":"Unfortunately, such databases only exist for a select few model organisms and thus limit the applicability of this approach. Our hope is to leverage phenomenology gleaned from the correlations between expression and space for organisms with such a database that ","category":"page"}]
}
