var documenterSearchIndex = {"docs":
[{"location":"lib/io/#Data-Input/Output","page":"Data Input/Output","title":"Data Input/Output","text":"","category":"section"},{"location":"lib/io/#Types","page":"Data Input/Output","title":"Types","text":"","category":"section"},{"location":"lib/io/","page":"Data Input/Output","title":"Data Input/Output","text":"Modules = [SeqSpace.DataIO]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/io/#SeqSpace.DataIO.PLYProp","page":"Data Input/Output","title":"SeqSpace.DataIO.PLYProp","text":"struct PLYProp\n    name :: Symbol\n    type :: Type\n    len  :: Union{Type,Nothing}\nend\n\nData structure that encapsulates a property of a PLY file. If len is not nothing, it is assumed to be a collection of properties, i.e. a list.\n\n\n\n\n\n","category":"type"},{"location":"lib/io/#SeqSpace.DataIO.plytype","page":"Data Input/Output","title":"SeqSpace.DataIO.plytype","text":"const plytype = Dict{String, Type}\n\nLookup table for keywords of PLY file format matched to type information.\n\n\n\n\n\n","category":"constant"},{"location":"lib/io/#Functions","page":"Data Input/Output","title":"Functions","text":"","category":"section"},{"location":"lib/io/","page":"Data Input/Output","title":"Data Input/Output","text":"Modules = [SeqSpace.DataIO]\nOrder = [:function]","category":"page"},{"location":"lib/io/#SeqSpace.DataIO.expand_matrix-Tuple{IO, String}","page":"Data Input/Output","title":"SeqSpace.DataIO.expand_matrix","text":"expand_matrix(io::IO, dir::String)\n\nRead a matrix in MTX format from io stream and dump as 3 text files into directory dir.\n\n\n\n\n\n","category":"method"},{"location":"lib/io/#SeqSpace.DataIO.fill!-Tuple{Array{SeqSpace.DataIO.PLYProp}, IO}","page":"Data Input/Output","title":"SeqSpace.DataIO.fill!","text":"fill!(prop::Array{PLYProp}, io::IO)\n\nRead and parse a single line from io stream. Fill the interpreted property into prop.\n\n\n\n\n\n","category":"method"},{"location":"lib/io/#SeqSpace.DataIO.read_matrix-Tuple{IO}","page":"Data Input/Output","title":"SeqSpace.DataIO.read_matrix","text":"read_matrix(io::IO; type=Float64, named_cols=false, named_rows=false, start_cols=1, start_rows=1)\n\nRead a white-space delimited matrix from io stream of element type type. If named_cols is true, the first line is assumed to be column labels. If named_rows is true, the first column of each row is assumed to be denote the label.\n\n\n\n\n\n","category":"method"},{"location":"lib/io/#SeqSpace.DataIO.read_mtx-Tuple{IO}","page":"Data Input/Output","title":"SeqSpace.DataIO.read_mtx","text":"read_mtx(io::IO)\n\nRead a matrix from io stream assuming bytes are formatted in matrix exchance file format.\n\n\n\n\n\n","category":"method"},{"location":"lib/io/#SeqSpace.DataIO.read_obj-Tuple{IO}","page":"Data Input/Output","title":"SeqSpace.DataIO.read_obj","text":"read_obj(io::IO)\n\nRead an oriented mesh from io stream assuming bytes are formatted in OBJ file format.\n\n\n\n\n\n","category":"method"},{"location":"lib/io/#SeqSpace.DataIO.read_ply-Tuple{IO}","page":"Data Input/Output","title":"SeqSpace.DataIO.read_ply","text":"read_ply(io::IO)\n\nRead an oriented mesh from io stream assuming bytes are formatted in PLY file format.\n\n\n\n\n\n","category":"method"},{"location":"lib/io/#SeqSpace.DataIO.readprops-Tuple{IO, Vector{SeqSpace.DataIO.PLYProp}}","page":"Data Input/Output","title":"SeqSpace.DataIO.readprops","text":"readprops(io::IO, props::Array{PLYProp,1})\n\nRead a collection of properties from io stream into props array. Return a named tuple of parsed properties.\n\n\n\n\n\n","category":"method"},{"location":"lib/pointcloud/#Point-Cloud","page":"Point Cloud","title":"Point Cloud","text":"","category":"section"},{"location":"lib/pointcloud/#Types","page":"Point Cloud","title":"Types","text":"","category":"section"},{"location":"lib/pointcloud/","page":"Point Cloud","title":"Point Cloud","text":"Modules = [SeqSpace.PointCloud]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.Edge","page":"Point Cloud","title":"SeqSpace.PointCloud.Edge","text":"struct Edge{T <: Real}\n    verts    :: Tuple{Int, Int}\n    distance :: T\nend\n\nConnects two neighboring verts by an edge of length distance.\n\n\n\n\n\n","category":"type"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.Graph","page":"Point Cloud","title":"SeqSpace.PointCloud.Graph","text":"struct Graph{T <: Real}\n    verts :: Array{Vertex{T},1}\n    edges :: Array{Edge{T},1}\nend\n\nA generic graph data structure containing vertices (points in space) stored within verts connected by edges.\n\n\n\n\n\n","category":"type"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.Vertex","page":"Point Cloud","title":"SeqSpace.PointCloud.Vertex","text":"struct Vertex{T <: Real}\n    position :: Array{T}\nend\n\nRepresents a single cell within a larger point cloud. The embedding space is normalized gene expression.\n\n\n\n\n\n","category":"type"},{"location":"lib/pointcloud/#Functions","page":"Point Cloud","title":"Functions","text":"","category":"section"},{"location":"lib/pointcloud/","page":"Point Cloud","title":"Point Cloud","text":"Modules = [SeqSpace.PointCloud]\nOrder = [:function]","category":"page"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.adjacency_list-Tuple{SeqSpace.PointCloud.Graph}","page":"Point Cloud","title":"SeqSpace.PointCloud.adjacency_list","text":"adjacency_list(G :: Graph)\n\nReturn the flattened adjacency list for graph G.\n\n\n\n\n\n","category":"method"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.dijkstra!-Tuple{Any, Any, Any}","page":"Point Cloud","title":"SeqSpace.PointCloud.dijkstra!","text":"dijkstra!(dist, adj, src)\n\nCompute the shortest path from src to all other points given adjacency list adj and distances dist using Dijkstra's algorithm.\n\n\n\n\n\n","category":"method"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.floyd_warshall-Tuple{SeqSpace.PointCloud.Graph}","page":"Point Cloud","title":"SeqSpace.PointCloud.floyd_warshall","text":"floyd_warshall(G :: Graph)\n\nCompute the shortest path from all vertices to all other vertices within graph G.\n\n\n\n\n\n","category":"method"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.geodesics-Tuple{Any, Any}","page":"Point Cloud","title":"SeqSpace.PointCloud.geodesics","text":"geodesics(x, k; D=missing, accept=(d)->true, sparse=true)\n\nCompute the matrix of pairwise distances, given a pointcloud x and neighborhood cutoff k, from the resultant neighborhood graph. If sparse is true, it will utilize Dijkstra's algorithm, individually for each point. If sparse is false, it will utilize the Floyd Warshall algorithm.\n\n\n\n\n\n","category":"method"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.geodesics-Tuple{SeqSpace.PointCloud.Graph}","page":"Point Cloud","title":"SeqSpace.PointCloud.geodesics","text":"geodesics(G :: Graph; sparse=true)\n\nCompute the matrix of pairwise distances, given a neighborhood graph G, weighted by local Euclidean distance. If sparse is true, it will utilize Dijkstra's algorithm, individually for each point. If sparse is false, it will utilize the Floyd Warshall algorithm.\n\n\n\n\n\n","category":"method"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.isomap-Tuple{Any, Any}","page":"Point Cloud","title":"SeqSpace.PointCloud.isomap","text":"isomap(x, dₒ; k=12, sparse=true)\n\nCompute the isometric embedding of point cloud x into dₒ dimensions. Geodesic distances between all points are estimated by utilizing the shortest path defined by the neighborhood graph. The embedding is computed from a multidimensional scaling analysis on the resultant geodesics.\n\n\n\n\n\n","category":"method"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.mds-Tuple{Any, Any}","page":"Point Cloud","title":"SeqSpace.PointCloud.mds","text":"mds(D², dₒ)\n\nComputes the lowest dₒ components from a Multidimensional Scaling analysis given pairwise squared distances D².\n\n\n\n\n\n","category":"method"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.neighborhood-Union{Tuple{T}, Tuple{Any, T}} where T<:AbstractFloat","page":"Point Cloud","title":"SeqSpace.PointCloud.neighborhood","text":"neighborhood(x, k :: T; D=missing, accept=(d)->true) where T <: AbstractFloat\n\nConstructs a neighborhood graph of all neighbors within euclidean distance k for each point of cloud x. If D is given, it is assumed to be a dense matrix of pairwise distances.\n\n\n\n\n\n","category":"method"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.neighborhood-Union{Tuple{T}, Tuple{Any, T}} where T<:Integer","page":"Point Cloud","title":"SeqSpace.PointCloud.neighborhood","text":"neighborhood(x, k :: T; D=missing, accept=(d)->true) where T <: Integer\n\nConstructs a neighborhood graph of the k nearest neighbor for each point of cloud x. If D is given, it is assumed to be a dense matrix of pairwise distances.\n\n\n\n\n\n","category":"method"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.scaling-Tuple{Any, Any}","page":"Point Cloud","title":"SeqSpace.PointCloud.scaling","text":"scaling(D, N)\n\nEstimate the Hausdorff dimension by computing how the number of points contained within balls scales with varying radius.\n\n\n\n\n\n","category":"method"},{"location":"lib/pointcloud/#SeqSpace.PointCloud.upper_tri-Tuple{Any}","page":"Point Cloud","title":"SeqSpace.PointCloud.upper_tri","text":"upper_tri(x)\n\nReturns the upper triangular portion of matrix x.\n\n\n\n\n\n","category":"method"},{"location":"lib/infer/#Supervised-Spatial-Inference","page":"Supervised Spatial Inference","title":"Supervised Spatial Inference","text":"","category":"section"},{"location":"lib/infer/#Types","page":"Supervised Spatial Inference","title":"Types","text":"","category":"section"},{"location":"lib/infer/","page":"Supervised Spatial Inference","title":"Supervised Spatial Inference","text":"Modules = [SeqSpace.Inference]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/infer/#Functions","page":"Supervised Spatial Inference","title":"Functions","text":"","category":"section"},{"location":"lib/infer/","page":"Supervised Spatial Inference","title":"Supervised Spatial Inference","text":"Modules = [SeqSpace.Inference]\nOrder = [:function]","category":"page"},{"location":"lib/infer/#SeqSpace.Inference.cost-Tuple{Any, Any}","page":"Supervised Spatial Inference","title":"SeqSpace.Inference.cost","text":"cost(ref, qry; α=1, β=1, γ=0, ω=nothing)\n\nReturn the cost matrix J_ilpha associated to matching cells in qry to cells in ref. The cost matrix is computed by a heuristic distance between quantiles. Deprecated.\n\n\n\n\n\n","category":"method"},{"location":"lib/infer/#SeqSpace.Inference.cost_simple-Tuple{Any, Any}","page":"Supervised Spatial Inference","title":"SeqSpace.Inference.cost_simple","text":"cost_simple(ref, qry)\n\nReturn the cost matrix J_ialpha associated to matching cells in qry to cells in ref. The cost matrix is computed by hamming distance between cells via transforming quantiles to continuous spin variables. Deprecated.\n\n\n\n\n\n","category":"method"},{"location":"lib/infer/#SeqSpace.Inference.cost_transform-Tuple{Any, Any}","page":"Supervised Spatial Inference","title":"SeqSpace.Inference.cost_transform","text":"cost_transform(ref, qry; ω=nothing, ν=nothing)\n\nReturn the cost matrix J_ialpha associated to matching cells in qry to cells in ref. The cost matrix is computed by:\n\nTransforming the qry distribution to the ref distribution.\nLooking at the SSE across transformed genes.\n\nUse this unless you know what you are doing.\n\n\n\n\n\n","category":"method"},{"location":"lib/infer/#SeqSpace.Inference.inversion-Tuple{Any, Any}","page":"Supervised Spatial Inference","title":"SeqSpace.Inference.inversion","text":"inversion(counts, genes; ν=nothing, ω=nothing, refdb=nothing)\n\nInfer the original position of scRNAseq data counts where genes, given by genes are arranged along rows. The sampling probability over space is computed by regularized optimal transport by comparing to the Berkeley Drosophila Transcription Network Project database. The cost matrix is determined by summing over the 1D Wasserstein metric over all genes within the BDTNP databse. Returns the inversion as a function of inverse temperature.\n\n\n\n\n\n","category":"method"},{"location":"lib/infer/#SeqSpace.Inference.sinkhorn-Tuple{Matrix{Float64}}","page":"Supervised Spatial Inference","title":"SeqSpace.Inference.sinkhorn","text":"sinkhorn(M::Array{Float64,2};\n              a::Maybe{Array{Float64}} = missing,\n              b::Maybe{Array{Float64}} = missing,\n              maxᵢ::Integer            = 1000,\n              τ::Real                  = 1e-5,\n              verbose::Bool            = false\n)\n\nRescale matrix M to have row & column marginals a and b respectively. Will terminate either when constraints are held to within tolerance τ or the number of iterations exceed maxᵢ.\n\n\n\n\n\n","category":"method"},{"location":"lib/infer/#SeqSpace.Inference.transform-Tuple{Any, Any, Any}","page":"Supervised Spatial Inference","title":"SeqSpace.Inference.transform","text":"transform(src, dst, ν)\n\nTransform distribution src to distribution dst by minimizing the Wasserstein metric. This is equivalent to x to F^-1_dstleft(F_srcleft(xright)right) where F denotes the cumulative density function.\n\n\n\n\n\n","category":"method"},{"location":"lib/infer/#SeqSpace.Inference.virtualembryo-Tuple{}","page":"Supervised Spatial Inference","title":"SeqSpace.Inference.virtualembryo","text":"virtualembryo(;directory=\"/home/nolln/mnt/data/drosophila/dvex\")\n\nLoad the Berkeley Drosophila Transcriptional Network Project database. directory should be path to folder containing two folders:\n\nbdtnp.txt.gz    : gene expression over point cloud of virtual cells\ngeometry.txt.gz : spatial position (x,y,z) of point cloud of virtual cells.\n\n\n\n\n\n","category":"method"},{"location":"lib/generate/#Point-Cloud-Generation","page":"Point Cloud Generation","title":"Point Cloud Generation","text":"","category":"section"},{"location":"lib/generate/#Types","page":"Point Cloud Generation","title":"Types","text":"","category":"section"},{"location":"lib/generate/","page":"Point Cloud Generation","title":"Point Cloud Generation","text":"Modules = [SeqSpace.Generate]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/generate/#Functions","page":"Point Cloud Generation","title":"Functions","text":"","category":"section"},{"location":"lib/generate/","page":"Point Cloud Generation","title":"Point Cloud Generation","text":"Modules = [SeqSpace.Generate]\nOrder = [:function]","category":"page"},{"location":"lib/generate/#SeqSpace.Generate.sphere-Tuple{Any}","page":"Point Cloud Generation","title":"SeqSpace.Generate.sphere","text":"sphere(N; R=1)\n\nGenerate a spherical point cloud of N points with extent of radius R.\n\n\n\n\n\n","category":"method"},{"location":"lib/generate/#SeqSpace.Generate.swissroll-Tuple{Any}","page":"Point Cloud Generation","title":"SeqSpace.Generate.swissroll","text":"swissroll(N; z₀=10, R=1/20)\n\nGenerate a point cloud of N distributed on a swiss roll manifold with unit radius and length racz₀R.\n\n\n\n\n\n","category":"method"},{"location":"lib/generate/#SeqSpace.Generate.torus-Tuple{Any}","page":"Point Cloud Generation","title":"SeqSpace.Generate.torus","text":"torus(N; R=2, r=1)\n\nGenerate a point cloud of N distributed on a torus, sized inner r and outer radius R respectively.\n\n\n\n\n\n","category":"method"},{"location":"lib/util/#Utilities","page":"Utilities","title":"Utilities","text":"","category":"section"},{"location":"lib/util/#Types","page":"Utilities","title":"Types","text":"","category":"section"},{"location":"lib/util/","page":"Utilities","title":"Utilities","text":"Modules = [SeqSpace.scRNA.Utility]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/util/#Functions","page":"Utilities","title":"Functions","text":"","category":"section"},{"location":"lib/util/","page":"Utilities","title":"Utilities","text":"Modules = [SeqSpace.scRNA.Utility]\nOrder = [:function]","category":"page"},{"location":"lib/util/#SeqSpace.scRNA.Utility.sinkhorn-Tuple{Any}","page":"Utilities","title":"SeqSpace.scRNA.Utility.sinkhorn","text":"sinkhorn(A; r=[], c=[], maxit=1000, δ=1e-6, verbose=false)\n\nCompute the row and column multiplicative factors that constrain marginals of A to unity. Returns a boolean flag if algorithm converges within tolerance δ within maxit iterations. If r is given and non-empty, row marginals will be constrained to user-supplied inputs. If c is given and non-empty, column marginals will be constrained to user-supplied inputs.\n\n\n\n\n\n","category":"method"},{"location":"sci/autoencode/#Manifold-learning","page":"Manifold learning","title":"Manifold learning","text":"","category":"section"},{"location":"sci/autoencode/#Introduction","page":"Manifold learning","title":"Introduction","text":"","category":"section"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"In section scRNAseq spatial inference, we demonstrated that scRNAseq expression data can be directly mapped to space using a reference database  of gene expression patterns, provided by the Berkeley Drosophila Transcriptional Network Project. These results motivate a more ambitious question: is positional information directly encoded within gene expression and, as such, can we utilize just scRNAseq counts to infer cellular spatial position de-novo?","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"We anchor our inference approach on the continuum ansatz of cellular states in a developing embryo. Specifically, we postulate that gene expression is a smoothly varying, continuous function of space, and potentially other continuous degrees of freedom such as time. Said another way, neighboring cells are assumed to be infinitesimally \"close\" in expression. This formulation is a departure from the conventional, discrete, view of cellular fates taken, for example, in the description of the striped gene patterning observed during the Anterior-Posterior segmentation of the Drosophila melongaster embryo. From our viewpoint, a \"discontinuity\" of gene expression in a handful of components can also be explained by the curvature of a smooth surface embedded in very high dimensions.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"As formulated, spatial inference from scRNAseq data is equivalent to non-linear dimensional reduction: we want to find a small number of degrees of freedom that parameterize the variation across sim 10^4 genes. From Drosophila results, we know that at least minimally, we can describe gene expression within early Drosophila embryogenesis by 31 relevant components. In this section, we analyze the data within this subspace to show that the data is ultimately generated by an even smaller set of degrees of freedom. Furthermore, we outline a protocol to \"learn\" this parameterization and show that it can recapitulate space.","category":"page"},{"location":"sci/autoencode/#Empirical-analysis","page":"Manifold learning","title":"Empirical analysis","text":"","category":"section"},{"location":"sci/autoencode/#Hausdorff-dimension-estimation","page":"Manifold learning","title":"Hausdorff dimension estimation","text":"","category":"section"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"A critical parameter to determine empirically is the underlying dimensionality of gene expression of early _Drosophila embryogenesis, as given by the scRNAseq data. Informally, we expect that if the data is sampled from an underlying d dimensional manifold, than the number of points n enclosed by a sphere of radius R should grow as","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"    n(R) sim R^d","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"However, we note that performing this comparison utilizing the Euclidean metric between normalized cellular expression would be manifestly incorrect; we postulated that gene expression is a manifold which implies a Euclidean metric locally between neighbors. There is no a priori reason to expect Euclidean distances to be a good measure for far cells. Similar considerations hold for other postulated global metrics. Instead, we estimate geodesic distances empirically, in analog to the Isomap algorithm [1].","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"[1]: A Global Geometric Framework for Nonlinear Dimensionality Reduction","category":"page"},{"location":"sci/autoencode/#Neighborhood-graph-construction","page":"Manifold learning","title":"Neighborhood graph construction","text":"","category":"section"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"The first task is to formulate an algorithm to approximate the metric space the point cloud is sampled from and subsequently utilize our estimate to compute all pairwise distances. We proceed guided by intuition gleaned from Differential Geometry: pairwise distances within local neighborhoods are expected to be well-described by a Euclidean metric in the tangent space. Conversely, macroscopic distances can only be computed via integration against the underlying metric along the corresponding geodesic. We denote D_alphabeta as the resultant pairwise distances between cell alphabeta.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"The basic construction of the neighborhood graph is demonstrated in the below cartoon.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"<p align=\"center\">\n<img src=\"/assets/drosophila/radius_scaling_neighborhood.svg\" width=\"32%\" class=\"center\"/>\n<img src=\"/assets/drosophila/radius_scaling_shortest_path.svg\" width=\"32%\" class=\"center\"/>\n<img src=\"/assets/drosophila/radius_scaling_ball.svg\" width=\"32%\" class=\"center\"/>\n</p>","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Our algorithm to construct the neighborhood graph proceeds in three main steps. First, we visit the local neighborhood of each point (cell) within our dataset. In principle, the neighborhood can be defined by either a fixed number of neighbors k or a given radius R. In practice, for Drosophila melongaster we chose to define neighborhood by k=8. The neighborhood is assumed to be a good approximation to the manifold tangent space and thus pairwise distances between the point and its neighbors are computed within the euclidean metric. Each pairwise neighborhood relationship is captured by an edge, weighted by the pairwise distance. The collection of all cells, and their neighborhood edges, constitute the neighborhood graph.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Given a neighborhood graph as defined above, computing the \"geodesic\" distance between cells reduces to finding the shortest path between all pairs of points. This can be solved using either the Floyd-Warshall algorithm in mathcalO(N^3) time [2] or iteratively using Dijikstra's algorithm in mathcalO(N(Nlog N + E)) time [3]. We chose the later due to the sparsity of the neighborhood graph. The estimation of geodesic distances, if performed as described, asymptotically approaches the true value as the density of points increases [4].","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"[2]: Algorithm 97: shortest path","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"[3]: A note on two problems in connexion with graphs","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"[4]: Graph approximations to geodesics on embedded manifolds","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Given our estimate for all pairwise geodesic distances, we can now empirically estimate the Hausdorff dimension. Simply stated, the estimate is done by analyzing the scaling relationship between number of points enclosed within a geodesic ball of radius R and the radius R itself. Below we show the results plotted for each cell within our dataset, along with a trend line n(R) sim R^3 shown in red.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"<p align=\"center\">\n<img src=\"/assets/drosophila/pointcloud_scaling.png\" width=\"68%\" class=\"center\"/>\n</p>","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"The manifold clearly looks three-dimensional.","category":"page"},{"location":"sci/autoencode/#Physical-proximity-implies-expression-proximity","page":"Manifold learning","title":"Physical proximity implies expression proximity","text":"","category":"section"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"In Neighborhood graph construction, we demonstrated that the normalized scRNAseq appears to be three-dimensional. This supports half of our continuum ansatz: gene expression of early Drosophila embryogenesis is consistent with being distributed along a low-dimensional manifold. However, a priori is is unclear that such dimensions meaningfully correspond to space. Additionally, at the developmental stage sampled, the Drosophila embryo is a two-dimensional epithelial monolayer and as such it is unclear what a potential third dimension would parameterize. To this end, we utilize the positional labels calculated in scRNAseq spatial inference to assay if physically close cells are close along the putative manifold.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"<p align=\"center\">\n<img src=\"/assets/drosophila/pointcloud_locality.png\" width=\"68%\" class=\"center\"/>\n</p>","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Specifically, we demonstrate that conditioning on varying average spatial geodesic distances results in quantitative effects in the distribution of pairwise expression geodesic distances. Physically close cells are not only close in expression, but the median of the distribution increases quantitatively (albeit non-linearly) with increasing conditioned shells of distance. Taken together, the scRNAseq data is empirically consistent with our continuum expression ansatz.","category":"page"},{"location":"sci/autoencode/#Isomap-dimensional-reduction","page":"Manifold learning","title":"Isomap dimensional reduction","text":"","category":"section"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"We provide additional evidence in support of our underlying hypothesis by utilizing the Isomap algorithm [1]. As such, we simply perform a Principal Coordinates Analysis on the geodesic distances estimated in Neighborhood graph construction. This is tantamount to finding a set of low-dimensional coordinates xi_alpha that minimize the strain between their Euclidean embedding and the given distance matrix D_alphabeta.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"    E(xi_alpha) = left(fracleft(sum_alphabeta D_alphabeta - xi_alpha - xi_beta^2right)^2sum_alphabeta D_alphabeta^2 right)^12","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"<p align=\"center\">\n<img src=\"/assets/drosophila/isomap_AP.png\" width=\"42%\" class=\"center\"/>\n<img src=\"/assets/drosophila/isomap_DV.png\" width=\"42%\" class=\"center\"/>\n</p>","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Above, we show the resultant embedding, colored by the estimated AP (anterior-posterior) and DV (dorsal-vental) position, obtained by averaging over the distribution obtained in scRNAseq spatial inference. It is clear that two of the major axes of the embedding quantitatively segregate both spatial axes of the embryo. We emphasize that the estimated spatial positions were not used to generate the shown embedding, but rather only the underlying neighborhood distances of cells within our scRNAseq dataset. This is taken as strong evidence in support of our underlying hypothesis.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"By varying the dimensionality of the Isomap embedding, we see that three dimensions is the \"knee\" of dimensioning returns, consistent with the 3-dimensional scaling observed above.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"<p align=\"center\">\n<img src=\"/assets/drosophila/isomap_dimension.png\" width=\"68%\" class=\"center\"/>\n</p>","category":"page"},{"location":"sci/autoencode/#De-novo-manifold-learning","page":"Manifold learning","title":"De-novo manifold learning","text":"","category":"section"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"All considerations taken together, we wish to formulate an unsupervised method for nonlinear, dimensional reduction. The natural choice for network architecture is thus an autoencoder [5], shown graphically below.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"<p align=\"center\">\n<img src=\"/assets/autoencode/auto_encoder.svg\" width=\"68%\" class=\"center\"/>\n</p>","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"However, in order to enforce the continuum expression ansatz, the neural network should also explicitly conserve the topology of our neighborhood graph; neighborhood rankings should be preserved under the identified mapping. Lastly, we constrain the learned latent representation to be uniformally sampled. This can be viewed as an additional assumption on top of our continuum expression ansatz. Specifically, we assume cells were sampled from the embryonic positions uniformally, i.e. without bias, and, as such, restrict our view to uniformally sampled parameterizations of the underlying expression manifold.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"[5]: Modular learning in neural networks","category":"page"},{"location":"sci/autoencode/#Network-architecture","page":"Manifold learning","title":"Network architecture","text":"","category":"section"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"The input to the neural network was the 35 statistically significant components determined in Drosophila results. The encoder layer was designed to be 100 neurons wide and 6 levels deep with dense connections between each layer. In order to prevent overfitting and accelerate training of deep layers, batch normalization was utilized between each latent layer [6]. Linear and exponential linear unit activation functions were chosen for the input/output and latent layers respectively. The decoder layer was taken to be mirror symmetric with respect to the encoding layer for simplicity. Training occurred on 80 of the data, batched into groups of 128 cells. Validation of the resultant network was performed on the remaining 20 to ensure parameters were not overfit. We note that the final results presented here were determined to not be highly sensitive to the specific architectural details outlined here.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"[6]: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.","category":"page"},{"location":"sci/autoencode/#Topological-conservation","page":"Manifold learning","title":"Topological conservation","text":"","category":"section"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"In order to learn an interpretable latent space representation of the intrinsic gene expression manifold, we wish to constrain the estimated pullback to conserve the topology of the input scRNAseq data. This immediately poses the question: what topological features do we wish to preserve from the data to latent space and how do we empirically measure them? The answers immediately determine the additional terms one must add to the objective function used for training.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"We opt to utilize an explicitly geometric formalism that will implicitly constrain topology. The intuition for this choice is guided by Differential Geometry: a metric tensor uniquely defines a distance function between any two points on a manifold; the topology induced by this distance function will always coincide with the original topology of the manifold. Thus, by imposing preservation of pairwise distances in the latent space relative to the data, we implicitly conserve topology. It is important to note that this assumes our original scRNAseq data is sampled from a metric space that we have access to. We note that there have been recent promising attempts at designing loss functions parameterized by explicit topological invariants formulated by Topological Data Analysis, e.g. persistent homology. Lastly, one could envision having each network layer operate on a simplicial complex, rather than a flat vector of real numbers, however it is unclear how to parameterize the feed-forward function.","category":"page"},{"location":"sci/autoencode/#Isometric-formulation","page":"Manifold learning","title":"Isometric formulation","text":"","category":"section"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"The most straightforward manner to preserve distances between the input data and the latent representation is to impose isometry, i.e. distances in both spaces quantitatively agree. This would be achieved by supplementing the objective function with an Isomap analog","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"E_iso sim displaystylesumlimits_alphabeta left(D_alphabeta - leftleft xi_alpha - xi_beta rightright right)^2","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Utilizing this term is problematic for several reasons:","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Large distances dominate the energetics and as such large-scale features of the intrinsic manifold will be preferentially fit.\nGenerically, d dimensional manifolds can not be isometrically embedded into mathbbR^d, e.g. the sphere into the plane. This is seen empirically by the empirical three dimensional scaling but long tail of dimensions seen in the isomap correlation.\nIt trusts the computed distances quantitatively. We simply want close cells to be close in the resultant latent space.","category":"page"},{"location":"sci/autoencode/#Monometric-formulation","page":"Manifold learning","title":"Monometric formulation","text":"","category":"section"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Consider a vector psi_alpha of scores of length n we wish to rank. Furthermore, define sigma in Sigma_n to be an arbitrary permutation of n such scores. We define the argsort to be the permutation that sorts psi in descending order","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"    barsigmaleft(bmpsiright) equiv left(sigma_1left(bmpsiright)sigma_nleft(bmpsiright)right) qquad textsuch that qquad\n    psi_barsigma_1 ge psi_barsigma_1 ge  ge psi_barsigma_n","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"The definition of the sorted vector of scores barbmpsi_alpha equiv psi_barsigma_alpha thus follows naturally. Lastly, the rank of vector bmpsi is defined as the inverse permutation of argsort.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"    Rleft(bmpsiright) equiv barsigma^-1left(bmpsiright)","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"We wish to devise an objective function that contains functions of the rank of some latent space variables. However, R(bmpsi) is a non-differentiable function; it maps a vector in mathbbR^n to a permutation of n items. Hence, we can not directly utilize the rank in a loss function as there is no way to backpropagate gradient information to the network parameters. In order to rectify this limitation, we first reformulate the ranking problem as a linear programming problem that permits efficient regularization. Note, the presentation here follows closely the original paper [7]","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"[7]: Fast Differentiable Sorting and Ranking","category":"page"},{"location":"sci/autoencode/#Linear-program-formulation","page":"Manifold learning","title":"Linear program formulation","text":"","category":"section"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"The sorting and ranking problem can be formulated as discrete optimization over the set of n-permutations Sigma_n","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"    barsigmaleft(bmpsiright) equiv undersetbmsigmainSigma_nmathrmargmax  displaystylesumlimits_alpha psi_sigma_alpha rho_alpha","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"    Rleft(bmpsiright)\n    equiv barsigmaleft(bmpsiright)^-1 \n    equiv leftundersetbmsigmainSigma_nmathrmargmax  displaystylesumlimits_alpha psi_sigma_alpha rho_alpha right^-1\n    equiv leftundersetbmsigma^-1inSigma_nmathrmargmax  displaystylesumlimits_alpha psi_alpha rho_sigma^-1_alpha right^-1\n    equiv undersetbmpiinSigma_nmathrmargmax  displaystylesumlimits_alpha psi_alpha rho_pi(alpha)","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"where rho_alpha equiv left(n n-1  1right) In order to regularize the problem, and thus allow for continuous optimization, we imagine the convex hull of all permutations induced by an arbitrary vector bmomega in mathbbR^n.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"    Omegaleft(bmomegaright) equiv textconvhullleftleftbmomega_sigma_alpha sigma in Sigma_n rightright subset mathbbR^n","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"This is often referred to as the permutahedron of bmomega; it is a convex polytope in n-dimensions whose vertices are the permutations of bmomega It follows directly from the fundamental theorem of linear programming, that the solution will almost surely be achieved at the vertex. Thus the above discrete formulation can be rewritten as an optimization over continuous vectors contained on the permutahedron","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"    bmpsi_barsigmaleft(bmpsiright) equiv undersetbmomegainOmegaleft(bmpsiright)mathrmargmax  bmomegacdotbmrho\n    qquad\n    bmrho_Rleft(bmpsiright) equiv undersetbmomegainOmegaleft(bmrhoright)mathrmargmax  bmpsicdotbmomega","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Utilizing the fact that rho_Rleft(bmpsiright) = Rleft(-bmpsiright)","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"    Rleft(bmpsiright) equiv -undersetbmomegainOmegaleft(bmrhoright)mathrmargmax  bmpsicdotbmomega","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Unfortunately, since bmpsi appears in the rank objective function, any small perturbation in bmpsi can force the solution of the linear program to discontinuously transition to another vertex. As such, in its current form, it is still not differentiable. Note, this is not true for the sorted vector, it appears in the constraint polyhedron; it has a unique Jacobian and can be directly used in neural networks. The only way to proceed is to introduce convex regularization.","category":"page"},{"location":"sci/autoencode/#Regularization","page":"Manifold learning","title":"Regularization","text":"","category":"section"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"We revise our objective function by Euclidean projection and thus introduce quadratic regularization on the norm of the solution. Specifically, we define the soft rank operators as the extrema of the objective function","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"    tildeRleft(bmpsiright) equiv undersetbmomegainOmegaleft(bmrhoright)mathrmargmax left -bmpsicdotbmomega\n    - fracepsilon2leftleftomegarightright^2 right","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Note that the limit epsilon rightarrow 0 reproduces the linear programming formulation of the rank operator introduced above. Conversely, in the limit epsilon rightarrow infty, the solution will go to a constant vector that has the smallest modulus on the permutahedron.","category":"page"},{"location":"sci/autoencode/#Solution","page":"Manifold learning","title":"Solution","text":"","category":"section"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"It has been demonstrated before that the above problem reduces to simple isotonic regression[7][8]. Specifically,","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Rleft(bmpsiright) = -fracbmpsiepsilon -\n    leftundersetomega_1 ge omega_2 ge  ge omega_nmathrmargmin\n    frac12 leftleftbmomega + bmrho + fracbmpsiepsilon rightright^2right_sigma^-1(bmpsi)\nequiv -fracbmpsiepsilon - tildebmomegaleft(bmpsibmrhoright)","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Importantly, isotonic regression is well-studied and can be solved in linear time. Furthermore, the solution admits a simple, calculatable Jacobian","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"partial_psi_alpha R_betaleft(bmpsiright)\n= frac-delta_alphabetaepsilon - partial_psi_alphatildeomega_betaleft(bmpsibmrhoright)\n= frac-delta_alphabetaepsilon - \n    beginpmatrix\n    bmB_1  bm0  bm0 \n    bm0    ddots  bm0 \n    bm0    bm0  bmB_m \n    endpmatrix_alphabeta","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"where bmB_i denotes the matrix corresponding to the i^th block obtained during isotonic regression. It is a constant matrix whose number of rows and columns equals the size of the block, and whose values all sum to 1. The Julia implementation can be found in the Differentiable rank section.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"[8]: SparseMAP: Differentiable Sparse Structured Inference","category":"page"},{"location":"sci/autoencode/#Loss-function","page":"Manifold learning","title":"Loss function","text":"","category":"section"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"The term of the loss function enforcing monometricity is thus taken to be","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"    E_x equiv fracsumlimits_alphabeta left(Rleft(D_alphabeta^2right) - Rleft(vecpsi_alpha-vecpsi_beta^2right)right)^2sqrtsumlimits_alphabetaR^2left(D_alphabeta^2right)sumlimits_alphabeta R^2left(vecpsi_alpha-vecpsi_beta^2right)","category":"page"},{"location":"sci/autoencode/#Uniform-sampling-of-latent-space","page":"Manifold learning","title":"Uniform sampling of latent space","text":"","category":"section"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"In order to learn an interpretable latent space representation of the intrinsic gene expression manifold, we also wish to constrain the estimated pullback to uniformally sample the latent space. When posed in the language of Optimal Transport, this is equivalent to a semi-discrete formulation of the Wasserstein distance, in which the point cloud in d dimensions is mapped optimally to 01^d. It can be shown [9] that this is equivalent to integrating the moment of inertia over the equal area power diagram in d dimensions, a computationally intensive procedure as the latent dimension grows.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"We seek a balance between analytical correctness and computational tractability. To this end, we forgo imposing that the full d-dimensional joint distribution is uniform; instead we impose that the one-dimensional marginals over each independent latent dimension is uniform. For one-dimensional distributions, the Wasserstein metric has a unique analytical expression in terms of the empirical cumulative density function F(x) and thus can be computed in linear time [10]. As such, the term in the loss function enforcing latent uniformity is taken to be","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"    E_u equiv frac1ddisplaystylesumlimits_d frac1N displaystylesumlimits_alpha left Fleft(Phi^-1left(vecz_alpharight)_dright) - Rleft(Phi^-1left(vecz_alpharight)_dright)right","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"[9]: Optimal Transport for Applied Mathematicians","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"[10]: Calculation of the Wasserstein Distance Between Probability Distributions on the Line","category":"page"},{"location":"sci/autoencode/#Network-objective-function","page":"Manifold learning","title":"Network objective function","text":"","category":"section"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Putting all terms together, we arrive at our final formulation of the loss function (where Phi and Phi^-1 denote the decoder and encoder stages respectively).","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"E equiv fracdisplaystylesumlimits_alpha leftvecz_alpha - Phileft(Phi^-1left(vecz_alpharight)right) right^2displaystylesumlimits_alpha vecz_alpha^2 + lambda_x E_x + lambda_u E_u","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Stated plainly, all three terms, in left-to-right order, seek to find a low-dimensional map that is (i) invertible, (ii) preserves topological neighborhood relationships, (iii) is uniformally sampled by our dataset. For all results shown below, we chose lambda_x=lambda_u=1 for simplicity; exploration of hyperparameter optimization would be an interesting future direction.","category":"page"},{"location":"sci/autoencode/#Swiss-roll-validation","page":"Manifold learning","title":"Swiss roll validation","text":"","category":"section"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Before continuing with analysis of the Drosophila melongaster analysis, we first verify our proposed novel manifold learning technique on a canonical surface, the two dimensional swiss roll, shown below.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"<p align=\"center\">\n<img src=\"/assets/swissroll/inputdata.png\" width=\"48%\" class=\"center\"/>\n<img src=\"/assets/swissroll/loss.png\" width=\"48%\" class=\"center\"/>\n</p>","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Specifically, we generated a 2D swiss roll, embedded in 30 dimensions, with 1 random Gaussian noise added to each component. The neural network, as specified in Network objective function, was minimized until convergence (shown above). The resultant 2D latent space, the output of the encoding stage, is shown below after a simple affine transformation to align the phi axis along the horizontal axis. Importantly, the uniformization constraint results in an approximately linear relationship between the known latent variables used to generate the 2D surface, and the latent parameterization variables.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"<p align=\"center\">\n<img src=\"/assets/swissroll/latent.png\" width=\"48%\" class=\"center\"/>\n<img src=\"/assets/swissroll/latent_correlation.png\" width=\"48%\" class=\"center\"/>\n</p>","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"Critically, the output of the decoder stage correlates with the input data such that R^2 sim 98. This held true for 10 independent trained networks with no observed large-scale non-linearity left in the latent space. We conclude that our construction accurately recapitulates the 2D intrinsic parameterization of the toy data.","category":"page"},{"location":"sci/autoencode/#Drosophila-manifold","page":"Manifold learning","title":"Drosophila manifold","text":"","category":"section"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"We now show the results of the minimization of the energy specified in Network objective function using the architecture outlined in Network architecture, with three latent dimensions. Below, we show a scatter plot of all scRNAseq cells transformed to the learned latent representation. The left and right plots are colored according to the estimated AP and DV positions respectively, as obtained by scRNAseq spatial inference. We emphasize here that we did not train on the spatial positions, but rather the continuity of space directly follows from imposing the continuity of gene expression in the latent space. Additionally, we did not constrain AP and DV axes, shown as red and green arrows in the triad, to be orthogonal. Rather, we simply looked for the directions in the latent space that maximally correlate with our estimated position. The resultant two directions have negligible overlap. The input data and the output of the decoder stage correlate such that R^2 sim 68, implying we can capture roughly sim 70  of the variance observed in normalized gene expression with just three latent dimensions.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"<p align=\"center\">\n<img src=\"/assets/autoencode/AP_colormap.png\" width=\"48%\" class=\"center\"/>\n<img src=\"/assets/autoencode/DV_colormap.png\" width=\"48%\" class=\"center\"/>\n</p>","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"We observe good linear agreement with our estimated AP/DV directions in the latent space and the estimated spatial position for each cell, as shown above. Our ability to resolve AP position is limited to a precision of roughly 08mm, much larger than the cellular resolution obtained utilizing FISH data of the 4 gap genes [11]. The causal driver behind this is likely multifaceted: (i) scRNAseq is fundamentally noisy (ii) we have a relatively small sized dataset, (iii) our estimates of embryonic position themselves are noisy values. It would be interesting to revisit this analysis leveraging a higher sequencing depth per cell allowable with modern scRNAseq techniques or utilizing novel spatial-omics to provide more exact positional values. Interestingly, our estimated DV axis appears to be qualitatively nonlinear with strongest predictive power at the poles and weak relationship in the bulk.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"The benefit of using an autoencoder network is that its is both generalizable and generative: we ultimately have a function that maps scRNAseq to a latent space and back out to the original expression domain. For example, we can \"generate\" novel scRNAseq data from our interpolated model by sampling points in the latent space and transforming the points through the decoder stage. More importantly, this allows us to directly leverage tools from vector calculus to estimate volumes of phase space. As shown in the below right figure, we observe both the anterior and posterior poles contain more \"volume\" of the gene expression manifold for a unit volume of physical space when compared to cells sampled from the lateral region. Assuming a constant density of states over the intrinsic gene expression manifold, this implies a higher density of cell fates at the head and tail of the embryo than the lateral ectoderm. This is in contrast to previous analyses done on the AP pattern of the 4 gap genes over the mid-sadigittal plane of the embryo which predicted a constant positional information [11]. We note that this is an interesting direction for future study.","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"[11]: Positional information, in bits","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"<p align=\"center\">\n<img src=\"/assets/autoencode/coordinate_scatter.png\" width=\"48%\" class=\"center\"/>\n<img src=\"/assets/autoencode/positional_info.png\" width=\"48%\" class=\"center\"/>\n</p>","category":"page"},{"location":"sci/autoencode/","page":"Manifold learning","title":"Manifold learning","text":"An interesting question to consider for future studies: what is the third latent dimension we discover within the scRNAseq data? We analyzed the genes that maximally correlate (negatively or positively) with this direction in the latent space, as shown by the blue arrow above. The below table captures some examples of the top hits for both signs. Theses genes are putatively ubiquitously expressed, consistent with the notion that this is an orthogonal axis to the directions of spatial variation. We postulate this direction is involved in the cell cycle as RNApolymerase, myosin light chain, and other mitotic markers form on \"pole\" of the axis, while ed and other negative growth regulators are enriched on the conjugate pole.","category":"page"},{"location":"lib/scrna/#scRNAseq-Data","page":"scRNAseq Data","title":"scRNAseq Data","text":"","category":"section"},{"location":"lib/scrna/#Types","page":"scRNAseq Data","title":"Types","text":"","category":"section"},{"location":"lib/scrna/","page":"scRNAseq Data","title":"scRNAseq Data","text":"Modules = [SeqSpace.scRNA]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/scrna/#SeqSpace.scRNA.Count","page":"scRNAseq Data","title":"SeqSpace.scRNA.Count","text":"struct Count{T <: Real} <: AbstractArray{T,2}\n    data :: Array{T,2}\n    gene :: Array{AbstractString,1}\n    cell :: Array{AbstractString,1}\nend\n\nData structure used to represent count data obtained during a scRNAseq sequencing experiment. Individual cells are stored as column vectors while expression of singular genes are obtained as row vectors. data contains the raw/normalized count matrix. gene and cell contain the row/column labels respectively. Genes and cells can be indexed either by integers or names, i.e. strings.\n\n\n\n\n\n","category":"type"},{"location":"lib/scrna/#Functions","page":"scRNAseq Data","title":"Functions","text":"","category":"section"},{"location":"lib/scrna/","page":"scRNAseq Data","title":"scRNAseq Data","text":"Modules = [SeqSpace.scRNA]\nOrder = [:function]","category":"page"},{"location":"lib/scrna/#Base.:∪-Union{Tuple{S}, Tuple{T}, Tuple{SeqSpace.scRNA.Count{T}, SeqSpace.scRNA.Count{S}}} where {T<:Real, S<:Real}","page":"scRNAseq Data","title":"Base.:∪","text":"∪(seq₁::Count{T}, seq₂::Count{S}) where {T <: Real, S <: Real}\n\nCollate count matrix seq₁ and seq₂ by taking the union across genes. Reorders rows of seq₂ to match gene names of seq₁. Additional genes in seq₂ not contained in seq₁ are added as augmented rows.\n\n\n\n\n\n","category":"method"},{"location":"lib/scrna/#SeqSpace.scRNA.:∩-Union{Tuple{S}, Tuple{T}, Tuple{SeqSpace.scRNA.Count{T}, SeqSpace.scRNA.Count{S}}} where {T<:Real, S<:Real}","page":"scRNAseq Data","title":"SeqSpace.scRNA.:∩","text":"∩(seq₁::Count{T}, seq₂::Count{S}) where {T <: Real, S <: Real}\n\nCollate count matrix seq₁ and seq₂ by taking the union across genes. Reorders rows of seq₂ to match gene names of seq₁. Only keeps genes present in both seq₁ and seq₂.\n\n\n\n\n\n","category":"method"},{"location":"lib/scrna/#SeqSpace.scRNA.filtercell-Tuple{Any, SeqSpace.scRNA.Count}","page":"scRNAseq Data","title":"SeqSpace.scRNA.filtercell","text":"filtercells(f, seq::Count)\n\nFilters cells of count matrix seq based upon column function f.\n\n\n\n\n\n","category":"method"},{"location":"lib/scrna/#SeqSpace.scRNA.filtergene-Tuple{Any, SeqSpace.scRNA.Count}","page":"scRNAseq Data","title":"SeqSpace.scRNA.filtergene","text":"filtergene(f, seq::Count)\n\nFilters genes of count matrix seq based upon row function f.\n\n\n\n\n\n","category":"method"},{"location":"lib/scrna/#SeqSpace.scRNA.generate-Tuple{Any, Any}","page":"scRNAseq Data","title":"SeqSpace.scRNA.generate","text":"generate(ngene, ncell; ρ=(α=Gamma(0.25,2), β=Normal(1,.01), γ=Gamma(3,3)))\n\nGenerate scRNAseq data assuming a monoclonal population of cells sampled against a heteroskedastic negative binomial model.\n\n\n\n\n\n","category":"method"},{"location":"lib/scrna/#SeqSpace.scRNA.load-Tuple{AbstractString}","page":"scRNAseq Data","title":"SeqSpace.scRNA.load","text":"load(dir::AbstractString; batch=missing)\n\nRead in scRNAseq experimental data from directory dir. The directory is expected to contain the following files:\n\nbarcodes.tsv : one cell name per line\nfeatures.tsv : one gene name per line\nmatrix.mtx : count matrix in mtx format\n\nIf batch is not missing, then it will be appended to each cell label.\n\n\n\n\n\n","category":"method"},{"location":"lib/distance/#Distances","page":"Distances","title":"Distances","text":"","category":"section"},{"location":"lib/distance/#Types","page":"Distances","title":"Types","text":"","category":"section"},{"location":"lib/distance/","page":"Distances","title":"Distances","text":"Modules = [SeqSpace.PointCloud.Distances]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/distance/#Functions","page":"Distances","title":"Functions","text":"","category":"section"},{"location":"lib/distance/","page":"Distances","title":"Distances","text":"Modules = [SeqSpace.PointCloud.Distances]\nOrder = [:function]","category":"page"},{"location":"lib/distance/#SeqSpace.PointCloud.Distances.euclidean-Tuple{Any}","page":"Distances","title":"SeqSpace.PointCloud.Distances.euclidean","text":"euclidean(X)\n\nCompute the pairwise distances between points X according to the Euclidean metric. Assumes X is sized d times N where d and N denote dimensionality and cardinality respectively.\n\n\n\n\n\n","category":"method"},{"location":"lib/distance/#SeqSpace.PointCloud.Distances.euclidean²-Tuple{Any}","page":"Distances","title":"SeqSpace.PointCloud.Distances.euclidean²","text":"euclidean²(X)\n\nCompute the pairwise squared distances between points X according to the Euclidean metric. Assumes X is sized d times N where d and N denote dimensionality and cardinality respectively.\n\n\n\n\n\n","category":"method"},{"location":"lib/distance/#SeqSpace.PointCloud.Distances.jensen_shannon-Tuple{Any}","page":"Distances","title":"SeqSpace.PointCloud.Distances.jensen_shannon","text":"jensen_shannon(P)\n\nCompute the pairwise distances between probability distributions P according to the Jensen-Shannon divergence. Assumes P is sized d times N where d and N denote dimensionality and cardinality respectively.\n\n\n\n\n\n","category":"method"},{"location":"lib/distance/#SeqSpace.PointCloud.Distances.kullback_liebler-Tuple{Any, Any}","page":"Distances","title":"SeqSpace.PointCloud.Distances.kullback_liebler","text":"kullback_liebler(p, q)\n\nCompute the pairwise distances between probability distributions p and q according to the Kullback-Liebler divergence. Assumes p and q are normalized.\n\n\n\n\n\n","category":"method"},{"location":"lib/queue/#Priority-Queue","page":"Priority Queue","title":"Priority Queue","text":"","category":"section"},{"location":"lib/queue/#Types","page":"Priority Queue","title":"Types","text":"","category":"section"},{"location":"lib/queue/","page":"Priority Queue","title":"Priority Queue","text":"Modules = [SeqSpace.PointCloud.PriorityQueue]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/queue/#SeqSpace.PointCloud.PriorityQueue.RankedQueue","page":"Priority Queue","title":"SeqSpace.PointCloud.PriorityQueue.RankedQueue","text":"struct RankedQueue{T <: Real, S <: Any}\n    rank :: Array{T, 1}\n    data :: Array{S, 1}\nend\n\nMaintains a priority queue of data. Each datum has a rank that determines it's priority in the queue. rank and data are sorted in ascending order.\n\n\n\n\n\n","category":"type"},{"location":"lib/queue/#Functions","page":"Priority Queue","title":"Functions","text":"","category":"section"},{"location":"lib/queue/","page":"Priority Queue","title":"Priority Queue","text":"Modules = [SeqSpace.PointCloud.PriorityQueue]\nOrder = [:function]","category":"page"},{"location":"lib/queue/#Base.insert!-Union{Tuple{S}, Tuple{T}, Tuple{SeqSpace.PointCloud.PriorityQueue.RankedQueue{T}, S, T}} where {T<:Real, S}","page":"Priority Queue","title":"Base.insert!","text":"insert!(q::RankedQueue{T}, data::S, rank::T) where {T <: Real, S <: Any}\n\nPush a new element data with priority rank onto the ranked queue q. Rotates the queue until priority is sorted in ascending order.\n\n\n\n\n\n","category":"method"},{"location":"lib/queue/#Base.take!-Tuple{SeqSpace.PointCloud.PriorityQueue.RankedQueue}","page":"Priority Queue","title":"Base.take!","text":"take!(q::RankedQueue)\n\nPop off the element with element with lowest rank/highest priority.\n\n\n\n\n\n","category":"method"},{"location":"lib/queue/#SeqSpace.PointCloud.PriorityQueue.left-Tuple{Any}","page":"Priority Queue","title":"SeqSpace.PointCloud.PriorityQueue.left","text":"left(i)\n\nReturn the index of the left child of node i.\n\n\n\n\n\n","category":"method"},{"location":"lib/queue/#SeqSpace.PointCloud.PriorityQueue.parent-Tuple{Any}","page":"Priority Queue","title":"SeqSpace.PointCloud.PriorityQueue.parent","text":"parent(i)\n\nReturn the index of the parent of node i.\n\n\n\n\n\n","category":"method"},{"location":"lib/queue/#SeqSpace.PointCloud.PriorityQueue.right-Tuple{Any}","page":"Priority Queue","title":"SeqSpace.PointCloud.PriorityQueue.right","text":"right(i)\n\nReturn the index of the right child of node i.\n\n\n\n\n\n","category":"method"},{"location":"lib/queue/#SeqSpace.PointCloud.PriorityQueue.rotatedown!-Tuple{SeqSpace.PointCloud.PriorityQueue.RankedQueue, Any}","page":"Priority Queue","title":"SeqSpace.PointCloud.PriorityQueue.rotatedown!","text":"rotatedown!(q::RankedQueue, i)\n\nModify the ranked queue by pushing down the node at index i until the priority is sorted.\n\n\n\n\n\n","category":"method"},{"location":"lib/queue/#SeqSpace.PointCloud.PriorityQueue.rotatedown!-Tuple{SeqSpace.PointCloud.PriorityQueue.RankedQueue}","page":"Priority Queue","title":"SeqSpace.PointCloud.PriorityQueue.rotatedown!","text":"rotatedown!(q::RankedQueue)\n\nModify the ranked queue by pushing down the root until the priority is sorted.\n\n\n\n\n\n","category":"method"},{"location":"lib/queue/#SeqSpace.PointCloud.PriorityQueue.rotateup!-Tuple{SeqSpace.PointCloud.PriorityQueue.RankedQueue, Any}","page":"Priority Queue","title":"SeqSpace.PointCloud.PriorityQueue.rotateup!","text":"rotateup!(q::RankedQueue, i)\n\nModify the ranked queue by pushing up the node at index i until the priority is sorted again.\n\n\n\n\n\n","category":"method"},{"location":"lib/queue/#SeqSpace.PointCloud.PriorityQueue.rotateup!-Tuple{SeqSpace.PointCloud.PriorityQueue.RankedQueue}","page":"Priority Queue","title":"SeqSpace.PointCloud.PriorityQueue.rotateup!","text":"rotateup!(q::RankedQueue)\n\nModify the ranked queue by pushing up the last element until the priority is sorted.\n\n\n\n\n\n","category":"method"},{"location":"lib/queue/#SeqSpace.PointCloud.PriorityQueue.update!-Union{Tuple{S}, Tuple{T}, Tuple{SeqSpace.PointCloud.PriorityQueue.RankedQueue{T, S}, S, T}} where {T<:Real, S}","page":"Priority Queue","title":"SeqSpace.PointCloud.PriorityQueue.update!","text":"update!(q::RankedQueue{T, S}, data::S, new::T) where {T <: Real, S <: Any}\n\nChange the priority of element data to rank new. Will panic if data is not contained in queue q.\n\n\n\n\n\n","category":"method"},{"location":"lib/rank/#Differentiable-Rank","page":"Differentiable Rank","title":"Differentiable Rank","text":"","category":"section"},{"location":"lib/rank/#Types","page":"Differentiable Rank","title":"Types","text":"","category":"section"},{"location":"lib/rank/","page":"Differentiable Rank","title":"Differentiable Rank","text":"Modules = [SeqSpace.SoftRank]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/rank/#Functions","page":"Differentiable Rank","title":"Functions","text":"","category":"section"},{"location":"lib/rank/","page":"Differentiable Rank","title":"Differentiable Rank","text":"Modules = [SeqSpace.SoftRank]\nOrder = [:function]","category":"page"},{"location":"lib/rank/#SeqSpace.SoftRank.isotonic-Tuple{Any}","page":"Differentiable Rank","title":"SeqSpace.SoftRank.isotonic","text":"isotonic(x)\n\nIsotonically regress on data vector x. Returns the monotonically increasing fit.\n\n\n\n\n\n","category":"method"},{"location":"lib/rank/#SeqSpace.SoftRank.partition-Tuple{Any}","page":"Differentiable Rank","title":"SeqSpace.SoftRank.partition","text":"partition(x; ϵ=1e-9)\n\nCompute the sizes of each partition of constant value obtained by isotonic regression of data x. ϵ denotes the tolerance for what is considered equal in floating point terms.\n\n\n\n\n\n","category":"method"},{"location":"lib/rank/#SeqSpace.SoftRank.projection-Tuple{Any}","page":"Differentiable Rank","title":"SeqSpace.SoftRank.projection","text":"projection(x)\n\nRegularize the ranking algorithm by euclidean projection onto the permutehedron.\n\n\n\n\n\n","category":"method"},{"location":"lib/rank/#SeqSpace.SoftRank.rank-Tuple{Any}","page":"Differentiable Rank","title":"SeqSpace.SoftRank.rank","text":"rank(x)\n\nRank the items of vector x in ascending order.\n\n\n\n\n\n","category":"method"},{"location":"lib/rank/#SeqSpace.SoftRank.softrank-Tuple{Any}","page":"Differentiable Rank","title":"SeqSpace.SoftRank.softrank","text":"softrank(x)\n\nRegularization of the ranking algorithm. Scaled euclidean projection onto the permutehedron.\n\n\n\n\n\n","category":"method"},{"location":"lib/rank/#SeqSpace.SoftRank.∇isotonic-Tuple{Any, Any}","page":"Differentiable Rank","title":"SeqSpace.SoftRank.∇isotonic","text":"∇isotonic(x)\n\nCompute the gradient of isotonic regression with respect to the inputs about the solution point.\n\n\n\n\n\n","category":"method"},{"location":"sci/normalize/#scRNAseq-normalization","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"","category":"section"},{"location":"sci/normalize/#Introduction","page":"scRNAseq normalization","title":"Introduction","text":"","category":"section"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"scRNA sequencing is a relatively novel technique added to the Biologist's toolkit that allows one to quantitatively probe the \"Statistical Mechanics\" of Biology. In contrast to traditional bulk RNAseq techniques, which provide population averages, scRNAseq technology measures the complete ensemble of cellular gene expression. Consequently, considerable activity has been focused on mapping the taxonomy of cell states; projects such as the Human Cell Atlas promise to provide a complete enumeration of microscopic cell types within the human body. However, despite recent progress, scRNAseq data have many numerous technical limitations and sources of statistical bias that must be considered before any downstream analysis. The sources of noise within the data include, but are not limited to:","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"Sequence depth bias: PCR and reverse transcription efficiency will vary across reactions, resulting in artificial variation of sequencing depth across different cells.\nAmplification bias: PCR primers are not perfectly random. As a result, some genes will amplify preferentially over others. This will distort the underlying expression distribution.\nBatch effect: scRNAseq runs have non-trivial distortions causing cells within a given sequencing run to have greater correlation within than across batches.\nDropout: Due to molecular competition in the underlying amplification reactions, some genes will fail to amplify during PCR due to early losses. This will lead to more zeros in the resulting count matrix than you would expect by chance.","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"These biases are approximately rectified by a preprocessing step known as normalization. At a coarse level, all such methods can be thought of as transforming the obtained count matrix from absolute numbers into an estimate of differential expression, i.e. a comparison of a count relative to the measured distribution. Such a procedure is analogous to that of the z-score of a univariate normally-distributed random variable; however, in the case of scRNAseq data, we don't have access to the underlying sampling distributions a priori, it must be estimated empirically. The choice of sampling prior, and details of how the distribution is estimated, delineate normalization practices.","category":"page"},{"location":"sci/normalize/#Overview-of-current-methods","page":"scRNAseq normalization","title":"Overview of current methods","text":"","category":"section"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"TODO: FILL OUT","category":"page"},{"location":"sci/normalize/#Our-approach","page":"scRNAseq normalization","title":"Our approach","text":"","category":"section"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"We model the count matrix n_alphai, where alpha and i indexes cells and genes respectively, obtained from scRNA sequencing as an unknown, low-rank mean mu_alphai with additive full-rank noise delta_alphai that captures all unknown technical noise and bias.","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"    tag1 n_alphai = mu_alphai + delta_alphai ","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"Our goal during the normalization procedure is two-fold:","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"Estimate the low-rank mean mu_alphai.\nEstimate the sampling variance langledelta_alphai^2rangle of the experimental noise. Brackets denote an average over (theoretical) realizations of sequencing. This will eventually require an explicit model.","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"<p align=\"center\">\n<figure>\n  <img src=\"/assets/drosophila/heteroskedastic.png\" width=\"79%\" />\n  <figurecaption>\n  Count matrix is heteroskedastic: variation of scales across genes (rows) and cells (columns)\n  </figurecaption>\n</figure>\n</p>","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"Given both quantities, normalization over gene and cell-specific biases can be achieved by imposing the variances of all marginal distributions to be one. Specifically, we rescale all counts by cell-specific c_alpha and gene-specific g_i factors tilden_alphai equiv c_alpha n_alphai g_i such that","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"    tag2 displaystylesumlimits_alpha langle tildedelta_alphai^2 rangle = displaystylesumlimits_alphac_alpha^2 langle delta_alphai^2 rangle g_i^2 = N_g quad textand quad displaystylesumlimits_i langle tildedelta_alphai^2 rangle = displaystylesumlimits_ic_alpha^2 langle delta_alphai^2 rangle g_i^2 = N_c","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"This system of equations can be solved by the Sinkhorn-Knopp algorithm, provided we have a model for langledelta_alphai^2rangle parameterized by measurables. Within this formalism, this choice of model fully determines the normalization scheme. Owing to dropout and other sources of overdispersion, as shown empircally in the figure below, we model scRNAseq counts as sampled from an empirically estimated Heteroskedastic Negative Binomial distribution.","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"<p align=\"center\">\n<figure>\n  <img src=\"/assets/drosophila/overdispersed_mean_vs_variance.png\" width=\"49%\" />\n  <img src=\"/assets/drosophila/overdispersed_zeros.png\" width=\"49%\" />\n  <figurecaption>\n  scRNAseq data for Drosophila is overdispersed.\n  </figurecaption>\n</figure>\n</p>","category":"page"},{"location":"sci/normalize/#Case-studies","page":"scRNAseq normalization","title":"Case studies","text":"","category":"section"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"Before detailing our explicit algorithm, it is helpful to consider a few simpler examples. In the following sections, homoskedastic is used to denote count matrices whose elements are  independent and identically distributed (iid), while heteroskedastic denotes more complicated scenarios where each element has a unique distribution.","category":"page"},{"location":"sci/normalize/#Homoskedastic-Gaussian","page":"scRNAseq normalization","title":"Homoskedastic Gaussian","text":"","category":"section"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"This is slight perturbation of canonical random matrix theory and will serve as a useful pedagogical starting point. Assume mu_alphai is a quenched, low-rank matrix and each element of delta_alphai is sampled from a Gaussian with zero mean and variance sigma^2. In this limit, Eq.(1) reduces to the well-studied spiked population covariance model.","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"If mu_alphai = 0, the spectral decomposition of the count matrix n_alphai would be given by the Marchenko-Pastur distribution asymptotically. As such, singular values would be bounded by barlambda equiv 1+sigmasqrtN_gN_c.","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"<p align=\"center\">\n<img src=\"/assets/drosophila/marchenko-pastur.png\" width=\"49%\" class=\"center\"/>\n</p>\n<p align=\"center\">\nMarchenko pastur distribution (orange) vs random matrix eigenvalues (blue)\n</p>","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"Now consider the case of a rank 1 mean, i.e. 1 cell type in the data.","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"    n_alphai = gamma x_alpha barx_i + delta_alphai","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"It has been shown[1][2] that this model exhibits the following asymptotic phase transition:","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"If gamma le barlambda, the top singular value of n_alphai converges to barlambda. Additionally, the overlap of the left and right eigenvector with x and barx respectively converge to 0.\nIf gamma  barlambda, the top singular value of n_alphai converges to gamma + barlambdagamma. Additionally, the overlap of the left and right eigenvector with x and barx respectively converge to 1-(barlambdagamma)^2","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"This procedure can generalized to higher rank spike-ins; sub-leading principal components can be found by simply subtracting the previously inferred component from the count matrix n_alphai. As such, we can only expect to meaningful measure the principal components of mu_alphai that fall above the sampling noise floor, given by the Marchenko-Pastur distribution. Consequently, this forces us to define the statistically significant rank of the count matrix. This is exhibited empirically below.","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"<p align=\"center\">\n<img src=\"/assets/drosophila/gaussian_svd.png\" width=\"49%\" class=\"center\"/>\n<img src=\"/assets/drosophila/gaussian_overlap.png\" width=\"49%\" class=\"center\"/>\n</p>","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"[1]: The singular values and vectors of low rank perturbations of large rectangular random matrices","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"[2]: The largest eigenvalue of rank one deformation of large Wigner matrices","category":"page"},{"location":"sci/normalize/#Heteroskedastic-Poisson","page":"scRNAseq normalization","title":"Heteroskedastic Poisson","text":"","category":"section"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"The above results have been shown to hold for non-Gaussian Wigner noise matrices[3], provided each element is still iid. This is manifestly not the case we care about; the normalization procedure must account for heterogeneous sampling variances across both cells and genes. However, it has been shown[4][5] that the distribution of eigenvalues converges almost surely to the Marchenko-Pastur distribution, provided the constraint of Eq.(2) is satisfied! In other words, the eigenvalues of a heteroskedastic matrix is expected converge to that of a random Wigner matrix, provided the row and column variances are uniform (set to unity for convenience).","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"For the remainder of this section, assume the count matrix is sampled from a Poisson distribution, such that langledelta_alphai^2rangle = mu_alphai. As n_alphai is an unbiased estimator for the mean mu_alphai, Eq (2) reduces to","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"displaystylesumlimits_alphac_alpha^2 n_alpha i g_i^2 = N_g quad textand quad displaystylesumlimits_ic_alpha^2 n_alphai g_i^2 = N_c","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"which provides an explicit system of equations to estimate the scaling factors c_alpha and g_i. Once obtained, tildemu_alphai can be estimated via singular value decomposition of c_alpha n_alphai g_i; all components with singular value greater than barlambda = sqrtN_c+sqrtN_g can be confidently attributed to the \"true mean\" mu_alphai while all other components fall amongst the noise. An example is shown below:","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"<p align=\"center\">\n<img src=\"/assets/drosophila/poisson_svd.png\" width=\"49%\" class=\"center\"/>\n<img src=\"/assets/drosophila/poisson_overlap.png\" width=\"49%\" class=\"center\"/>\n</p>","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"[3]: Asymptotics of Sample Eigenstructure for a Large Dimensional Spiked Covariance Model","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"[4]: Biwhitening Reveals the Rank of a Count Matrix","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"[5]: A Review of Matrix Scaling and Sinkhorn's Normal Form for Matrices and Positive Maps","category":"page"},{"location":"sci/normalize/#Heteroskedastic-Negative-Binomial","page":"scRNAseq normalization","title":"Heteroskedastic Negative Binomial","text":"","category":"section"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"A negative binomial distribution is often used to model overdispersed count data, i.e. a process in which the variance grows superlinearly with the mean. Canonically the distribution arises from the distribution of the number of successes (with probabiliy p) obtained after Theta_3 failures of a Bernoulli process. However, the generative stochastic process can equivalently be modelled as an underlying Poisson process in which the emission rate is itself a stochastic variable drawn from a Gamma distribution. This allows us to analytically continue Theta_3 from an integer to the reals. Provided we have estimated the mean mu_ialpha and the overdisperson factor Theta_3i, the unbiased estimator for the variance is given by","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"    tag3 langle delta_ialpha^2 rangle = mu_ialphaleft(frac1 + mu_ialphaTheta_3i1 + Theta_3iright)","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"Direct substitution into Eq. (2) would provide the necessary cell-specific c_alpha and gene-specific g_i scaling factors.","category":"page"},{"location":"sci/normalize/#Empirical-sampling-distribution","page":"scRNAseq normalization","title":"Empirical sampling distribution","text":"","category":"section"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"In order to normalize the estimated sampling variance, we must formulate a method to fit a negative binomial to the measured count data per gene. We parameterize the distribution as follows:","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"    pleft(nmuTheta_3right) = fracGammaleft(n+Theta_3right)Gammaleft(n+1right)Gammaleft(Theta_3right)left(fracmumu+Theta_3right)^nleft(fracTheta_3mu+Theta_3right)^Theta_3","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"All results, unless explicitly stated otherwise, are obtained empirically from scRNAseq data obtained during early Drosophila melongaster embryogenesis.","category":"page"},{"location":"sci/normalize/#Generalized-linear-model","page":"scRNAseq normalization","title":"Generalized linear model","text":"","category":"section"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"A central complication in modeling the counts of a given gene across cells with the above generative model is that there are confounding variables that require consideration. For the present discussion, we explictly model the hetereogeneous sequencing depth across cells: naively we expect that if we sequenced a given cell with 2times depth, each gene should scale accordingly. As such, we formulate a generalized linear model","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"    tag4 log mu_ialpha = Theta_1i + Theta_2i log chi_alpha","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"where chi_alpha denotes the total sequencing depth of cell alpha We note this formulation can be easily extended to account for other confounds such as batch effect or cell type. The likelihood function for cell alpha, gene i is","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"    pleft(n_ialphaTheta_1iTheta_2i Theta_3ichi_alpharight) = fracGammaleft(n_ialpha+Theta_3iright)Gammaleft(n_ialpha+1right)Gammaleft(Theta_3iright)left(fracmu_ialphamu_ialpha+Theta_3iright)^n_ialphaleft(fracTheta_3imu_ialpha+Theta_3iright)^Theta_3i","category":"page"},{"location":"sci/normalize/#Maximum-Likelihood-Estimation","page":"scRNAseq normalization","title":"Maximum Likelihood Estimation","text":"","category":"section"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"Theta_1i Theta_2i Theta_3i  represent 3N_g parameters we must infer from the data. We note that a priori this problem is overdetermined; our count matrix is sized N_g times N_c  and thus we attempt to estimate all parameters within a maximum likelihood framework. This is equivalent to minimizing (for each gene independently)","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"beginaligned\n    tag5 mathcalL_i = displaystylesumlimits_alpha\n        left(n_ialpha + Theta_3iright)logleft(e^Theta_1ichi_alpha^Theta_2i + Theta_3iright) \n      - Theta_3ilogleft(Theta_3iright) \n      - n_ialphaleft(Theta_1i + Theta_2ilog chi_alpharight)\n      - logleft(fracGammaleft(n_ialpha+Theta_3iright)Gammaleft(n_ialpha+1right)Gammaleft(Theta_3iright)right)\nendaligned","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"Empirically it was determined that to provide a robust estimate of all three parameters that our confounding variables chi_alpha are given by","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"    chi_alpha equiv expleft(langle logleft(n_ialpha+1right) rangleright) - 1","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"where angle brackets denote the empirical average over genes for cell alpha. Once the parameters Theta_1i Theta_2i Theta_3i  are estimated, Eqns. (2-4) can be utilized to estimate the normalized mean count matrix tildemu_ialpha. This will also immediately teach us the statistically significant rank of the counting matrix.","category":"page"},{"location":"sci/normalize/#Synthetic-data-verification","page":"scRNAseq normalization","title":"Synthetic data verification","text":"","category":"section"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"As a first check on the methodology, we generated toy counting data sampled from negative binomial distribution with low rank mean. After estimation of parameters by minimizing Eq. (5), we solved Eq. (2) utilizing our unbiased estimator given by Eq. (3). The result is shown below.","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"<p align=\"center\">\n<img src=\"/assets/drosophila/negbinom.png\" width=\"49%\" class=\"center\"/>\n<img src=\"/assets/drosophila/negbinom_mean.png\" width=\"49%\" class=\"center\"/>\n</p>","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"As shown, we underestimate the true rank as a few components have singular values below the Marchenko-Pastur noise floor. This, along with our noisy estimation of the overdispersion factor Theta_3 contributes to our noisier mean estimation when compared to the Poisson case above.","category":"page"},{"location":"sci/normalize/#Filter-uncertain-genes","page":"scRNAseq normalization","title":"Filter uncertain genes","text":"","category":"section"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"The uncertainty of our parameter estimates can be computed by the second derivative of the likelihood around our determined minima","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"    delta Theta_a^2 = leftpartial_Theta_bpartial_Theta_c mathcalLright^-1_aa","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"Unsurprisingly, the uncertainty of our estimates for parameters Theta_1i Theta_2i Theta_3i  is strongly dependent upon the underlying gene expression; our estimates for lowly expressed genes are highly uncertain. This can be seen in the below figure, which shows the scatter plot of the parameter estimate Theta_a versus its uncertainty delta Theta_a, colored by the average gene expression.","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"<p align=\"center\">\n<figure>\n  <img src=\"/assets/drosophila/nb_1_uncertainty_vs_expression.png\" width=\"32%\" />\n  <img src=\"/assets/drosophila/nb_2_uncertainty_vs_expression.png\" width=\"32%\" />\n  <img src=\"/assets/drosophila/nb_3_uncertainty_vs_expression.png\" width=\"32%\" />\n  <figurecaption>\n  Each point is a gene (row). Color of point determined by mean expression of gene.\n  </figurecaption>\n</figure>\n</p>","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"We note that the estimated Theta_1 monotonically grows with increasing gene expression, as was expected by construction. Conversely, the average estimate for Theta_2 shows no obvious trend against expression levels, however the uncertainty deltaTheta_2 monotonically increases with decreasing gene expression. The estimated Theta_3 also appears to be independent of average expression counts with a family of curves with increasing uncertainty defined by decreasing gene levels.","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"We capture the total uncertainty in our estimates by analyzing the trace of uncertainty delta Theta^2 = delta Theta_1^2 + delta Theta_2^2 + delta Theta_3^2 A scatter plot of the uncertainty versus average expression level is shown below. We see that expression level is an imperfect predictor of MLE uncertainty. The dashed cyan line denotes our chosen uncertainty cutoff used to remove genes from our count matrix. The right figure displays the cumulative density function for the filtered genes; as can be seen these are lowly expressed genes that are, in practice, 2 state variables.","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"<p align=\"center\">\n<figure>\n  <img src=\"/assets/drosophila/nb_total_uncertainty_vs_expression.png\" width=\"49%\" />\n  <img src=\"/assets/drosophila/nb_badfits.png\" width=\"49%\" />\n  <figurecaption>\n  Filter genes with bad fits. Genes with high uncertainty are determined to be lowly expressed.\n  </figurecaption>\n</figure>\n</p>","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"Interestingly, as shown below, the mean estimated value for Theta_2 is slightly higher than 1. This phenomenon persists across all gene expression levels.","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"<p align=\"center\">\n<figure>\n  <img src=\"/assets/drosophila/nb_param2.png\" width=\"49%\" />\n  <img src=\"/assets/drosophila/nb_param3.png\" width=\"49%\" />\n  <figurecaption>\n  Parameter distributions\n  </figurecaption>\n</figure>\n</p>","category":"page"},{"location":"sci/normalize/#Verify-estimates-via-bootstrap","page":"scRNAseq normalization","title":"Verify estimates via bootstrap","text":"","category":"section"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"In order to test that the method does not overfit the data, especially lowly expressed genes, we validated our estimates via bootstrap. Specifically, we re-ran our maximum likelihood estimation of Theta_1i Theta_2i Theta_3i  over subsamples of our given set of cells 100 times for each gene. The mean and standard deviation of the resultant empirical distribution was compared directly to our estimates and uncertainty calculations performed on the full dataset. As shown below, we find great quantitative agreement between the empirical estimates and our original calculations, suggesting strongly that we are not overfitting the data.","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"<p align=\"center\">\n<figure>\n  <img src=\"/assets/drosophila/bootstrap_1.png\" width=\"60%\" />\n  <img src=\"/assets/drosophila/bootstrap_2.png\" width=\"60%\" />\n  <img src=\"/assets/drosophila/bootstrap_3.png\" width=\"60%\" />\n</figure>\n</p>","category":"page"},{"location":"sci/normalize/#Drosophila-results","page":"scRNAseq normalization","title":"Drosophila results","text":"","category":"section"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"Just as we performed using the toy data, once parameters Theta_1i Theta_2i Theta_3i  are estimated, we can utilize Eqs. (2-5) to normalize the variance matrix and subsequently estimate the mean mu_ialpha. As shown below, we find there are sim 30 statistically significant linear dimensions in the scRNAseq data obtained during early Drosophila melongaster embryogenesis. Interestingly, while not fully delocalized as seen by the participation ratio of the \"noise\" components, we see that roughly sim 1000 genes contribute significantly to each component suggesting these are coarse \"pathways\" discovered.","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"<p align=\"center\">\n<img src=\"/assets/drosophila/rank_estimate.png\" width=\"49%\" class=\"center\"/>\n<img src=\"/assets/drosophila/participation_ratio.png\" width=\"49%\" class=\"center\"/>\n</p>","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"To ensure element-wise positivity, the estimated tildemu_ialpha is obtained by performing non-negative matrix factorization on the rescaled tilden_ialpha with rank 35. The factorization was initialized using the nndsvda algorithm [6] and minimized the least squares objective via a multiplicative update [7].","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"[6]: SVD based initialization: A head start for nonnegative matrix factorization","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"[7]: Algorithms for Non-negative Matrix Factorization","category":"page"},{"location":"sci/normalize/#Pearson-residuals","page":"scRNAseq normalization","title":"Pearson residuals","text":"","category":"section"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"Once obtained, tildemu_ialpha provides us an estimate for the rescaled mean of the expression of gene i for cell alpha. It is important to note, this will still have gene-specific and cell-specific scales by construction. However, our goal was originally to convert our raw count matrix into more natural units where variation across sequencing depth and gene expression are normalized out. As such, our normalization pipeline requires one last step: convert each rescaled mean into a z-score that measures expression differentially to the observed distribution across cells. Recall that a negative binomial stochastic model is equivalent to a Poisson-Gamma mixture, i.e. a poisson process whose mean is drawn from a Gamma distribution. To this end, we estimate the significance of each tildemu_ialpha by fitting a Gamma distribution across cells for each gene, in exactly the same way as we performed for the raw count data with a negative binomial distribution. Specifically, we take the mean to be given by Eq. (4) and minimize the analog of Eq. (5); we omit details here in the interest of brevity. This was determined to be an excellent stochastic model for the computed tildemu_ialpha, as shown by the linear quantile-quantile plot below.","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"<p align=\"center\">\n<img src=\"/assets/drosophila/gamma_qq.png\" width=\"49%\" class=\"center\"/>\n</p>","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"Taken together, our normalized gene count z_ialpha is given by (the mean and variance is given by the estimated Gamma distribution)","category":"page"},{"location":"sci/normalize/","page":"scRNAseq normalization","title":"scRNAseq normalization","text":"    tag6 z_ialpha equiv fractildemu_ialpha - mu_ialphasigma_ialpha","category":"page"},{"location":"cli/drosophila/#Drosophila-pipeline","page":"Drosophila pipeline","title":"Drosophila pipeline","text":"","category":"section"},{"location":"cli/drosophila/","page":"Drosophila pipeline","title":"Drosophila pipeline","text":"Found at path bin/drosophila.jl. Can either be pointed to filtered data from original Science paper or the raw data obtained from GEO ????. Running this command will reproduce all plots shown in the manuscript.","category":"page"},{"location":"lib/manifold/#Point-Cloud-Differential-Geometry","page":"Point Cloud Differential Geometry","title":"Point Cloud Differential Geometry","text":"","category":"section"},{"location":"lib/manifold/#Types","page":"Point Cloud Differential Geometry","title":"Types","text":"","category":"section"},{"location":"lib/manifold/","page":"Point Cloud Differential Geometry","title":"Point Cloud Differential Geometry","text":"Modules = [SeqSpace.DifferentialGeometry]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.Manifold","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.Manifold","text":"struct Manifold{T <: Real}\n    mesh :: Mesh{T}\n    surf :: Surface{T}\nend\n\nStore the representation of a differential geometry object. mesh is the empirical point cloud. surf is the estimated differentiable surface.\n\n\n\n\n\n","category":"type"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.Mesh","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.Mesh","text":"struct Mesh{T <: Real}\n    r   :: Array{T, 2}\n    vₙ  :: Array{T, 2}\n    tri :: Array{Int, 2}\nend\n\nStore a 2 dimensional triangular mesh, embedded into arbitrary dimensions. r and vₙ denote vertex positions and normals respectively. tri denotes (non-oriented) triangular faces.\n\n\n\n\n\n","category":"type"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.Surface","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.Surface","text":"struct Surface{T <: Real}\n    Θ  :: Array{T, 2}\n    x  :: Array{T}\n    L  :: T\n    Λ  :: Array{Polynomial{T}}\n    ∂Λ :: Array{Polynomial{T}}\nend\n\nStore a representation of a 2D surface embedded into higher dimensional Euclidean space. Fits the surface by:\n\nPartition the x axis so that each resultant interval has, on average, 20 points.\nFit points within each partition to a 2D ellipse.\nFit a polynomial function to each elliptical parameter over all partitions.\nUse polynomials to estimate tangent vectors.\n\nΘ denotes the elliptical parameters fit per partition. x denotes the input data. L denotes the length along the x axis. Λ denotes the polynomials for each elliptical parameter. ∂Λ denotes the derivative of polynomials for each elliptical parameter.\n\n\n\n\n\n","category":"type"},{"location":"lib/manifold/#Functions","page":"Point Cloud Differential Geometry","title":"Functions","text":"","category":"section"},{"location":"lib/manifold/","page":"Point Cloud Differential Geometry","title":"Point Cloud Differential Geometry","text":"Modules = [SeqSpace.DifferentialGeometry]\nOrder = [:function]","category":"page"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.basis-Tuple{SeqSpace.DifferentialGeometry.Surface, Any}","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.basis","text":"function basis(s::Surface, r)\n\nCompute the tangent vectors hatbme_phi hatbme_x associated to each point within r. Assumes all points within r are distributed over the surface. Assumes r is sized N times 3.\n\n\n\n\n\n","category":"method"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.ellipse-Tuple{Any}","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.ellipse","text":"ellipse(r)\n\nFit an ellipse to 2D point cloud r. r is assumed to be sized N times 2.\n\n\n\n\n\n","category":"method"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.interpolate-Tuple{Any, Any, Any}","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.interpolate","text":"function interpolate(M, ϕ, r)\n\nInterpolate the tensor field ϕ, defined at points r onto the vertices of manifold M. Interpolation is computed by finding the containing triangle within the mesh of M for each point r. Linear interpolation is performed per face.\n\n\n\n\n\n","category":"method"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.makefield-Tuple{SeqSpace.DifferentialGeometry.Manifold, AbstractArray}","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.makefield","text":"makefield(ℳ::Manifold, ϕ::AbstractArray; field::Symbol = :ℝ³)\n\nReturn an arbitrary tensor field defined over either the embedding space or the cylindrical pullback of manifold M. Higher level function than either scalar or vector.\n\n\n\n\n\n","category":"method"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.mesh","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.mesh","text":"mesh(io::IO, type::Symbol=:obj)\n\nLoad a Mesh object from stream io formatted with type. As of now, only .obj files are supported.\n\n\n\n\n\n","category":"function"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.order-Tuple{Any, Any}","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.order","text":"function order(tri, r)\n\nReorder the triangulation so that all labels are counterclockwise.\n\n\n\n\n\n","category":"method"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.pullback-Tuple{SeqSpace.DifferentialGeometry.Manifold}","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.pullback","text":"function pullback(ℳ::Manifold)\n\nReturns the 2D cylindrical projection of the mesh, as estimated by the internal surface.\n\n\n\n\n\n","category":"method"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.pullback-Tuple{SeqSpace.DifferentialGeometry.Surface, Any}","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.pullback","text":"function pullback(s::Surface, r)\n\nTransform Euclidean point cloud r into a 2D cylindrical projection defined by s. Cylindrical coordinates are [x,φ]. Assumes all points within r are distributed over the surface. Assumes r is sized N times 3.\n\n\n\n\n\n","category":"method"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.rescale-Tuple{Any}","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.rescale","text":"rescale(x)\n\nRescale array x to run between 01.\n\n\n\n\n\n","category":"method"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.scalar-Tuple{SeqSpace.DifferentialGeometry.Manifold, Any, Symbol}","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.scalar","text":"scalar(M::Manifold, ϕ, field::Symbol)\n\nReturn a scalar field ϕ interpolated onto either the embedded space of manifold M or the pullback. If field is :ℝ² the pullback is computed. If field is :ℝ³ the embedding space is returned.\n\n\n\n\n\n","category":"method"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.triangulation-Tuple{Any, Any}","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.triangulation","text":"function triangulation(r₀, rᵢ)\n\nReturn the triangle containing each point given in rᵢ. The triangulation used is delaunay triangulation defined by point cloud r₀.\n\n\n\n\n\n","category":"method"},{"location":"lib/manifold/#SeqSpace.DifferentialGeometry.vector-Tuple{SeqSpace.DifferentialGeometry.Manifold, Any, Symbol}","page":"Point Cloud Differential Geometry","title":"SeqSpace.DifferentialGeometry.vector","text":"scalar(M::Manifold, ϕ, field::Symbol)\n\nReturn a vector field field ϕ interpolated onto either the embedded space of manifold M or the pullback. If field is :ℝ² the pullback is computed. Only the tangent space component is kept. If field is :ℝ³ the embedding space is returned.\n\n\n\n\n\n","category":"method"},{"location":"lib/voronoi/#Voronoi","page":"Voronoi","title":"Voronoi","text":"","category":"section"},{"location":"lib/voronoi/#Types","page":"Voronoi","title":"Types","text":"","category":"section"},{"location":"lib/voronoi/","page":"Voronoi","title":"Voronoi","text":"Modules = [SeqSpace.Voronoi]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/voronoi/#Functions","page":"Voronoi","title":"Functions","text":"","category":"section"},{"location":"lib/voronoi/","page":"Voronoi","title":"Voronoi","text":"Modules = [SeqSpace.Voronoi]\nOrder = [:function]","category":"page"},{"location":"lib/voronoi/#SeqSpace.Voronoi.areas-Tuple{Any}","page":"Voronoi","title":"SeqSpace.Voronoi.areas","text":"areas(x)\n\nCompute the areas of all 2-dimensional simplices generated by the delaunay construction for pointcloud x. Assumes x is sized N times 2\n\n\n\n\n\n","category":"method"},{"location":"lib/voronoi/#SeqSpace.Voronoi.boundary-Tuple{Any}","page":"Voronoi","title":"SeqSpace.Voronoi.boundary","text":"boundary(d)\n\nReturns the boundary vertices of a unit cube in d dimensions.\n\n\n\n\n\n","category":"method"},{"location":"lib/voronoi/#SeqSpace.Voronoi.tessellation-Tuple{Any}","page":"Voronoi","title":"SeqSpace.Voronoi.tessellation","text":"tessellation(q)\n\nConstruct a voronoi tessellation from generating points q. Return just vertices of construction.\n\n\n\n\n\n","category":"method"},{"location":"lib/voronoi/#SeqSpace.Voronoi.volumes-Tuple{Any}","page":"Voronoi","title":"SeqSpace.Voronoi.volumes","text":"volumes(x)\n\nCompute the volume of all d-dimensional simplices generated by the delaunay construction for pointcloud x. Assumes x is sized N times d\n\n\n\n\n\n","category":"method"},{"location":"lib/seqspace/#SeqSpace-API","page":"SeqSpace API","title":"SeqSpace API","text":"","category":"section"},{"location":"lib/seqspace/#Types","page":"SeqSpace API","title":"Types","text":"","category":"section"},{"location":"lib/seqspace/","page":"SeqSpace API","title":"SeqSpace API","text":"Modules = [SeqSpace]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/seqspace/#SeqSpace.HyperParams","page":"SeqSpace API","title":"SeqSpace.HyperParams","text":"mutable struct HyperParams\n    dₒ :: Int\n    Ws :: Array{Int,1}\n    BN :: Array{Int,1}\n    DO :: Array{Int,1}\n    N  :: Int\n    δ  :: Int\n    η  :: Float64\n    B  :: Int\n    V  :: Int\n    k  :: Int\n    γₓ :: Float32\n    γᵤ :: Float32\n    g  :: Function\nend\n\nHyperParams is a collection of parameters that specify the network architecture and training hyperparameters of the autoencoder. dₒ is the desired output dimensionality of the encoding layer Ws is a collection of the network layer widths. The number of entries controls the depth. The decoder is mirror-symmetric. BN is the collection of layers that will be followed by batch normalization. DO is the collection of layers that will be followed by dropout. N  is the number of epochs to train against δ  is the number of epochs that will be sampled for logging η  is the learning rate B  is the batch size V  is the number of points to partition for validation purposes, i.e. won't be training against k  is the number of neighbors to use to estimate geodesics γₓ is the prefactor of distance soft rank loss γᵤ is the prefactor of uniform density loss g is the metric given to latent space\n\n\n\n\n\n","category":"type"},{"location":"lib/seqspace/#SeqSpace.Result","page":"SeqSpace API","title":"SeqSpace.Result","text":"struct Result\n    param :: HyperParams\n    loss  :: NamedTuple{(:train, :valid), Tuple{Array{Float64,1},Array{Float64,1}} }\n    model\nend\n\nStore the output of a trained autoencoder. param stores the input hyperparameters used to design and train the neural network. loss traces the dynamics of the optimization found during training. model represents the learned pullback and pushforward functions.\n\n\n\n\n\n","category":"type"},{"location":"lib/seqspace/#Functions","page":"SeqSpace API","title":"Functions","text":"","category":"section"},{"location":"lib/seqspace/","page":"SeqSpace API","title":"SeqSpace API","text":"Modules = [SeqSpace]\nOrder = [:function]","category":"page"},{"location":"lib/seqspace/#SeqSpace.buildloss-Tuple{Any, Any, Any}","page":"SeqSpace API","title":"SeqSpace.buildloss","text":"buildloss(model, D², param)\n\nReturn a loss function used to train a neural network model according to input hyperparameters param. model is a object with three fields, pullback, pushforward, and identity. pullback and pushforward refers to the encoder and decoder layers respectively, while the identity is the composition. D² is a matrix of pairwise distances that will be used as a quenched hyperparameter in the distance soft rank loss.\n\n\n\n\n\n","category":"method"},{"location":"lib/seqspace/#SeqSpace.cylinder²-Tuple{Any}","page":"SeqSpace API","title":"SeqSpace.cylinder²","text":"cylinders²(x)\n\nGenerate the matrix of pairwise distances between vectors x, assuming the points are distributed on a cylinder. x is assumed to be d \times N where d denotes the dimensionality of the vector and N denotes the number. The first coordinate of x is assumed to be the polar coordinate.\n\n\n\n\n\n","category":"method"},{"location":"lib/seqspace/#SeqSpace.euclidean²","page":"SeqSpace API","title":"SeqSpace.euclidean²","text":"euclidean²(x)\n\nGenerate the matrix of pairwise distances between vectors x, assuming the Euclidean metric. x is assumed to be d \times N where d denotes the dimensionality of the vector and N denotes the number.\n\n\n\n\n\n","category":"function"},{"location":"lib/seqspace/#SeqSpace.extendfit-Tuple{Result, Any, Any}","page":"SeqSpace API","title":"SeqSpace.extendfit","text":"extendfit(result::Result, input, epochs)\n\nRetrain model within result on input data for epochs more iterations. Returns a new Result.\n\n\n\n\n\n","category":"method"},{"location":"lib/seqspace/#SeqSpace.fitmodel-Tuple{Any, Any}","page":"SeqSpace API","title":"SeqSpace.fitmodel","text":"fitmodel(data, param; D²=nothing, chatty=true)\n\nTrain an autoencoder model, specified with param hyperparams, to fit data. data is assumed to be sized d \times N where d and N are dimensionality and cardinality respectively. If not nothing, D² is assumed to be a precomputed distance matrix of point cloud data. If chatty is true, function will print to stdout. Returns a Result type.\n\n\n\n\n\n","category":"method"},{"location":"lib/seqspace/#SeqSpace.linearprojection-Tuple{Any, Any}","page":"SeqSpace API","title":"SeqSpace.linearprojection","text":"linearprojection(x, d; Δ=1, Λ=nothing)\n\nProject an empirical distance matrix x onto d top principal components. Centers the result to have zero mean. Returns the projection, as well as a function to transform a projected vector back to the embedding space. Ignores top Δ principal components. If Λ is not nothing, assumes it is a precomputed SVD decomposition.\n\n\n\n\n\n","category":"method"},{"location":"lib/seqspace/#SeqSpace.marshal-Tuple{Result}","page":"SeqSpace API","title":"SeqSpace.marshal","text":"marshal(r::Result)\n\nSerialize a trained autoencoder to binary format suitable for disk storage. Store parameters of model as contiguous array\n\n\n\n\n\n","category":"method"},{"location":"lib/seqspace/#SeqSpace.unmarshal-Tuple{Any}","page":"SeqSpace API","title":"SeqSpace.unmarshal","text":"unmarshal(r::Result)\n\nDeserialize a trained autoencoder from binary format to semantic format. Represents model as a collection of functors.\n\n\n\n\n\n","category":"method"},{"location":"lib/normalize/#scRNAseq-Normalization","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"","category":"section"},{"location":"lib/normalize/#Types","page":"scRNAseq Normalization","title":"Types","text":"","category":"section"},{"location":"lib/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"Modules = [SeqSpace.Normalize]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/normalize/#SeqSpace.Normalize.FitType","page":"scRNAseq Normalization","title":"SeqSpace.Normalize.FitType","text":"FitType = NamedTuple{\n(\n :likelihood,\n :parameters,\n :uncertainty,\n :residual\n),\nTuple{Float64, Vector{Float64}, Vector{Float64}, Vector{Float64}}\n\nStores the result of MLE fit of one gene.\n\n\n\n\n\n","category":"type"},{"location":"lib/normalize/#Functions","page":"scRNAseq Normalization","title":"Functions","text":"","category":"section"},{"location":"lib/normalize/","page":"scRNAseq Normalization","title":"scRNAseq Normalization","text":"Modules = [SeqSpace.Normalize]\nOrder = [:function]","category":"page"},{"location":"lib/normalize/#SeqSpace.Normalize.bootstrap-Tuple{Any, Any}","page":"scRNAseq Normalization","title":"SeqSpace.Normalize.bootstrap","text":"bootstrap(count, depth; stochastic=negativebinomial, samples=50)\n\nEmpirically verify the MLE fit of count, using a GLM model generated by stochastic with confounding depth variables by bootstrap. One third of cells are removed and the parameters are re-estimated with the remaining cells. This process is repeated samples times. The resultant distribution of estimation is returned.\n\n\n\n\n\n","category":"method"},{"location":"lib/normalize/#SeqSpace.Normalize.fit-Tuple{Any, Any, Any}","page":"scRNAseq Normalization","title":"SeqSpace.Normalize.fit","text":"fit(stochastic, count, depth)\n\nFit a generative model stochastic to gene expression count data, assuming confounding sequencing depth. stochastic can be either negativebinomial or gamma.\n\n\n\n\n\n","category":"method"},{"location":"lib/normalize/#SeqSpace.Normalize.gamma-Tuple{Any, Any}","page":"scRNAseq Normalization","title":"SeqSpace.Normalize.gamma","text":"gamma(count, depth)\n\nCompute the log likelihood of a gamma distributed generalized linear model (GLM) with log link function for the estimated mean count of a single gene. The sequencing depth for each sequenced cell is assumed to be the only confounding variables.\n\n\n\n\n\n","category":"method"},{"location":"lib/normalize/#SeqSpace.Normalize.glm-Tuple{Any}","page":"scRNAseq Normalization","title":"SeqSpace.Normalize.glm","text":"glm(data; stochastic=negativebinomial, ϵ=1)\n\nFit a generalized linear model (GLM) to the matrix data. Genes are assumed to be on rows, cells over columns. The underlying generative model is passed by stochastic.\n\n\n\n\n\n","category":"method"},{"location":"lib/normalize/#SeqSpace.Normalize.logmean","page":"scRNAseq Normalization","title":"SeqSpace.Normalize.logmean","text":"logmean(x;ϵ=1)\n\nCompute the geometric mean: xpleft(langle logleft(x + epsilon right) rangleright) - epsilon\n\n\n\n\n\n","category":"function"},{"location":"lib/normalize/#SeqSpace.Normalize.logvar","page":"scRNAseq Normalization","title":"SeqSpace.Normalize.logvar","text":"logmean(x;ϵ=1)\n\nCompute the geometric variance: expleft(langle logleft(x + epsilon right)^2 rangle_cright) - epsilon\n\n\n\n\n\n","category":"function"},{"location":"lib/normalize/#SeqSpace.Normalize.negativebinomial-Tuple{Any, Any}","page":"scRNAseq Normalization","title":"SeqSpace.Normalize.negativebinomial","text":"negativebinomial(count, depth)\n\nCompute the log likelihood of a negative binomial generalized linear model (GLM) with log link function for count of a single gene. The sequencing depth for each sequenced cell is assumed to be the only confounding variables.\n\n\n\n\n\n","category":"method"},{"location":"lib/normalize/#SeqSpace.Normalize.prior-Tuple{Any}","page":"scRNAseq Normalization","title":"SeqSpace.Normalize.prior","text":"prior(params)\n\nEstimate a generalized normal distribution to params by maximum likelihood estimation. In practice, used to compute the empirical prior for overdispersion factor Θ₃ in the negative binomial.\n\n\n\n\n\n","category":"method"},{"location":"lib/model/#Autoencoder-Manifold-Learning","page":"Autoencoder Manifold Learning","title":"Autoencoder Manifold Learning","text":"","category":"section"},{"location":"lib/model/#Types","page":"Autoencoder Manifold Learning","title":"Types","text":"","category":"section"},{"location":"lib/model/","page":"Autoencoder Manifold Learning","title":"Autoencoder Manifold Learning","text":"Modules = [SeqSpace.ML]\nOrder = [:type, :constant]","category":"page"},{"location":"lib/model/#SeqSpace.ML.LayerIterator","page":"Autoencoder Manifold Learning","title":"SeqSpace.ML.LayerIterator","text":"struct LayerIterator\n    width     :: Array{Int}\n    dropout   :: Set{Int}\n    normalize :: Set{Int}\n    σᵢ        :: Function\n    σₒ        :: Function\n    σ         :: Function\nend\n\nAn iterator used to generate dense latent layers within a neural network. width denotes the widths of each layer; the length of this array immediately determines the depth. dropout denotes the layers, as given by width that are followed by a dropout layer. normalize denotes the layers, as given by width that are followed by a batch normalization layer. σᵢ, σₒ, σ is the activation energy on the first, last, and intermediate layers respectively.\n\n\n\n\n\n","category":"type"},{"location":"lib/model/#Functions","page":"Autoencoder Manifold Learning","title":"Functions","text":"","category":"section"},{"location":"lib/model/","page":"Autoencoder Manifold Learning","title":"Autoencoder Manifold Learning","text":"Modules = [SeqSpace.ML]\nOrder = [:function]","category":"page"},{"location":"lib/model/#SeqSpace.ML.batch-Tuple{Any, Any}","page":"Autoencoder Manifold Learning","title":"SeqSpace.ML.batch","text":"batch(data, n)\n\nRandomly partition data into groups of size n.\n\n\n\n\n\n","category":"method"},{"location":"lib/model/#SeqSpace.ML.model-Tuple{Any, Any}","page":"Autoencoder Manifold Learning","title":"SeqSpace.ML.model","text":"model(dᵢ, dₒ; Ws=Int[], normalizes=Int[], dropouts=Int[], σ=elu)\n\nInitialize an autoencoding neural network with input dimension dᵢ and latent layers dₒ. Ws specifies both the width and depth of the encoder layer - the width of each layer is given as an entry in the array while the length specifies the depth. normalizes and dropouts denote which layers are followed by batch normalization and dropout specifically. The decoder layer is given the mirror symmetric architecture.\n\n\n\n\n\n","category":"method"},{"location":"lib/model/#SeqSpace.ML.train!-NTuple{4, Any}","page":"Autoencoder Manifold Learning","title":"SeqSpace.ML.train!","text":"train!(model, data, index, loss; B=64, η=1e-3, N=100, log=noop)\n\nTrains autoencoder model on data by minimizing loss. index stores the underlying indices of data used for training. Will mutate the underlying parameters of model. Optional parameters include:\n\nB denotes the batch size to be used.\nN denotes the number of epochs.\nη denotes the learning rate.\n\n\n\n\n\n","category":"method"},{"location":"lib/model/#SeqSpace.ML.update_dimension-Tuple{Any, Any}","page":"Autoencoder Manifold Learning","title":"SeqSpace.ML.update_dimension","text":"update_dimension(model, dₒ; ϵ = 1e-6)\n\nAdd a colection of new neurons in the encoding layer to encode in the encoding layer to increase dimensions to dₒ. Model weights for the initial dimensions are kept the same.\n\n\n\n\n\n","category":"method"},{"location":"lib/model/#SeqSpace.ML.validate-Tuple{Any, Any}","page":"Autoencoder Manifold Learning","title":"SeqSpace.ML.validate","text":"validate(data, len)\n\nReserve len samples from data during training process to allow for model validation.\n\n\n\n\n\n","category":"method"},{"location":"#SeqSpace","page":"Home","title":"SeqSpace","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A fast, self-contained Julia library to normalize scRNAseq data and learn its underlying geometric structure in both a supervised and unsupervised fashion.","category":"page"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"SeqSpace is an experimental Julia library and command line tool suite that will both normalize and subsequently learn/parameterize scRNAseq data to a low-dimensional manifold. Our approach is orthogonal to traditional scRNAseq pipelines such as UMAP or tSNE that focus primarily on clustering low-dimensional embeddings; we do not assume our data is drawn from categorical cell types. Instead, our methodology is intended to be used on datasets that are well described by a small number of continuous degrees of freedom, for example to measure:","category":"page"},{"location":"","page":"Home","title":"Home","text":"positional information of cells undergoing morphogenesis\ntime within cell cycle\ncellular aging","category":"page"},{"location":"","page":"Home","title":"Home","text":"However, at present, it has only been empirically validated on scRNAseq data obtained during early Drosophila embryogenesis.","category":"page"},{"location":"","page":"Home","title":"Home","text":"There are many loosely connected library modules that are designed to help parameterize low-dimensional scRNAseq data in some capacity . This documentation is written to both help navigate across these different modules, as well as to describe and motivate the algorithmic design in detail. The main functionality contained within the codebase is:","category":"page"},{"location":"","page":"Home","title":"Home","text":"scRNAseq normalization\nscRNAseq pointcloud operations\nsupervised scRNAseq spatial mapping\nunsupervised scRNAseq manifold learning","category":"page"},{"location":"","page":"Home","title":"Home","text":"We refer the interested reader to both our in-depth algorithmic expositions as well as the library API documentation for more details.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"There are multiple ways to install the SeqSpace library","category":"page"},{"location":"#From-Julia-REPL","page":"Home","title":"From Julia REPL","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"    (@v1.x) pkg> add https://github.com/nnoll/seqspace.git","category":"page"},{"location":"#From-Command-Line","page":"Home","title":"From Command Line","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"    julia -e 'using Pkg; Pkg.add(\"https://github.com/nnoll/seqspace.git\"); Pkg.build()'","category":"page"},{"location":"#Local-Environment","page":"Home","title":"Local Environment","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Clone the repository.","category":"page"},{"location":"","page":"Home","title":"Home","text":"    git clone https://github.com/nnoll/seqspace.git && cd seqspace","category":"page"},{"location":"","page":"Home","title":"Home","text":"Build the package. This will create a seperate Julia environment for SeqSpace","category":"page"},{"location":"","page":"Home","title":"Home","text":"    julia --project=. -e 'using Pkg; Pkg.build()'","category":"page"},{"location":"","page":"Home","title":"Home","text":"Enter the REPL","category":"page"},{"location":"","page":"Home","title":"Home","text":"    julia --project=.","category":"page"},{"location":"#Citing","page":"Home","title":"Citing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"TBA","category":"page"},{"location":"sci/inference/#scRNAseq-spatial-inference","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"","category":"section"},{"location":"sci/inference/#Introduction","page":"scRNAseq spatial inference","title":"Introduction","text":"","category":"section"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"In order to understand the regulation of morphogenesis, it is paramount to understand both the dynamics of gene expression at the single-cell level, as well as, interactions between the transcriptomic state of neighboring cells. With the advent of single-cell sequencing technology, it has now become possible to directly measure the transcriptome of cellular aggregates at cellular resolution, however, parameterizing such data by space remains challenging. Specifically, in the process of isolating individual cells for downstream sequencing, embryonic cells must be dissociated and suspended in liquid. This prevents straightforward application of scRNAseq technology to the problems of Developmental Biology.","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"The straightforward resolution to this conundrum is to leverage pre-existing databases of known in-situ markers. Such curated datasets should provide the spatial expression profiles for a small subset of genes that could be matched against scRNAseq counts measured from the same organism at the same stage of development. Once such a subset of genes are \"matched\" between the in-situ and scRNAseq data, the spatial pattern for each gene for the remainder of the transcriptome would come for \"free.\" The collection of estimated gene expression patterns would function as a high-resolution atlas over physical space of the embryo's shape; a potentially important resource for researchers.","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"Additionally, such an inference would provide us with an estimate of the position on the embryo each cell was sampled from. Assuming one has the ability to detect the intrinsic manifold of gene expression, as detailed elsewhere, such positional labels would provide an interesting overlay to attempt to learn the genotype to space map encoded by the genome. Below we provide an (incomplete) overview of past attempts at this problem, as well as detail our attempt to infer the position of scRNAseq cells.","category":"page"},{"location":"sci/inference/#Overview-of-current-methods","page":"scRNAseq spatial inference","title":"Overview of current methods","text":"","category":"section"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"FILL ME OUT There are published techniques that attempt to solve this problem, however our attempts to utilize them were unsuccessful.","category":"page"},{"location":"sci/inference/#Our-approach","page":"scRNAseq spatial inference","title":"Our approach","text":"","category":"section"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"While the below discussion is general, all results shown within the context of Drosophila melongaster and thus will be utilizing the Berkeley Drosophila Transcriptional Network Project database [1]. Specifically, the BDTNP collaboration produced a \"virtual embryo\" pointcloud discretizing the spatial expression pattern of 84 genes. The virtual embryo was constructed by aligning florescent pointcloud data obtained by FISH of similarly staged embryos.","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"<p align=\"center\">\n<figure>\n  <img src=\"/assets/drosophila/bdtnp.jpg\" width=\"99%\" />\n  <figurecaption>\n  Cartoon of the procedure used to generate the virtual embryo by the BDTNP.\n  </figurecaption>\n</figure>\n</p>","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"[1]: Registering Drosophila Embryos at Cellular Resolution to Build a Quantitative 3D Atlas of Gene Expression Patterns and Morphology","category":"page"},{"location":"sci/inference/#Objective-function","page":"scRNAseq spatial inference","title":"Objective function","text":"","category":"section"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"Let alpha and a denote indices over scRNAseq cells and embryonic spatial positions respectively. Our goal is to solve for the sampling probability distribution rho_aalpha, i.e. the probability that cell alpha was sampled from position a. We pose this problem using the tools of Statistical Physics and thus formulate the solution as the extrema of the following free energy","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"    tag1 F(rho_aalpha) equiv displaystylesumlimits_alphaa epsilonleft(vecg_alphavecG_aright)rho_aalpha + Trho_aalphalogleft(rho_aalpharight)","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"where vecg_alpha and vecG_a denote the transcriptomic state of cell alpha and position a respectively. We note that, as formulated, this problem is equivalent to regularized optimal transport[2]. epsilon denotes the energy required to map cell alpha onto position a. For now, we leave the functional form generic but denote that it explicitly depends upon the gene expression value of both the sequenced cell alpha and the in-situ position a. Additionally, T is a parameter analogous to a thermodynamic \"temperature\"; it controls the precision that we demand of the inferred position for each sequenced cell. As T rightarrow 0, each cell is constrained to injectively map onto a spatial position; the problem reduces to the assignment problem[3]. Conversely, as T rightarrow infty, the entropic term dominates; the solution converges onto the uniform distribution.","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"[2]: Sinkhorn Distances: Lightspeed Computation of Optimal Transportation Distances","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"[3]: Computational optimal transport: With applications to data science","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"However, if extremized as written, Eq. (1) would not result in a well-formed probability distribution. Specifically, we must additionally constrain the row and column sum of rho_aalpha to have correctly interpretable marginals. As such, in order for rho_aalpha to be interpreted as the probability that cell alpha was sampled from position a, forall alpha  sum_a rho_aalpha = 1 must hold. Additionally, we assume there were no biases in cellular isolation during the sequencing procedure.  Thus each position is assumed to be sampled uniformally, and thus impose uniform spatial coverage forall a  sum_alpha rho_aalpha = fracN_cN_x. N_x and N_c denote the number of in-situ positions and sequenced cells respectively. Taken together, our full free energy is of the form","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"    tildeF(rho_aalpha)equivdisplaystylesumlimits_alphaa epsilonleft(vecg_alphavecG_aright)rho_aalpha + Trho_aalphalogleft(rho_aalpharight) + displaystylesum_aLambda_aleftfracN_cN_x-sum_alpha rho_aalpha right + displaystylesum_alpha lambda_alphaleft1-sum_arho_aalpha right","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"The solution is found to be","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"    tag2 rho_aalpha^* = e^Lambda_a e^-T^-1left(epsilonleft(vecg_alphavecG_aright)-1right) e^lambda_alpha","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"where Lambda_a and lambda_alpha are determined by utilizing the Sinkhorn-Knopp algorithm in conjunction with the marginal constraints prescribed above. Eq. (2) provides a fast, scalable algorithm to estimate the sampling posterior given any cost function epsilon(vecg_avecG_alpha). All that remains is to formulate an explicit model.","category":"page"},{"location":"sci/inference/#Microscopic-model","page":"scRNAseq spatial inference","title":"Microscopic model","text":"","category":"section"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"As seen by Eq. (2), the cost function can be viewed as the energy of a Boltzmann distribution: Eleft(vecg_alpha vecG_aright) equiv left(vecg_alpha vecG_aright) Hence, an obvious interpretation of E(vecg_alpha vecG_a) is as the negative log-likelihood that vecg_alpha and vecG_a were sampled from the sample entity. Our first simplifying assumption is that genes within the database are statistically independent of each other and thus the log likelihood is additive","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"    tag3 E(vecg_alpha vecG_a) = frac1N_gdisplaystylesumlimits_i=1^N_g varepsilonleft(g_alpha i G_airight)","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"where i indexes genes and varepsilon denotes the single-body energetics. Thus the problem has been reduced to parameterizing the log-likelihood that g_alpha i and G_ai were sampled from the same underlying cell. However, the complication is that our scRNAseq data and the in-situ expression database are not directly relatable. Both datasets are in manifestly different unit systems, the scRNAseq data are expressed in UMI counts while the underlying database is ultimately florescent intensity collated from a myriad of FISH experiments.","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"We postulate that the \"true\" transformation that maps scRNAseq counts g_alpha i to florescent intensity G_ai should minimize distortions between both observed distortions under the action of said map. This ultimately suggests we identify the putative transformation via minimizing the Wasserstein metric via optimal transport [4]. It has been shown [5] that strictly convex cost functions varepsilon between 1D distributions admit a unique optimal transport solution. Specifically, if we denote the cumulative density of g_alpha i and G_ai by phi_i and Phi_i respectively, the minimizing transformation is given by Phi_i^-1 circ phi_i. We assume a Gaussian sampling probability with mean given by the BDTNP database such that our one-body energy takes the form","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"    tag4 varepsilonleft(g_alpha i G_airight) equiv left(Phi^-1_ileft(phi_ileft(g_alpha iright)right) - G_airight)^2","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"Importantly, we have enforced that each gene has unit variance within its sampling distribution. Non-unit uniform variance would simply rescale our temperature parameter and thus can be ignored. Conversely, heterogeneous variances would effectively act as a additive weighting prefactor in the summation of Eq. (3). We do not consider this case here but note that it is an interesting avenue for future improvement.","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"Eqns. (2-4) uniquely determine the sampling probability rho_aalpha modulo one free parameter, temperature T. A priori we expect the fit of gene expression to the database to be non-monotonic with respect to temperature. Minimizing of Eqn. (1) is singular as T to 0 and reduces to the assignment problem and thus is expected to be highly susceptible to noise. Conversely, as T to infty the entropic contribution to Eqn. (1) dominates and thus admits a uniform sampling distribution with no patterning. As such, we fix the temperature by hyperparameter optimization, i.e. to be the value that maximally correlates to the original BDTNP database.","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"[4]: Optimal transport: old and new","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"[5]: Mass Transportation Problems: Volume I: Theory","category":"page"},{"location":"sci/inference/#Results","page":"scRNAseq spatial inference","title":"Results","text":"","category":"section"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"Once the sampling probability rho_aalpha is known, we can immediately compute the mean expression profile for each gene","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"    barg_ia equiv displaystylesumlimits_alpha rho_aalpha g_ialpha","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"We compare the computed mean expression profile to the \"known\" pattern as given by the BDTNP database. As shown in the figure below, we capture sim 70 of the variance of the database, on average across all 84 genes, at the optimal temperature. The red vertical bars display the standard deviation across all genes. The mapping at the optimal temperature corresponds to a sampling entropy rho_ialpha of sim 6 bits, corresponding to each scRNAseq cell mapping to sim 60 positions of the embryo.","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"<p align=\"center\">\n<figure>\n  <img src=\"/assets/drosophila/bdtnp_fit.png\" width=\"49%\" />\n  <img src=\"/assets/drosophila/bdtnp_entropy.png\" width=\"49%\" />\n  <figurecaption>\n  (Left)Residuals of scRNAseq expression patterns matches to BDTNP database.\n  (Right) Entropy of sampling distribution, averaged over cells.\n  </figurecaption>\n</figure>\n</p>","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"We note that our positional precision is significantly less than what has been found in previous studies utilizing florescence data [6] that concluded gene expression could identify position with subcellular accuracy. The origin of the discovered \"imprecision\" is likely multifaceted owing, in part, to technical noise inherent to scRNAseq technology, systematic errors due to mistiming of the embryo stage between the database and scRNAseq data, as well as true biological variation associated to the full 2D positional information. Deconvolving the contributions of each stochastic process to our measurement error is an interesting future direction for research.","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"[6]: Positional information, in bits","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"In addition to matching the small subset of genes contained within the BDTNP database, we also produce predictions for gene expression for the remaining sim 10^4 genes. The full database of predictions is available upon request; an exploratory web front-end is currently underway. Below we display a representative sample.","category":"page"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"<p align=\"center\">\n<figure>\n  <img src=\"/assets/drosophila/gene/eve.png\" width=\"32%\" />\n  <img src=\"/assets/drosophila/gene/ftz.png\" width=\"32%\" />\n  <img src=\"/assets/drosophila/gene/twi.png\" width=\"32%\" />\n  <figurecaption>\n  Genes shown (left-to-right): eve, ftz, twi\n  </figurecaption>\n</figure>\n</p>","category":"page"},{"location":"sci/inference/#Discussion","page":"scRNAseq spatial inference","title":"Discussion","text":"","category":"section"},{"location":"sci/inference/","page":"scRNAseq spatial inference","title":"scRNAseq spatial inference","text":"Unfortunately, such databases only exist for a select few model organisms and thus limit the applicability of this approach. Our hope is to leverage phenomenology gleaned from the correlations between expression and space for organisms with such a database that ","category":"page"}]
}
